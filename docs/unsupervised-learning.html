<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 无监督学习方法 | 现代精算统计模型</title>
  <meta name="description" content="The output format is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5 无监督学习方法 | 现代精算统计模型" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The output format is bookdown::gitbook." />
  <meta name="github-repo" content="sxpyggy/Modern-Actuarial-Models" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 无监督学习方法 | 现代精算统计模型" />
  
  <meta name="twitter:description" content="The output format is bookdown::gitbook." />
  

<meta name="author" content="Modern Actuarial Models" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="boosting.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">现代精算统计模型</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>👨‍🏫 欢迎</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#答疑"><i class="fa fa-check"></i>🤔 答疑</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#课程安排"><i class="fa fa-check"></i>🗓️ 课程安排</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i>简介</a></li>
<li class="chapter" data-level="1" data-path="pre.html"><a href="pre.html"><i class="fa fa-check"></i><b>1</b> 准备工作</a><ul>
<li class="chapter" data-level="1.1" data-path="pre.html"><a href="pre.html#常用链接"><i class="fa fa-check"></i><b>1.1</b> 常用链接</a></li>
<li class="chapter" data-level="1.2" data-path="pre.html"><a href="pre.html#克隆代码"><i class="fa fa-check"></i><b>1.2</b> 克隆代码</a></li>
<li class="chapter" data-level="1.3" data-path="pre.html"><a href="pre.html#r-interface-to-keras"><i class="fa fa-check"></i><b>1.3</b> R interface to Keras</a><ul>
<li class="chapter" data-level="1.3.1" data-path="pre.html"><a href="pre.html#r自动安装"><i class="fa fa-check"></i><b>1.3.1</b> R自动安装</a></li>
<li class="chapter" data-level="1.3.2" data-path="pre.html"><a href="pre.html#使用reticulate关联conda环境"><i class="fa fa-check"></i><b>1.3.2</b> 使用reticulate关联conda环境</a></li>
<li class="chapter" data-level="1.3.3" data-path="pre.html"><a href="pre.html#指定conda安装"><i class="fa fa-check"></i><b>1.3.3</b> 指定conda安装</a></li>
<li class="chapter" data-level="1.3.4" data-path="pre.html"><a href="pre.html#使用reticulate安装"><i class="fa fa-check"></i><b>1.3.4</b> 使用reticulate安装</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="pre.html"><a href="pre.html#r-interface-to-python"><i class="fa fa-check"></i><b>1.4</b> R interface to Python</a><ul>
<li class="chapter" data-level="1.4.1" data-path="pre.html"><a href="pre.html#reticulate-常见命令"><i class="fa fa-check"></i><b>1.4.1</b> reticulate 常见命令</a></li>
<li class="chapter" data-level="1.4.2" data-path="pre.html"><a href="pre.html#切换r关联的conda环境"><i class="fa fa-check"></i><b>1.4.2</b> 切换R关联的conda环境</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="pre.html"><a href="pre.html#python"><i class="fa fa-check"></i><b>1.5</b> Python</a><ul>
<li class="chapter" data-level="1.5.1" data-path="pre.html"><a href="pre.html#conda环境"><i class="fa fa-check"></i><b>1.5.1</b> Conda环境</a></li>
<li class="chapter" data-level="1.5.2" data-path="pre.html"><a href="pre.html#常用的conda命令"><i class="fa fa-check"></i><b>1.5.2</b> 常用的Conda命令</a></li>
<li class="chapter" data-level="1.5.3" data-path="pre.html"><a href="pre.html#tensorflowpytorch-gpu-version"><i class="fa fa-check"></i><b>1.5.3</b> Tensorflow/Pytorch GPU version</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="french.html"><a href="french.html"><i class="fa fa-check"></i><b>2</b> 车险索赔频率预测</a><ul>
<li class="chapter" data-level="2.1" data-path="french.html"><a href="french.html#背景介绍"><i class="fa fa-check"></i><b>2.1</b> 背景介绍</a></li>
<li class="chapter" data-level="2.2" data-path="french.html"><a href="french.html#预测模型概述"><i class="fa fa-check"></i><b>2.2</b> 预测模型概述</a></li>
<li class="chapter" data-level="2.3" data-path="french.html"><a href="french.html#特征工程"><i class="fa fa-check"></i><b>2.3</b> 特征工程</a><ul>
<li class="chapter" data-level="2.3.1" data-path="french.html"><a href="french.html#截断"><i class="fa fa-check"></i><b>2.3.1</b> 截断</a></li>
<li class="chapter" data-level="2.3.2" data-path="french.html"><a href="french.html#离散化"><i class="fa fa-check"></i><b>2.3.2</b> 离散化</a></li>
<li class="chapter" data-level="2.3.3" data-path="french.html"><a href="french.html#设定基础水平"><i class="fa fa-check"></i><b>2.3.3</b> 设定基础水平</a></li>
<li class="chapter" data-level="2.3.4" data-path="french.html"><a href="french.html#协变量变形"><i class="fa fa-check"></i><b>2.3.4</b> 协变量变形</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="french.html"><a href="french.html#训练集-验证集-测试集"><i class="fa fa-check"></i><b>2.4</b> 训练集-验证集-测试集</a></li>
<li class="chapter" data-level="2.5" data-path="french.html"><a href="french.html#泊松偏差损失函数"><i class="fa fa-check"></i><b>2.5</b> 泊松偏差损失函数</a></li>
<li class="chapter" data-level="2.6" data-path="french.html"><a href="french.html#泊松回归模型"><i class="fa fa-check"></i><b>2.6</b> 泊松回归模型</a></li>
<li class="chapter" data-level="2.7" data-path="french.html"><a href="french.html#泊松可加模型"><i class="fa fa-check"></i><b>2.7</b> 泊松可加模型</a></li>
<li class="chapter" data-level="2.8" data-path="french.html"><a href="french.html#泊松回归树"><i class="fa fa-check"></i><b>2.8</b> 泊松回归树</a></li>
<li class="chapter" data-level="2.9" data-path="french.html"><a href="french.html#随机森林"><i class="fa fa-check"></i><b>2.9</b> 随机森林</a></li>
<li class="chapter" data-level="2.10" data-path="french.html"><a href="french.html#泊松提升树"><i class="fa fa-check"></i><b>2.10</b> 泊松提升树</a></li>
<li class="chapter" data-level="2.11" data-path="french.html"><a href="french.html#模型比较"><i class="fa fa-check"></i><b>2.11</b> 模型比较</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nn.html"><a href="nn.html"><i class="fa fa-check"></i><b>3</b> 神经网络</a><ul>
<li class="chapter" data-level="3.1" data-path="nn.html"><a href="nn.html#建立神经网络的一般步骤"><i class="fa fa-check"></i><b>3.1</b> 建立神经网络的一般步骤</a><ul>
<li class="chapter" data-level="3.1.1" data-path="nn.html"><a href="nn.html#明确目标和数据类型"><i class="fa fa-check"></i><b>3.1.1</b> 明确目标和数据类型</a></li>
<li class="chapter" data-level="3.1.2" data-path="nn.html"><a href="nn.html#数据预处理"><i class="fa fa-check"></i><b>3.1.2</b> 数据预处理</a></li>
<li class="chapter" data-level="3.1.3" data-path="nn.html"><a href="nn.html#选取合适的神经网络类型"><i class="fa fa-check"></i><b>3.1.3</b> 选取合适的神经网络类型</a></li>
<li class="chapter" data-level="3.1.4" data-path="nn.html"><a href="nn.html#建立神经网络全连接神经网络"><i class="fa fa-check"></i><b>3.1.4</b> 建立神经网络（全连接神经网络）</a></li>
<li class="chapter" data-level="3.1.5" data-path="nn.html"><a href="nn.html#训练神经网络"><i class="fa fa-check"></i><b>3.1.5</b> 训练神经网络</a></li>
<li class="chapter" data-level="3.1.6" data-path="nn.html"><a href="nn.html#调参"><i class="fa fa-check"></i><b>3.1.6</b> 调参</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="nn.html"><a href="nn.html#数据预处理-1"><i class="fa fa-check"></i><b>3.2</b> 数据预处理</a></li>
<li class="chapter" data-level="3.3" data-path="nn.html"><a href="nn.html#神经网络提升模型-combined-actuarial-neural-network"><i class="fa fa-check"></i><b>3.3</b> 神经网络提升模型 （combined actuarial neural network）</a></li>
<li class="chapter" data-level="3.4" data-path="nn.html"><a href="nn.html#神经网络结构"><i class="fa fa-check"></i><b>3.4</b> 神经网络结构</a><ul>
<li class="chapter" data-level="3.4.1" data-path="nn.html"><a href="nn.html#结构参数"><i class="fa fa-check"></i><b>3.4.1</b> 结构参数</a></li>
<li class="chapter" data-level="3.4.2" data-path="nn.html"><a href="nn.html#输入层"><i class="fa fa-check"></i><b>3.4.2</b> 输入层</a></li>
<li class="chapter" data-level="3.4.3" data-path="nn.html"><a href="nn.html#embedding-layer"><i class="fa fa-check"></i><b>3.4.3</b> Embedding layer</a></li>
<li class="chapter" data-level="3.4.4" data-path="nn.html"><a href="nn.html#隐藏层"><i class="fa fa-check"></i><b>3.4.4</b> 隐藏层</a></li>
<li class="chapter" data-level="3.4.5" data-path="nn.html"><a href="nn.html#输出层"><i class="fa fa-check"></i><b>3.4.5</b> 输出层</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="nn.html"><a href="nn.html#训练神经网络-1"><i class="fa fa-check"></i><b>3.5</b> 训练神经网络</a></li>
<li class="chapter" data-level="3.6" data-path="nn.html"><a href="nn.html#总结"><i class="fa fa-check"></i><b>3.6</b> 总结</a></li>
<li class="chapter" data-level="3.7" data-path="nn.html"><a href="nn.html#其它模型"><i class="fa fa-check"></i><b>3.7</b> 其它模型</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>4</b> 提升方法 (Boosting)</a><ul>
<li class="chapter" data-level="4.1" data-path="boosting.html"><a href="boosting.html#adaboost"><i class="fa fa-check"></i><b>4.1</b> AdaBoost</a></li>
<li class="chapter" data-level="4.2" data-path="boosting.html"><a href="boosting.html#logit-boost-real-discrete-gentle-adaboost"><i class="fa fa-check"></i><b>4.2</b> Logit Boost (real, discrete, gentle AdaBoost)</a></li>
<li class="chapter" data-level="4.3" data-path="boosting.html"><a href="boosting.html#adaboost.m1"><i class="fa fa-check"></i><b>4.3</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="4.4" data-path="boosting.html"><a href="boosting.html#samme-stage-wise-additive-modeling-using-a-multi-class-exponential-loss-function"><i class="fa fa-check"></i><b>4.4</b> SAMME (Stage-wise Additive Modeling using a Multi-class Exponential loss function)</a></li>
<li class="chapter" data-level="4.5" data-path="boosting.html"><a href="boosting.html#samme.r-multi-class-real-adaboost"><i class="fa fa-check"></i><b>4.5</b> SAMME.R (multi-class real AdaBoost)</a></li>
<li class="chapter" data-level="4.6" data-path="boosting.html"><a href="boosting.html#gradient-boosting"><i class="fa fa-check"></i><b>4.6</b> Gradient Boosting</a></li>
<li class="chapter" data-level="4.7" data-path="boosting.html"><a href="boosting.html#newton-boosting"><i class="fa fa-check"></i><b>4.7</b> Newton Boosting</a></li>
<li class="chapter" data-level="4.8" data-path="boosting.html"><a href="boosting.html#xgboost"><i class="fa fa-check"></i><b>4.8</b> XGBoost</a></li>
<li class="chapter" data-level="4.9" data-path="boosting.html"><a href="boosting.html#case-study"><i class="fa fa-check"></i><b>4.9</b> Case study</a><ul>
<li class="chapter" data-level="4.9.1" data-path="boosting.html"><a href="boosting.html#数据描述"><i class="fa fa-check"></i><b>4.9.1</b> 数据描述</a></li>
<li class="chapter" data-level="4.9.2" data-path="boosting.html"><a href="boosting.html#数据预处理-2"><i class="fa fa-check"></i><b>4.9.2</b> 数据预处理</a></li>
<li class="chapter" data-level="4.9.3" data-path="boosting.html"><a href="boosting.html#特征工程-1"><i class="fa fa-check"></i><b>4.9.3</b> 特征工程</a></li>
<li class="chapter" data-level="4.9.4" data-path="boosting.html"><a href="boosting.html#建模流程"><i class="fa fa-check"></i><b>4.9.4</b> 建模流程</a></li>
<li class="chapter" data-level="4.9.5" data-path="boosting.html"><a href="boosting.html#模型度量gini系数"><i class="fa fa-check"></i><b>4.9.5</b> 模型度量——Gini系数</a></li>
<li class="chapter" data-level="4.9.6" data-path="boosting.html"><a href="boosting.html#建立adaboost模型"><i class="fa fa-check"></i><b>4.9.6</b> 建立AdaBoost模型</a></li>
<li class="chapter" data-level="4.9.7" data-path="boosting.html"><a href="boosting.html#建立xgboost模型"><i class="fa fa-check"></i><b>4.9.7</b> 建立XGBoost模型</a></li>
<li class="chapter" data-level="4.9.8" data-path="boosting.html"><a href="boosting.html#结论"><i class="fa fa-check"></i><b>4.9.8</b> 结论</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="boosting.html"><a href="boosting.html#appendix-commonly-used-python-code-for-py-beginners"><i class="fa fa-check"></i><b>4.10</b> Appendix: Commonly used Python code (for py-beginners)</a><ul>
<li class="chapter" data-level="4.10.1" data-path="boosting.html"><a href="boosting.html#python标准数据类型"><i class="fa fa-check"></i><b>4.10.1</b> Python标准数据类型</a></li>
<li class="chapter" data-level="4.10.2" data-path="boosting.html"><a href="boosting.html#python内置函数"><i class="fa fa-check"></i><b>4.10.2</b> Python内置函数</a></li>
<li class="chapter" data-level="4.10.3" data-path="boosting.html"><a href="boosting.html#numpy包"><i class="fa fa-check"></i><b>4.10.3</b> numpy包</a></li>
<li class="chapter" data-level="4.10.4" data-path="boosting.html"><a href="boosting.html#pandas包"><i class="fa fa-check"></i><b>4.10.4</b> pandas包</a></li>
<li class="chapter" data-level="4.10.5" data-path="boosting.html"><a href="boosting.html#matplotlib包"><i class="fa fa-check"></i><b>4.10.5</b> Matplotlib包</a></li>
<li class="chapter" data-level="4.10.6" data-path="boosting.html"><a href="boosting.html#常用教程网址"><i class="fa fa-check"></i><b>4.10.6</b> 常用教程网址</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>5</b> 无监督学习方法</a><ul>
<li class="chapter" data-level="5.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#主成分分析"><i class="fa fa-check"></i><b>5.1</b> 主成分分析</a></li>
<li class="chapter" data-level="5.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#自编码"><i class="fa fa-check"></i><b>5.2</b> 自编码</a><ul>
<li class="chapter" data-level="5.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#模型训练"><i class="fa fa-check"></i><b>5.2.1</b> 模型训练</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#聚类"><i class="fa fa-check"></i><b>5.3</b> 聚类</a><ul>
<li class="chapter" data-level="5.3.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>5.3.1</b> K-means clustering</a></li>
<li class="chapter" data-level="5.3.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-medoids-clustering-pam"><i class="fa fa-check"></i><b>5.3.2</b> K-medoids clustering (PAM)</a></li>
<li class="chapter" data-level="5.3.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#gaussian-mixture-modelsgmms"><i class="fa fa-check"></i><b>5.3.3</b> Gaussian mixture models(GMMs)</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#低维可视化"><i class="fa fa-check"></i><b>5.4</b> 低维可视化</a><ul>
<li class="chapter" data-level="5.4.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#t-sne"><i class="fa fa-check"></i><b>5.4.1</b> t-SNE</a></li>
<li class="chapter" data-level="5.4.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#umap"><i class="fa fa-check"></i><b>5.4.2</b> UMAP</a></li>
<li class="chapter" data-level="5.4.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#som"><i class="fa fa-check"></i><b>5.4.3</b> SOM</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#case-study-1"><i class="fa fa-check"></i><b>5.5</b> Case study</a><ul>
<li class="chapter" data-level="5.5.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#数据预处理-3"><i class="fa fa-check"></i><b>5.5.1</b> 数据预处理</a></li>
<li class="chapter" data-level="5.5.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#主成分分析-1"><i class="fa fa-check"></i><b>5.5.2</b> 主成分分析</a></li>
<li class="chapter" data-level="5.5.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#自编码-1"><i class="fa fa-check"></i><b>5.5.3</b> 自编码</a></li>
<li class="chapter" data-level="5.5.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means"><i class="fa fa-check"></i><b>5.5.4</b> K-means</a></li>
<li class="chapter" data-level="5.5.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-medoids"><i class="fa fa-check"></i><b>5.5.5</b> K-medoids</a></li>
<li class="chapter" data-level="5.5.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#gaussian-mixture-model"><i class="fa fa-check"></i><b>5.5.6</b> Gaussian mixture model</a></li>
<li class="chapter" data-level="5.5.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#t-sne-1"><i class="fa fa-check"></i><b>5.5.7</b> t-SNE</a></li>
<li class="chapter" data-level="5.5.8" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#umap-1"><i class="fa fa-check"></i><b>5.5.8</b> UMAP</a></li>
<li class="chapter" data-level="5.5.9" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#kohonen-map"><i class="fa fa-check"></i><b>5.5.9</b> Kohonen map</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/sxpyggy/Modern-Actuarial-Models/tree/modern-actuarial-models" target="blank">GitHub 仓库</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">现代精算统计模型</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsupervised-learning" class="section level1">
<h1><span class="header-section-number">5</span> 无监督学习方法</h1>
<p><em>梁译中、方明慧、王晗、高光远</em></p>
<p>大学及以后生活中最常用的学习方法。</p>
<p>无监督学习方法目的是降低数据（协变量，特征）维度、把案例按照相似特征聚类并以图形方式说明高维数据的技术。无监督学习不考虑响应变量，仅考虑这些特征本身的相似性。</p>
<p>本章将考虑以下几种方法：</p>
<ul>
<li><p><strong>降维</strong>：主成分分析（PCA）和瓶颈神经网络（BNN）。</p></li>
<li><p><strong>聚类</strong>：<span class="math inline">\(K\)</span>-means均值聚类，K-medoids聚类，高斯混合模型（GMM）</p></li>
<li><p><strong>可视化高维数据</strong>：变分自动编码器（VAE），t分布随机邻近嵌入（t-SNE），统一流形逼近和投影（UMAP），自组织映射（SOM）和Kohonen图。</p></li>
</ul>
<p>假设样本量为<span class="math inline">\(n\)</span>的样本有<span class="math inline">\(q\)</span>个特征<span class="math inline">\(\mathbf{x}_1^*,\ldots,\mathbf{x}^*_n\in\mathbb{R}^q\)</span>。其设计矩阵为
<span class="math display">\[\mathbf{X}=(\mathbf{x}_1^*,\ldots,\mathbf{x}^*_n)^\intercal\in\mathbb{R}^{n\times q}.\]</span>
把设计矩阵的每列进行标准化，得到
<span class="math display">\[\mathbf{X}=(x_{i,j})_{1\le i \le n,1\le j\le q}\in\mathbb{R}^{n\times q}.\]</span>
其中，第<span class="math inline">\(i\)</span>行是样本<span class="math inline">\(i\)</span>的特征<span class="math inline">\(\mathbf{x}_i\in\mathbb{R}^q, 1\le i\le n\)</span>, 第<span class="math inline">\(j\)</span>列是第<span class="math inline">\(j\)</span>个特征<span class="math inline">\(x_j\in\mathbb{R}^n\)</span>.</p>
<div id="主成分分析" class="section level2">
<h2><span class="header-section-number">5.1</span> 主成分分析</h2>
<p>简而言之，主成分分析的目的是降低高维数据的维数，使重构误差相对于原始数据最小。如果应用成功，它减少了特征空间的维数，并且它对于(精算)回归建模特别有用，因为它提供了少量的不相关的解释变量。</p>
<p>PCA适用于高斯分布，若变量显著不符合高斯分布，需要对数据进行预处理（比如取对数或其他方法）。</p>
<p>矩阵<span class="math inline">\(\mathbf{X}\)</span>的秩为<span class="math inline">\(q\le n\)</span>，可以找到<span class="math inline">\(q\)</span>个正交的<span class="math inline">\(q\)</span>维基向量<span class="math inline">\(\mathbf{v}_1,\ldots,\mathbf{v}_q\in\mathbb{R}^q\)</span>, 使得<span class="math inline">\(\mathbf{v}_1\)</span>为<span class="math inline">\(\mathbf{X}\)</span>波动最大的方向，<span class="math inline">\(\mathbf{v}_2\)</span>为与<span class="math inline">\(\mathbf{v}_1\)</span>正交方向上的<span class="math inline">\(\mathbf{X}\)</span>波动最大的方向，依次类推。</p>
<p>用数学公式表示如下：
<span class="math display">\[\mathbf{v}_1=\underset{||\omega||_2=1}{\arg \max}||\mathbf{X}\omega||_2^2=\underset{\omega^\intercal\omega=1}{\arg \max} (\omega^\intercal\mathbf{X}^\intercal\mathbf{X}\omega)\]</span></p>
<p><span class="math display">\[\mathbf{v}_2=\underset{||\omega||_2=1}{\arg \max}||\mathbf{X}\omega||_2^2 ~~~\text{ subject to } \mathbf{v}_1^\intercal\omega=0.\]</span></p>
<p><span class="math display">\[\ldots\]</span></p>
<p>主成分分析可通过以下两种方式实现</p>
<ul>
<li><p>求<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>或者<span class="math inline">\(\mathbf{X}\)</span>的协方差矩阵<span class="math inline">\(\mathbf{\Sigma}\)</span>的特征向量和特征值。易知<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}=n\times\mathbf{\Sigma}\)</span>，所以它们的特征向量相同。第一个特征向量即为<span class="math inline">\(\mathbf{v}_1\)</span>，第二个特征向量为<span class="math inline">\(\mathbf{v}_2\)</span>。前两个主成分为<span class="math inline">\(\mathbf{X}\mathbf{v}_1,\mathbf{X}\mathbf{v}_2\)</span></p></li>
<li><p>对<span class="math inline">\(\mathbf{X}\)</span>进行奇异值（singular value decomposition）分解:<span class="math display">\[\mathbf{X}=U\Lambda V^\intercal.\]</span>其中，对角矩阵<span class="math inline">\(\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_q)\)</span>的元素为<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>的特征值，<span class="math inline">\(V\)</span>为<span class="math inline">\(\mathbf{X}^\intercal \mathbf{X}\)</span>的特征向量。主成分可以通过<span class="math inline">\(\mathbf{X}V\)</span>求得。</p></li>
</ul>
<p>利用前<span class="math inline">\(p\)</span>个主成分可以重构设计矩阵的近似值<span class="math display">\[\mathbf{X}_p=U\text{diag}(\lambda_1,\ldots,\lambda_p,0,\ldots,0)V^{\intercal}.\]</span> 该近似值为以下极值问题的根<span class="math display">\[\underset{B\in\mathbb{R}^{n\times q}}{\arg \min}||\mathbf{X}-B||^2 ~~\text{subject to rank}(B)\le q,\]</span> 即矩阵<span class="math inline">\(\mathbf{X}_p\)</span>是所有秩为<span class="math inline">\(p\)</span>的矩阵中，与原始设计矩阵<span class="math inline">\(\mathbf{X}\)</span>重组平方误差(F范数)最小的矩阵。</p>
</div>
<div id="自编码" class="section level2">
<h2><span class="header-section-number">5.2</span> 自编码</h2>
<p>PCA对异常值很敏感，也有稳健的PCA版本。例如，Croux等人给出了一个基于中值绝对偏差(MADs)的算法，R包：pcaPP。
主成分分析可以看作是一个自动编码器。接下来，我们将更一般地介绍自动编码器，例如BNN。</p>
<p>自编码包含编码和解码两个镜面对称的映射：</p>
<ul>
<li><p>编码: <span class="math inline">\(\phi:\mathbb{R}^q\rightarrow\mathbb{R}^p\)</span></p></li>
<li><p>解码: <span class="math inline">\(\psi=\mathbb{R}^p\rightarrow\mathbb{R}^q\)</span></p></li>
</ul>
<p>作为非线性自编码器的一个例子，我们考虑瓶颈神经网络(BNN)。为了成功校准一个BNN，它的隐藏层数应该是奇数<span class="math inline">\(d\)</span> (<span class="math inline">\(d\)</span>称为神经网络的深度)，并且中心隐藏层应该是低维的，有<span class="math inline">\(p\)</span>个隐藏神经元，所有剩余的隐藏层应该是围绕这个中心隐藏层对称的。因此对于深度<span class="math inline">\(d = 3\)</span>的BNN，我们可以选择图<a href="unsupervised-learning.html#fig:bnn">5.1</a>展示的神经网络结构</p>
<div class="figure" style="text-align: center"><span id="fig:bnn"></span>
<img src="plots/5/bnn.png" alt="自编码 q=5,p=2" width="40%" />
<p class="caption">
Figure 5.1: 自编码 q=5,p=2
</p>
</div>
<div id="模型训练" class="section level3">
<h3><span class="header-section-number">5.2.1</span> 模型训练</h3>
<p>图<a href="unsupervised-learning.html#fig:bnn-train">5.2</a>展示了自编码的训练过程。</p>
<div class="figure" style="text-align: center"><span id="fig:bnn-train"></span>
<img src="plots/5/bnn_train.png" alt="自编码训练过程" width="60%"  />
<p class="caption">
Figure 5.2: 自编码训练过程
</p>
</div>
<p>具体过程如下：</p>
</div>
</div>
<div id="聚类" class="section level2">
<h2><span class="header-section-number">5.3</span> 聚类</h2>
<ol style="list-style-type: decimal">
<li><p>分层聚类：不需事先指定聚类个数 （1）自下而上：初始n类，再将相距最近的两类合并，建立一个新的类，直到最后合并成一类；（2）自上而下：初始1类，再将相距最远的样本分裂成两类，直到最后分裂成n个类。</p></li>
<li><p>基于质心的聚类: K-means, K-medoids</p></li>
<li><p>基于分布的聚类: Gaussian mixture models(GMMs)</p></li>
</ol>
<div id="k-means-clustering" class="section level3">
<h3><span class="header-section-number">5.3.1</span> K-means clustering</h3>
<p>K-means聚类是一种基于质心（centroid-based）的聚类方法，它将<span class="math inline">\(n\)</span>个样本点<span class="math inline">\(\mathbf{x}_i\in\mathcal{X}\subset\mathbb{R}^q\)</span>划分为<span class="math inline">\(K\)</span>个不相交的类:<span class="math display">\[\mathcal{C}_K:\mathbb{R}^q\rightarrow\mathcal{K}=\{1,\ldots,K\},~~\mathbf{x}\mapsto\mathcal{C}_K(\mathbf{x})\]</span>，以上给出了对特征空间<span class="math inline">\(\mathcal{X}\)</span>的一个分割<span class="math inline">\((C_1,\ldots,C_K)\)</span>，其中<span class="math display">\[C_k=\{\mathbf{x}\in\mathcal{X};\mathcal{C}_K(\mathbf{x})=k\}\]</span></p>
<p>确定<span class="math inline">\(\mathcal{C}_K\)</span>的原则是使总类内差异最小，这可以转化为计算使类内离差平方和总和最小的一个分割，所构造的目标函数为 ：
<span class="math display">\[\underset{(C_1,\ldots,C_K)}{\arg \min}\sum_{k=1}^K\sum_{\mathbf{x}_i\in C_k\cap\mathcal{X}}d(\mathbf{\mu}_k,\mathbf{x}_i)=\underset{(C_1,\ldots,C_K)}{\arg \min}\sum_{k=1}^K\sum_{\mathbf{x}_i\in C_k\cap\mathcal{X}}||\mathbf{\mu}_k,\mathbf{x}_i||_2^2\]</span></p>
<p>其中<span class="math inline">\(\mathbf{\mu}_k\)</span>为类均值向量，因此目标函数衡量了类内样本点围绕类均值向量的紧密程度，其值越小意味着类内样本相似度越高，聚类效果越好。但是上述目标函数并不容易找到最优解，这需要考虑<span class="math inline">\(n\)</span>个样本点所有可能的类划分，因此K-means算法采用了贪心策略，通过迭代优化来近似求解上述目标函数。</p>
</div>
<div id="k-medoids-clustering-pam" class="section level3">
<h3><span class="header-section-number">5.3.2</span> K-medoids clustering (PAM)</h3>
</div>
<div id="gaussian-mixture-modelsgmms" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Gaussian mixture models(GMMs)</h3>
</div>
</div>
<div id="低维可视化" class="section level2">
<h2><span class="header-section-number">5.4</span> 低维可视化</h2>
<div id="t-sne" class="section level3">
<h3><span class="header-section-number">5.4.1</span> t-SNE</h3>
<p>t分布-随机邻近嵌入(t-SNE, t-distributed stochastic neighbor embedding)由 Laurens van der Maaten和Geoffrey Hinton在2008年提出。t-SNE本质是一种嵌入模型，能够将高维空间中的数据映射到低维空间中，并保留数据集的局部特性。</p>
<p>基本原理
t-SNE将数据点之间的相似度转化为条件概率，原始空间中数据点的相似度由高斯联合分布表示，嵌入空间中数据点的相似度由t分布表示。
将原始空间和嵌入空间的联合概率分布的KL散度作为损失函数(loss function)，评估嵌入效果的好坏。通过梯度下降算法最小化损失函数，最终获得收敛结果。</p>
</div>
<div id="umap" class="section level3">
<h3><span class="header-section-number">5.4.2</span> UMAP</h3>
<p>统一流形逼近与投影(UMAP, Uniform Manifold Approximation and Projection)是建立在黎曼几何和代数拓扑理论框架上的新的降维流形学习技术。在可视化质量方面，UMAP算法与t-SNE具有竞争优势，但是它保留了更多全局结构、具有优越的运行性能、更好的可扩展性。</p>
<p>基本原理</p>
<ol style="list-style-type: decimal">
<li><p>计算高维的流形结构特征，确定高维空间中各个点之间的距离，从而构造高维的数据分布结构。</p></li>
<li><p>将它们投影到低维空间，根据高维空间点与点之间的相对关系，提取特征值，在低维空间中重构这种距离关系，并计算低维空间中各个点之间的距离。</p></li>
</ol>
<p>3。 使用随机梯度下降来最小化这些距离之间的差异。</p>
</div>
<div id="som" class="section level3">
<h3><span class="header-section-number">5.4.3</span> SOM</h3>
<p>自组织映射(Self-organizing map, SOM)是一种竞争型神经网络，由输入层和竞争层（常见2维）构成。</p>
<ul>
<li><p><strong>输入层</strong>神经元的数量由输入向量的维度决定，一个神经元对应一个特征</p></li>
<li><p><strong>竞争层</strong>的常见结构：矩形(Rectangular)、六边形(Hexagonal) 竞争层神经元的数量决定了最终模型的粒度与规模，对最终模型的准确性与泛化能力影响很大。</p></li>
<li><p><strong>基本原理</strong>：运用竞争学习(competitive learning)策略，竞争层各神经元竞争对输入层响应的机会，最后仅有一个神经元获胜，代表对输入层的分类。如此迭代，逐步优化网络。</p></li>
</ul>
<p>图<a href="unsupervised-learning.html#fig:som">5.3</a>展示了SOM的结构。</p>
<div class="figure" style="text-align: center"><span id="fig:som"></span>
<img src="plots/5/som.png" alt="SOM" width="60%"  />
<p class="caption">
Figure 5.3: SOM
</p>
</div>
</div>
</div>
<div id="case-study-1" class="section level2">
<h2><span class="header-section-number">5.5</span> Case study</h2>
<table>
<thead>
<tr class="header">
<th align="center">变量</th>
<th align="center">类型</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">brand</td>
<td align="center">factor</td>
<td>43个汽车品牌</td>
</tr>
<tr class="even">
<td align="center">type</td>
<td align="center">factor</td>
<td>96个水平</td>
</tr>
<tr class="odd">
<td align="center">model</td>
<td align="center">factor</td>
<td>113个水平</td>
</tr>
<tr class="even">
<td align="center">seats</td>
<td align="center">int</td>
<td>座位数</td>
</tr>
<tr class="odd">
<td align="center">max_power</td>
<td align="center">int</td>
<td>发动机最大功率(kW),取对数</td>
</tr>
<tr class="even">
<td align="center">max_torque</td>
<td align="center">num</td>
<td>最大转矩(Nm),取对数</td>
</tr>
<tr class="odd">
<td align="center">cubic_capacity</td>
<td align="center">int</td>
<td>容量(cm<span class="math inline">\(^3\)</span>),取对数</td>
</tr>
<tr class="even">
<td align="center">weight</td>
<td align="center">int</td>
<td>车重(kg)，取对数</td>
</tr>
<tr class="odd">
<td align="center">max_engine_speed</td>
<td align="center">int</td>
<td>发动机最大转速(rpm)</td>
</tr>
<tr class="even">
<td align="center">seconds_to_100</td>
<td align="center">int</td>
<td>达到100km/h所需要秒数</td>
</tr>
<tr class="odd">
<td align="center">top_speed</td>
<td align="center">int</td>
<td>最大行驶速度(km/h)</td>
</tr>
<tr class="even">
<td align="center">sports_car</td>
<td align="center">int</td>
<td>跑车</td>
</tr>
<tr class="odd">
<td align="center">tau</td>
<td align="center">num</td>
<td>专家评分</td>
</tr>
</tbody>
</table>
<div id="数据预处理-3" class="section level3">
<h3><span class="header-section-number">5.5.1</span> 数据预处理</h3>
</div>
<div id="主成分分析-1" class="section level3">
<h3><span class="header-section-number">5.5.2</span> 主成分分析</h3>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="unsupervised-learning.html#cb65-1"></a><span class="co"># standardize matrix</span></span>
<span id="cb65-2"><a href="unsupervised-learning.html#cb65-2"></a>X &lt;-<span class="st"> </span>X01<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">colMeans</span>(X01<span class="op">^</span><span class="dv">2</span>))[<span class="kw">col</span>(X01)]</span>
<span id="cb65-3"><a href="unsupervised-learning.html#cb65-3"></a></span>
<span id="cb65-4"><a href="unsupervised-learning.html#cb65-4"></a><span class="co"># eigenvectors and eigenvalues</span></span>
<span id="cb65-5"><a href="unsupervised-learning.html#cb65-5"></a>X1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X)</span>
<span id="cb65-6"><a href="unsupervised-learning.html#cb65-6"></a><span class="kw">nrow</span>(X1)</span>
<span id="cb65-7"><a href="unsupervised-learning.html#cb65-7"></a>A &lt;-<span class="st">  </span><span class="kw">t</span>(X1) <span class="op">%*%</span><span class="st"> </span>X1</span>
<span id="cb65-8"><a href="unsupervised-learning.html#cb65-8"></a>A</span>
<span id="cb65-9"><a href="unsupervised-learning.html#cb65-9"></a><span class="kw">sum</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value)<span class="op">/</span><span class="dv">5</span></span>
<span id="cb65-10"><a href="unsupervised-learning.html#cb65-10"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value)      <span class="co"># singular values</span></span>
<span id="cb65-11"><a href="unsupervised-learning.html#cb65-11"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A)<span class="op">$</span>value<span class="op">/</span><span class="kw">nrow</span>(X1))   <span class="co"># scaled eigenvalues</span></span>
<span id="cb65-12"><a href="unsupervised-learning.html#cb65-12"></a><span class="kw">eigen</span>(A)<span class="op">$</span>vector</span>
<span id="cb65-13"><a href="unsupervised-learning.html#cb65-13"></a>A1&lt;-<span class="kw">cor</span>(X1)</span>
<span id="cb65-14"><a href="unsupervised-learning.html#cb65-14"></a>A1<span class="op">*</span><span class="kw">nrow</span>(X1)</span>
<span id="cb65-15"><a href="unsupervised-learning.html#cb65-15"></a><span class="kw">sqrt</span>(<span class="kw">eigen</span>(A1)<span class="op">$</span>value)  </span>
<span id="cb65-16"><a href="unsupervised-learning.html#cb65-16"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>vector</span>
<span id="cb65-17"><a href="unsupervised-learning.html#cb65-17"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>value</span>
<span id="cb65-18"><a href="unsupervised-learning.html#cb65-18"></a></span>
<span id="cb65-19"><a href="unsupervised-learning.html#cb65-19"></a><span class="co"># singular value decomposition</span></span>
<span id="cb65-20"><a href="unsupervised-learning.html#cb65-20"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(X1)</span>
<span id="cb65-21"><a href="unsupervised-learning.html#cb65-21"></a>SVD<span class="op">$</span>d                       <span class="co"># singular values</span></span>
<span id="cb65-22"><a href="unsupervised-learning.html#cb65-22"></a><span class="kw">rbind</span>(SVD<span class="op">$</span>v[,<span class="dv">1</span>],SVD<span class="op">$</span>v[,<span class="dv">2</span>])  <span class="co"># first two right singular vectors</span></span>
<span id="cb65-23"><a href="unsupervised-learning.html#cb65-23"></a></span>
<span id="cb65-24"><a href="unsupervised-learning.html#cb65-24"></a><span class="co"># PCA with package PCA</span></span>
<span id="cb65-25"><a href="unsupervised-learning.html#cb65-25"></a>t.pca &lt;-<span class="st"> </span><span class="kw">princomp</span>(X1,<span class="dt">cor=</span><span class="ot">TRUE</span>)</span>
<span id="cb65-26"><a href="unsupervised-learning.html#cb65-26"></a>t.pca<span class="op">$</span>loadings          </span>
<span id="cb65-27"><a href="unsupervised-learning.html#cb65-27"></a><span class="kw">summary</span>(t.pca)</span>
<span id="cb65-28"><a href="unsupervised-learning.html#cb65-28"></a><span class="kw">eigen</span>(A1)<span class="op">$</span>value<span class="op">/</span><span class="kw">sum</span>(<span class="kw">eigen</span>(A1)<span class="op">$</span>value)</span>
<span id="cb65-29"><a href="unsupervised-learning.html#cb65-29"></a></span>
<span id="cb65-30"><a href="unsupervised-learning.html#cb65-30"></a><span class="co"># PCA Sports Cars weights</span></span>
<span id="cb65-31"><a href="unsupervised-learning.html#cb65-31"></a>alpha &lt;-<span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">1</span>]<span class="op">/</span>sds</span>
<span id="cb65-32"><a href="unsupervised-learning.html#cb65-32"></a>(alpha_star &lt;-<span class="st"> </span><span class="kw">c</span>(alpha[<span class="dv">1</span>],alpha[<span class="dv">2</span>]<span class="op">-</span>alpha[<span class="dv">1</span>], alpha[<span class="dv">3</span>], alpha[<span class="dv">4</span>], alpha[<span class="dv">5</span>]<span class="op">-</span>alpha[<span class="dv">2</span>])<span class="op">/</span>alpha[<span class="dv">1</span>])</span>
<span id="cb65-33"><a href="unsupervised-learning.html#cb65-33"></a></span>
<span id="cb65-34"><a href="unsupervised-learning.html#cb65-34"></a><span class="co"># plot first two principal components</span></span>
<span id="cb65-35"><a href="unsupervised-learning.html#cb65-35"></a>dat3 &lt;-<span class="st"> </span>d.data </span>
<span id="cb65-36"><a href="unsupervised-learning.html#cb65-36"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span>X1 <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">1</span>]</span>
<span id="cb65-37"><a href="unsupervised-learning.html#cb65-37"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span>X1 <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">2</span>]</span>
<span id="cb65-38"><a href="unsupervised-learning.html#cb65-38"></a></span>
<span id="cb65-39"><a href="unsupervised-learning.html#cb65-39"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">ylab=</span><span class="st">&quot;2nd principal component&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;1st principal component&quot;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;principal components analysis&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb65-40"><a href="unsupervised-learning.html#cb65-40"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(dat3<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),]</span>
<span id="cb65-41"><a href="unsupervised-learning.html#cb65-41"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb65-42"><a href="unsupervised-learning.html#cb65-42"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(dat3<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),]</span>
<span id="cb65-43"><a href="unsupervised-learning.html#cb65-43"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb65-44"><a href="unsupervised-learning.html#cb65-44"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;tau&gt;=21&quot;</span>, <span class="st">&quot;17&lt;=tau&lt;21&quot;</span>, <span class="st">&quot;tau&lt;17 (sports car)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb65-45"><a href="unsupervised-learning.html#cb65-45"></a></span>
<span id="cb65-46"><a href="unsupervised-learning.html#cb65-46"></a><span class="co"># reconstruction error</span></span>
<span id="cb65-47"><a href="unsupervised-learning.html#cb65-47"></a>reconstruction.PCA &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(<span class="dv">5</span>))</span>
<span id="cb65-48"><a href="unsupervised-learning.html#cb65-48"></a></span>
<span id="cb65-49"><a href="unsupervised-learning.html#cb65-49"></a><span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>){</span>
<span id="cb65-50"><a href="unsupervised-learning.html#cb65-50"></a>  Xp &lt;-<span class="st"> </span>SVD<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>p] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(SVD<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span>p]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X)</span>
<span id="cb65-51"><a href="unsupervised-learning.html#cb65-51"></a>  Xp &lt;-<span class="st"> </span><span class="kw">t</span>(Xp)</span>
<span id="cb65-52"><a href="unsupervised-learning.html#cb65-52"></a>  reconstruction.PCA[p] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(<span class="kw">as.matrix</span>((X<span class="op">-</span>Xp)<span class="op">^</span><span class="dv">2</span>))<span class="op">/</span><span class="kw">nrow</span>(X))</span>
<span id="cb65-53"><a href="unsupervised-learning.html#cb65-53"></a>               }</span>
<span id="cb65-54"><a href="unsupervised-learning.html#cb65-54"></a><span class="kw">round</span>(reconstruction.PCA,<span class="dv">2</span>)               </span>
<span id="cb65-55"><a href="unsupervised-learning.html#cb65-55"></a></span>
<span id="cb65-56"><a href="unsupervised-learning.html#cb65-56"></a><span class="co"># scatter plot</span></span>
<span id="cb65-57"><a href="unsupervised-learning.html#cb65-57"></a>switch_sign &lt;-<span class="st"> </span><span class="dv">-1</span>           <span class="co"># switch sign of the first component to make svd and princomp compatible</span></span>
<span id="cb65-58"><a href="unsupervised-learning.html#cb65-58"></a>tt.pca &lt;-<span class="st"> </span>t.pca<span class="op">$</span>scores</span>
<span id="cb65-59"><a href="unsupervised-learning.html#cb65-59"></a>tt.pca[,<span class="dv">1</span>] &lt;-<span class="st"> </span>switch_sign <span class="op">*</span>tt.pca[,<span class="dv">1</span>]</span>
<span id="cb65-60"><a href="unsupervised-learning.html#cb65-60"></a><span class="kw">pairs</span>(tt.pca,<span class="dt">diag.panel=</span>panel.qq,<span class="dt">upper.panel=</span>panel.cor)</span>
<span id="cb65-61"><a href="unsupervised-learning.html#cb65-61"></a></span>
<span id="cb65-62"><a href="unsupervised-learning.html#cb65-62"></a><span class="co"># biplot</span></span>
<span id="cb65-63"><a href="unsupervised-learning.html#cb65-63"></a>tt.pca &lt;-<span class="st"> </span>t.pca</span>
<span id="cb65-64"><a href="unsupervised-learning.html#cb65-64"></a>tt.pca<span class="op">$</span>scores[,<span class="dv">1</span>] &lt;-<span class="st">  </span>switch_sign <span class="op">*</span><span class="st"> </span>tt.pca<span class="op">$</span>scores[,<span class="dv">1</span>]</span>
<span id="cb65-65"><a href="unsupervised-learning.html#cb65-65"></a>tt.pca<span class="op">$</span>loadings[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span>] &lt;-<span class="st"> </span>switch_sign <span class="op">*</span><span class="st"> </span>tt.pca<span class="op">$</span>loadings[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">1</span>] </span>
<span id="cb65-66"><a href="unsupervised-learning.html#cb65-66"></a><span class="kw">biplot</span>(tt.pca,<span class="dt">choices=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>),<span class="dt">scale=</span><span class="dv">0</span>, <span class="dt">expand=</span><span class="dv">2</span>, <span class="dt">xlab=</span><span class="st">&quot;1st principal component&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;2nd principal component&quot;</span>, <span class="dt">cex=</span><span class="kw">c</span>(<span class="fl">0.4</span>,<span class="fl">1.5</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>,<span class="dv">7</span>))</span></code></pre></div>
</div>
<div id="自编码-1" class="section level3">
<h3><span class="header-section-number">5.5.3</span> 自编码</h3>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="unsupervised-learning.html#cb66-1"></a>bottleneck<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="cf">function</span>(q00, q22){</span>
<span id="cb66-2"><a href="unsupervised-learning.html#cb66-2"></a>   Input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(q00), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;Input&#39;</span>)</span>
<span id="cb66-3"><a href="unsupervised-learning.html#cb66-3"></a>   </span>
<span id="cb66-4"><a href="unsupervised-learning.html#cb66-4"></a>   Output =<span class="st"> </span>Input <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb66-5"><a href="unsupervised-learning.html#cb66-5"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q22, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Bottleneck&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb66-6"><a href="unsupervised-learning.html#cb66-6"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q00, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Output&#39;</span>)</span>
<span id="cb66-7"><a href="unsupervised-learning.html#cb66-7"></a></span>
<span id="cb66-8"><a href="unsupervised-learning.html#cb66-8"></a>   model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> Input, <span class="dt">outputs =</span> Output)</span>
<span id="cb66-9"><a href="unsupervised-learning.html#cb66-9"></a>   </span>
<span id="cb66-10"><a href="unsupervised-learning.html#cb66-10"></a>   model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(<span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(), <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>)</span>
<span id="cb66-11"><a href="unsupervised-learning.html#cb66-11"></a>   model</span>
<span id="cb66-12"><a href="unsupervised-learning.html#cb66-12"></a>   }</span>
<span id="cb66-13"><a href="unsupervised-learning.html#cb66-13"></a></span>
<span id="cb66-14"><a href="unsupervised-learning.html#cb66-14"></a>bottleneck<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="cf">function</span>(q00, q11, q22){   </span>
<span id="cb66-15"><a href="unsupervised-learning.html#cb66-15"></a>   Input &lt;-<span class="st"> </span><span class="kw">layer_input</span>(<span class="dt">shape =</span> <span class="kw">c</span>(q00), <span class="dt">dtype =</span> <span class="st">&#39;float32&#39;</span>, <span class="dt">name =</span> <span class="st">&#39;Input&#39;</span>)</span>
<span id="cb66-16"><a href="unsupervised-learning.html#cb66-16"></a>   </span>
<span id="cb66-17"><a href="unsupervised-learning.html#cb66-17"></a>   Encoder =<span class="st"> </span>Input <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb66-18"><a href="unsupervised-learning.html#cb66-18"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q11, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Layer1&#39;</span>) <span class="op">%&gt;%</span></span>
<span id="cb66-19"><a href="unsupervised-learning.html#cb66-19"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q22, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Bottleneck&#39;</span>) </span>
<span id="cb66-20"><a href="unsupervised-learning.html#cb66-20"></a></span>
<span id="cb66-21"><a href="unsupervised-learning.html#cb66-21"></a>   Decoder =<span class="st"> </span>Encoder <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb66-22"><a href="unsupervised-learning.html#cb66-22"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q11, <span class="dt">activation=</span><span class="st">&#39;tanh&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Layer3&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb66-23"><a href="unsupervised-learning.html#cb66-23"></a><span class="st">          </span><span class="kw">layer_dense</span>(<span class="dt">units=</span>q00, <span class="dt">activation=</span><span class="st">&#39;linear&#39;</span>, <span class="dt">use_bias=</span><span class="ot">FALSE</span>, <span class="dt">name=</span><span class="st">&#39;Output&#39;</span>)</span>
<span id="cb66-24"><a href="unsupervised-learning.html#cb66-24"></a></span>
<span id="cb66-25"><a href="unsupervised-learning.html#cb66-25"></a>   model &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs =</span> Input, <span class="dt">outputs =</span> Decoder)</span>
<span id="cb66-26"><a href="unsupervised-learning.html#cb66-26"></a>   model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(<span class="dt">optimizer =</span> <span class="kw">optimizer_nadam</span>(), <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>)</span>
<span id="cb66-27"><a href="unsupervised-learning.html#cb66-27"></a>   model</span>
<span id="cb66-28"><a href="unsupervised-learning.html#cb66-28"></a>   }</span>
<span id="cb66-29"><a href="unsupervised-learning.html#cb66-29"></a></span>
<span id="cb66-30"><a href="unsupervised-learning.html#cb66-30"></a><span class="co"># bottleneck architecture</span></span>
<span id="cb66-31"><a href="unsupervised-learning.html#cb66-31"></a>q1 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb66-32"><a href="unsupervised-learning.html#cb66-32"></a>q2 &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb66-33"><a href="unsupervised-learning.html#cb66-33"></a>q0 &lt;-<span class="st"> </span><span class="kw">ncol</span>(X)</span>
<span id="cb66-34"><a href="unsupervised-learning.html#cb66-34"></a></span>
<span id="cb66-35"><a href="unsupervised-learning.html#cb66-35"></a><span class="co"># pre-training 1: merging layers 1 and 3 (skipping bottleneck)</span></span>
<span id="cb66-36"><a href="unsupervised-learning.html#cb66-36"></a>model<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.1</span>(q0, q1)</span>
<span id="cb66-37"><a href="unsupervised-learning.html#cb66-37"></a>model<span class="fl">.1</span></span>
<span id="cb66-38"><a href="unsupervised-learning.html#cb66-38"></a>epochs &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb66-39"><a href="unsupervised-learning.html#cb66-39"></a>batch_size &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb66-40"><a href="unsupervised-learning.html#cb66-40"></a></span>
<span id="cb66-41"><a href="unsupervised-learning.html#cb66-41"></a><span class="co"># fit the merged model</span></span>
<span id="cb66-42"><a href="unsupervised-learning.html#cb66-42"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb66-43"><a href="unsupervised-learning.html#cb66-43"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.1</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(X), <span class="kw">as.matrix</span>(X), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb66-44"><a href="unsupervised-learning.html#cb66-44"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb66-45"><a href="unsupervised-learning.html#cb66-45"></a></span>
<span id="cb66-46"><a href="unsupervised-learning.html#cb66-46"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0),  <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="st">&quot;gradient descent algorithm&quot;</span>) </span>
<span id="cb66-47"><a href="unsupervised-learning.html#cb66-47"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="fl">0.6124</span>), <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>)</span>
<span id="cb66-48"><a href="unsupervised-learning.html#cb66-48"></a> </span>
<span id="cb66-49"><a href="unsupervised-learning.html#cb66-49"></a><span class="co"># neuron activations in the central layer </span></span>
<span id="cb66-50"><a href="unsupervised-learning.html#cb66-50"></a>zz &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs=</span>model<span class="fl">.1</span><span class="op">$</span>input, <span class="dt">outputs=</span><span class="kw">get_layer</span>(model<span class="fl">.1</span>, <span class="st">&#39;Bottleneck&#39;</span>)<span class="op">$</span>output)</span>
<span id="cb66-51"><a href="unsupervised-learning.html#cb66-51"></a>yy &lt;-<span class="st"> </span>zz <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb66-52"><a href="unsupervised-learning.html#cb66-52"></a></span>
<span id="cb66-53"><a href="unsupervised-learning.html#cb66-53"></a><span class="co"># pre-training 2: middlepart</span></span>
<span id="cb66-54"><a href="unsupervised-learning.html#cb66-54"></a>model<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.1</span>(q1, q2)</span>
<span id="cb66-55"><a href="unsupervised-learning.html#cb66-55"></a>model<span class="fl">.2</span></span>
<span id="cb66-56"><a href="unsupervised-learning.html#cb66-56"></a>epochs &lt;-<span class="st"> </span><span class="dv">2000</span></span>
<span id="cb66-57"><a href="unsupervised-learning.html#cb66-57"></a></span>
<span id="cb66-58"><a href="unsupervised-learning.html#cb66-58"></a><span class="co"># fit the merged model</span></span>
<span id="cb66-59"><a href="unsupervised-learning.html#cb66-59"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb66-60"><a href="unsupervised-learning.html#cb66-60"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.2</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(yy), <span class="kw">as.matrix</span>(yy), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb66-61"><a href="unsupervised-learning.html#cb66-61"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb66-62"><a href="unsupervised-learning.html#cb66-62"></a></span>
<span id="cb66-63"><a href="unsupervised-learning.html#cb66-63"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0),  <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="st">&quot;gradient descent algorithm&quot;</span>) </span>
<span id="cb66-64"><a href="unsupervised-learning.html#cb66-64"></a></span>
<span id="cb66-65"><a href="unsupervised-learning.html#cb66-65"></a><span class="co"># fitting the full model</span></span>
<span id="cb66-66"><a href="unsupervised-learning.html#cb66-66"></a>model<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">bottleneck.3</span>(q0, q1, q2)</span>
<span id="cb66-67"><a href="unsupervised-learning.html#cb66-67"></a>model<span class="fl">.3</span></span>
<span id="cb66-68"><a href="unsupervised-learning.html#cb66-68"></a></span>
<span id="cb66-69"><a href="unsupervised-learning.html#cb66-69"></a><span class="co"># set weights</span></span>
<span id="cb66-70"><a href="unsupervised-learning.html#cb66-70"></a>weight<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.3</span>)</span>
<span id="cb66-71"><a href="unsupervised-learning.html#cb66-71"></a>weight<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.1</span>)</span>
<span id="cb66-72"><a href="unsupervised-learning.html#cb66-72"></a>weight<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">get_weights</span>(model<span class="fl">.2</span>)</span>
<span id="cb66-73"><a href="unsupervised-learning.html#cb66-73"></a>weight<span class="fl">.3</span>[[<span class="dv">1</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.1</span>[[<span class="dv">1</span>]]</span>
<span id="cb66-74"><a href="unsupervised-learning.html#cb66-74"></a>weight<span class="fl">.3</span>[[<span class="dv">4</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.1</span>[[<span class="dv">2</span>]]</span>
<span id="cb66-75"><a href="unsupervised-learning.html#cb66-75"></a>weight<span class="fl">.3</span>[[<span class="dv">2</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.2</span>[[<span class="dv">1</span>]]</span>
<span id="cb66-76"><a href="unsupervised-learning.html#cb66-76"></a>weight<span class="fl">.3</span>[[<span class="dv">3</span>]] &lt;-<span class="st"> </span>weight<span class="fl">.2</span>[[<span class="dv">2</span>]]</span>
<span id="cb66-77"><a href="unsupervised-learning.html#cb66-77"></a><span class="kw">set_weights</span>(model<span class="fl">.3</span>, weight<span class="fl">.3</span>)</span>
<span id="cb66-78"><a href="unsupervised-learning.html#cb66-78"></a>fit0 &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb66-79"><a href="unsupervised-learning.html#cb66-79"></a></span>
<span id="cb66-80"><a href="unsupervised-learning.html#cb66-80"></a><span class="co"># reconstruction error of the pre-calibrated network</span></span>
<span id="cb66-81"><a href="unsupervised-learning.html#cb66-81"></a><span class="co"># note that this error may differ from the tutorial because we did not set a seed</span></span>
<span id="cb66-82"><a href="unsupervised-learning.html#cb66-82"></a><span class="kw">round</span>(<span class="kw">Frobenius.loss</span>(X,fit0),<span class="dv">4</span>)</span>
<span id="cb66-83"><a href="unsupervised-learning.html#cb66-83"></a></span>
<span id="cb66-84"><a href="unsupervised-learning.html#cb66-84"></a><span class="co"># calibrate full bottleneck network</span></span>
<span id="cb66-85"><a href="unsupervised-learning.html#cb66-85"></a>epochs &lt;-<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb66-86"><a href="unsupervised-learning.html#cb66-86"></a>batch_size &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)</span>
<span id="cb66-87"><a href="unsupervised-learning.html#cb66-87"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb66-88"><a href="unsupervised-learning.html#cb66-88"></a>  fit &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(<span class="kw">as.matrix</span>(X), <span class="kw">as.matrix</span>(X), <span class="dt">epochs=</span>epochs, <span class="dt">batch_size=</span>batch_size, <span class="dt">verbose=</span><span class="dv">0</span>)</span>
<span id="cb66-89"><a href="unsupervised-learning.html#cb66-89"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb66-90"><a href="unsupervised-learning.html#cb66-90"></a></span>
<span id="cb66-91"><a href="unsupervised-learning.html#cb66-91"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss)), <span class="dt">y=</span><span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="kw">max</span>(<span class="kw">sqrt</span>(fit[[<span class="dv">2</span>]]<span class="op">$</span>loss<span class="op">*</span>q0))),<span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span>.<span class="dv">5</span>, <span class="dt">xlab=</span><span class="st">&#39;epochs&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Frobenius norm loss&#39;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;gradient descent algorithm&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>) </span>
<span id="cb66-92"><a href="unsupervised-learning.html#cb66-92"></a><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="fl">0.6124</span>), <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>) </span>
<span id="cb66-93"><a href="unsupervised-learning.html#cb66-93"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;decrease GDM&quot;</span>, <span class="st">&quot;PCA(p=2)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;orange&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">19</span>,<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb66-94"><a href="unsupervised-learning.html#cb66-94"></a></span>
<span id="cb66-95"><a href="unsupervised-learning.html#cb66-95"></a><span class="co"># reconstruction error (slightly differs from 0.5611 because of missing seed)</span></span>
<span id="cb66-96"><a href="unsupervised-learning.html#cb66-96"></a>fit0 &lt;-<span class="st"> </span>model<span class="fl">.3</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">predict</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb66-97"><a href="unsupervised-learning.html#cb66-97"></a><span class="kw">round</span>(<span class="kw">Frobenius.loss</span>(X,fit0),<span class="dv">4</span>)</span>
<span id="cb66-98"><a href="unsupervised-learning.html#cb66-98"></a></span>
<span id="cb66-99"><a href="unsupervised-learning.html#cb66-99"></a><span class="co"># read off the bottleneck activations</span></span>
<span id="cb66-100"><a href="unsupervised-learning.html#cb66-100"></a>encoder &lt;-<span class="st"> </span><span class="kw">keras_model</span>(<span class="dt">inputs=</span>model<span class="fl">.3</span><span class="op">$</span>input, <span class="dt">outputs=</span><span class="kw">get_layer</span>(model<span class="fl">.3</span>, <span class="st">&#39;Bottleneck&#39;</span>)<span class="op">$</span>output)</span>
<span id="cb66-101"><a href="unsupervised-learning.html#cb66-101"></a>y&lt;-<span class="st"> </span><span class="kw">predict</span>(encoder,<span class="kw">as.matrix</span>(X))</span>
<span id="cb66-102"><a href="unsupervised-learning.html#cb66-102"></a>y0 &lt;-<span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(y))<span class="op">*</span><span class="fl">1.1</span></span>
<span id="cb66-103"><a href="unsupervised-learning.html#cb66-103"></a></span>
<span id="cb66-104"><a href="unsupervised-learning.html#cb66-104"></a><span class="co"># note that we may need sign switches to make it comparable to PCA</span></span>
<span id="cb66-105"><a href="unsupervised-learning.html#cb66-105"></a><span class="kw">plot</span>(<span class="dt">x=</span>y[,<span class="dv">1</span>], <span class="dt">y=</span>y[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>y0,y0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>y0,y0), <span class="dt">ylab=</span><span class="st">&quot;2nd bottleneck neuron&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;1st bottleneck neuron&quot;</span>, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;bottleneck neural network autoencoder&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb66-106"><a href="unsupervised-learning.html#cb66-106"></a>dat0 &lt;-<span class="st"> </span>y[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),]</span>
<span id="cb66-107"><a href="unsupervised-learning.html#cb66-107"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0[,<span class="dv">1</span>], <span class="dt">y=</span>dat0[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb66-108"><a href="unsupervised-learning.html#cb66-108"></a>dat0 &lt;-<span class="st"> </span>y[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),]</span>
<span id="cb66-109"><a href="unsupervised-learning.html#cb66-109"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0[,<span class="dv">1</span>], <span class="dt">y=</span>dat0[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb66-110"><a href="unsupervised-learning.html#cb66-110"></a><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;tau&gt;=21&quot;</span>, <span class="st">&quot;17&lt;=tau&lt;21&quot;</span>, <span class="st">&quot;tau&lt;17 (sports car)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
<p>从这个图中我们可以得出结论，两种方法都得到了相似的结果。
但在一般情况下，BNN的重构误差较小。在少数情况下，BNN可以得到更好的重建结果(右下角的橙色点)。
这个例子的主要结论是，对于低维数的问题，主成分分析通常就可以了，因为非线性的部分并没有发挥关键作用。</p>
</div>
<div id="k-means" class="section level3">
<h3><span class="header-section-number">5.5.4</span> K-means</h3>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="unsupervised-learning.html#cb67-1"></a><span class="co"># initialize</span></span>
<span id="cb67-2"><a href="unsupervised-learning.html#cb67-2"></a>Kaverage &lt;-<span class="st"> </span><span class="kw">colMeans</span>(X)</span>
<span id="cb67-3"><a href="unsupervised-learning.html#cb67-3"></a>K0 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb67-4"><a href="unsupervised-learning.html#cb67-4"></a>TWCD &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(K0))  <span class="co"># total within-cluster dissimilarity</span></span>
<span id="cb67-5"><a href="unsupervised-learning.html#cb67-5"></a>Classifier &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="dv">1</span>, <span class="kw">c</span>(K0, <span class="kw">nrow</span>(X)))</span>
<span id="cb67-6"><a href="unsupervised-learning.html#cb67-6"></a>(TWCD[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">sum</span>(<span class="kw">colSums</span>(<span class="kw">as.matrix</span>(X<span class="op">^</span><span class="dv">2</span>))))</span>
<span id="cb67-7"><a href="unsupervised-learning.html#cb67-7"></a></span>
<span id="cb67-8"><a href="unsupervised-learning.html#cb67-8"></a><span class="co"># run K-means algorithm</span></span>
<span id="cb67-9"><a href="unsupervised-learning.html#cb67-9"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb67-10"><a href="unsupervised-learning.html#cb67-10"></a><span class="cf">for</span> (K <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>K0){ </span>
<span id="cb67-11"><a href="unsupervised-learning.html#cb67-11"></a>   <span class="cf">if</span> (K<span class="op">==</span><span class="dv">2</span>){(K_res &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X,K) )}</span>
<span id="cb67-12"><a href="unsupervised-learning.html#cb67-12"></a>   <span class="cf">if</span> (K<span class="op">&gt;</span><span class="dv">2</span>){(K_res  &lt;-<span class="st"> </span><span class="kw">kmeans</span>(X,K_centers) )}</span>
<span id="cb67-13"><a href="unsupervised-learning.html#cb67-13"></a>   TWCD[K] &lt;-<span class="st"> </span><span class="kw">sum</span>(K_res<span class="op">$</span>withins)</span>
<span id="cb67-14"><a href="unsupervised-learning.html#cb67-14"></a>   Classifier[K,] &lt;-<span class="st"> </span>K_res<span class="op">$</span>cluster</span>
<span id="cb67-15"><a href="unsupervised-learning.html#cb67-15"></a>   K_centers &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="ot">NA</span>, <span class="kw">c</span>(K<span class="op">+</span><span class="dv">1</span>, <span class="kw">ncol</span>(X)))</span>
<span id="cb67-16"><a href="unsupervised-learning.html#cb67-16"></a>   K_centers[K<span class="op">+</span><span class="dv">1</span>,] &lt;-<span class="st"> </span>Kaverage</span>
<span id="cb67-17"><a href="unsupervised-learning.html#cb67-17"></a>   K_centers[<span class="dv">1</span><span class="op">:</span>K,] &lt;-<span class="st"> </span>K_res<span class="op">$</span>centers </span>
<span id="cb67-18"><a href="unsupervised-learning.html#cb67-18"></a>                }</span>
<span id="cb67-19"><a href="unsupervised-learning.html#cb67-19"></a></span>
<span id="cb67-20"><a href="unsupervised-learning.html#cb67-20"></a><span class="co"># plot losses                </span></span>
<span id="cb67-21"><a href="unsupervised-learning.html#cb67-21"></a>xtitle &lt;-<span class="st"> &quot;decrease in total within-cluster dissimilarity &quot;</span></span>
<span id="cb67-22"><a href="unsupervised-learning.html#cb67-22"></a><span class="kw">plot</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>K0), <span class="dt">y=</span>TWCD, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">max</span>(TWCD)), <span class="dt">main=</span><span class="kw">list</span>(xtitle, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylab=</span><span class="st">&quot;total within-cluster dissimilarity&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;hyperparameter K&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb67-23"><a href="unsupervised-learning.html#cb67-23"></a><span class="kw">lines</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>K0), <span class="dt">y=</span>TWCD, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</span>
<span id="cb67-24"><a href="unsupervised-learning.html#cb67-24"></a></span>
<span id="cb67-25"><a href="unsupervised-learning.html#cb67-25"></a><span class="co"># singular value decomposition</span></span>
<span id="cb67-26"><a href="unsupervised-learning.html#cb67-26"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb67-27"><a href="unsupervised-learning.html#cb67-27"></a>pca &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb67-28"><a href="unsupervised-learning.html#cb67-28"></a>dat3 &lt;-<span class="st"> </span>d.data</span>
<span id="cb67-29"><a href="unsupervised-learning.html#cb67-29"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]]</span>
<span id="cb67-30"><a href="unsupervised-learning.html#cb67-30"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]]</span>
<span id="cb67-31"><a href="unsupervised-learning.html#cb67-31"></a></span>
<span id="cb67-32"><a href="unsupervised-learning.html#cb67-32"></a>lim0 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb67-33"><a href="unsupervised-learning.html#cb67-33"></a></span>
<span id="cb67-34"><a href="unsupervised-learning.html#cb67-34"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;K-means vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb67-35"><a href="unsupervised-learning.html#cb67-35"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">4</span>),]</span>
<span id="cb67-36"><a href="unsupervised-learning.html#cb67-36"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb67-37"><a href="unsupervised-learning.html#cb67-37"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">1</span>),]</span>
<span id="cb67-38"><a href="unsupervised-learning.html#cb67-38"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb67-39"><a href="unsupervised-learning.html#cb67-39"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(Classifier[<span class="dv">4</span>,]<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb67-40"><a href="unsupervised-learning.html#cb67-40"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb67-41"><a href="unsupervised-learning.html#cb67-41"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="k-medoids" class="section level3">
<h3><span class="header-section-number">5.5.5</span> K-medoids</h3>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="unsupervised-learning.html#cb68-1"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb68-2"><a href="unsupervised-learning.html#cb68-2"></a>(K_res &lt;-<span class="st"> </span><span class="kw">pam</span>(X, <span class="dt">k=</span><span class="dv">4</span>, <span class="dt">metric=</span><span class="st">&quot;manhattan&quot;</span>, <span class="dt">diss=</span><span class="ot">FALSE</span>))</span>
<span id="cb68-3"><a href="unsupervised-learning.html#cb68-3"></a></span>
<span id="cb68-4"><a href="unsupervised-learning.html#cb68-4"></a><span class="co"># plot K-medoids versus PCA</span></span>
<span id="cb68-5"><a href="unsupervised-learning.html#cb68-5"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;K-medoids vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb68-6"><a href="unsupervised-learning.html#cb68-6"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">4</span>),]</span>
<span id="cb68-7"><a href="unsupervised-learning.html#cb68-7"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb68-8"><a href="unsupervised-learning.html#cb68-8"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb68-9"><a href="unsupervised-learning.html#cb68-9"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb68-10"><a href="unsupervised-learning.html#cb68-10"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(K_res<span class="op">$</span>cluster<span class="op">==</span><span class="dv">2</span>),]</span>
<span id="cb68-11"><a href="unsupervised-learning.html#cb68-11"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb68-12"><a href="unsupervised-learning.html#cb68-12"></a><span class="kw">points</span>(<span class="dt">x=</span>dat3[K_res<span class="op">$</span>id.med,<span class="st">&quot;v1&quot;</span>],<span class="dt">y=</span>dat3[K_res<span class="op">$</span>id.med,<span class="st">&quot;v2&quot;</span>], <span class="dt">col=</span><span class="st">&quot;black&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span><span class="dv">2</span>)</span>
<span id="cb68-13"><a href="unsupervised-learning.html#cb68-13"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="gaussian-mixture-model" class="section level3">
<h3><span class="header-section-number">5.5.6</span> Gaussian mixture model</h3>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="unsupervised-learning.html#cb69-1"></a>seed &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb69-2"><a href="unsupervised-learning.html#cb69-2"></a><span class="kw">set.seed</span>(seed)</span>
<span id="cb69-3"><a href="unsupervised-learning.html#cb69-3"></a>K_res &lt;-<span class="st"> </span><span class="kw">GMM</span>(X, <span class="dt">gaussian_comps=</span><span class="dv">4</span>, <span class="dt">dist_mode=</span><span class="st">&quot;eucl_dist&quot;</span>, <span class="dt">seed_mode=</span><span class="st">&quot;random_subset&quot;</span>, <span class="dt">em_iter=</span><span class="dv">5</span>, <span class="dt">seed=</span>seed)</span>
<span id="cb69-4"><a href="unsupervised-learning.html#cb69-4"></a><span class="kw">summary</span>(K_res)</span>
<span id="cb69-5"><a href="unsupervised-learning.html#cb69-5"></a>clust &lt;-<span class="st"> </span><span class="kw">predict_GMM</span>(X, K_res<span class="op">$</span>centroids, K_res<span class="op">$</span>covariance_matrices, K_res<span class="op">$</span>weights)<span class="op">$</span>cluster_labels</span>
<span id="cb69-6"><a href="unsupervised-learning.html#cb69-6"></a></span>
<span id="cb69-7"><a href="unsupervised-learning.html#cb69-7"></a>pred &lt;-<span class="st"> </span><span class="kw">predict_GMM</span>(X, K_res<span class="op">$</span>centroids, K_res<span class="op">$</span>covariance_matrices, K_res<span class="op">$</span>weights)</span>
<span id="cb69-8"><a href="unsupervised-learning.html#cb69-8"></a><span class="kw">names</span>(pred)</span>
<span id="cb69-9"><a href="unsupervised-learning.html#cb69-9"></a></span>
<span id="cb69-10"><a href="unsupervised-learning.html#cb69-10"></a>pred<span class="op">$</span>cluster_labels[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>]</span>
<span id="cb69-11"><a href="unsupervised-learning.html#cb69-11"></a>pred<span class="op">$</span>cluster_proba[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</span>
<span id="cb69-12"><a href="unsupervised-learning.html#cb69-12"></a></span>
<span id="cb69-13"><a href="unsupervised-learning.html#cb69-13"></a>K_res<span class="op">$</span>centroids</span>
<span id="cb69-14"><a href="unsupervised-learning.html#cb69-14"></a></span>
<span id="cb69-15"><a href="unsupervised-learning.html#cb69-15"></a><span class="co"># singular value decomposition</span></span>
<span id="cb69-16"><a href="unsupervised-learning.html#cb69-16"></a>SVD &lt;-<span class="st"> </span><span class="kw">svd</span>(<span class="kw">as.matrix</span>(X))</span>
<span id="cb69-17"><a href="unsupervised-learning.html#cb69-17"></a></span>
<span id="cb69-18"><a href="unsupervised-learning.html#cb69-18"></a>pca &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb69-19"><a href="unsupervised-learning.html#cb69-19"></a>dat3 &lt;-<span class="st"> </span>d.data</span>
<span id="cb69-20"><a href="unsupervised-learning.html#cb69-20"></a>dat3<span class="op">$</span>v1 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]]</span>
<span id="cb69-21"><a href="unsupervised-learning.html#cb69-21"></a>dat3<span class="op">$</span>v2 &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(X) <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]]</span>
<span id="cb69-22"><a href="unsupervised-learning.html#cb69-22"></a></span>
<span id="cb69-23"><a href="unsupervised-learning.html#cb69-23"></a>(kk1 &lt;-<span class="st"> </span>K_res<span class="op">$</span>centroids <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">1</span>]])</span>
<span id="cb69-24"><a href="unsupervised-learning.html#cb69-24"></a>(kk2 &lt;-<span class="st"> </span>K_res<span class="op">$</span>centroids <span class="op">%*%</span><span class="st"> </span>SVD<span class="op">$</span>v[,pca[<span class="dv">2</span>]])</span>
<span id="cb69-25"><a href="unsupervised-learning.html#cb69-25"></a></span>
<span id="cb69-26"><a href="unsupervised-learning.html#cb69-26"></a>lim0 &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb69-27"><a href="unsupervised-learning.html#cb69-27"></a></span>
<span id="cb69-28"><a href="unsupervised-learning.html#cb69-28"></a><span class="kw">plot</span>(<span class="dt">x=</span>dat3<span class="op">$</span>v1, <span class="dt">y=</span>dat3<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;orange&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span>lim0,lim0), <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">2</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),<span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;principal component &quot;</span>, pca[<span class="dv">1</span>], <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),, <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;GMM(diagonal) vs. PCA&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb69-29"><a href="unsupervised-learning.html#cb69-29"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">0</span>),]</span>
<span id="cb69-30"><a href="unsupervised-learning.html#cb69-30"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb69-31"><a href="unsupervised-learning.html#cb69-31"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">3</span>),]</span>
<span id="cb69-32"><a href="unsupervised-learning.html#cb69-32"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb69-33"><a href="unsupervised-learning.html#cb69-33"></a>dat0 &lt;-<span class="st"> </span>dat3[<span class="kw">which</span>(clust<span class="op">==</span><span class="dv">1</span>),]</span>
<span id="cb69-34"><a href="unsupervised-learning.html#cb69-34"></a><span class="kw">points</span>(<span class="dt">x=</span>dat0<span class="op">$</span>v1, <span class="dt">y=</span>dat0<span class="op">$</span>v2, <span class="dt">col=</span><span class="st">&quot;magenta&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb69-35"><a href="unsupervised-learning.html#cb69-35"></a><span class="kw">points</span>(<span class="dt">x=</span>kk1,<span class="dt">y=</span>kk2, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">cex=</span><span class="dv">2</span>)</span>
<span id="cb69-36"><a href="unsupervised-learning.html#cb69-36"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;cluster 1&quot;</span>, <span class="st">&quot;cluster 2&quot;</span>, <span class="st">&quot;cluster 3&quot;</span>, <span class="st">&quot;cluster 4&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;magenta&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
<p>图<a href="unsupervised-learning.html#fig:cluster-results">5.4</a>展示了聚类的结果，GMMs最好。</p>
<div class="figure" style="text-align: center"><span id="fig:cluster-results"></span>
<img src="plots/5/cluster.png" alt="聚类比较" width="60%"  />
<p class="caption">
Figure 5.4: 聚类比较
</p>
</div>
</div>
<div id="t-sne-1" class="section level3">
<h3><span class="header-section-number">5.5.7</span> t-SNE</h3>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="unsupervised-learning.html#cb70-1"></a><span class="kw">require</span>(MASS)</span>
<span id="cb70-2"><a href="unsupervised-learning.html#cb70-2"></a><span class="kw">library</span>(plyr)</span>
<span id="cb70-3"><a href="unsupervised-learning.html#cb70-3"></a><span class="kw">library</span>(stringr)</span>
<span id="cb70-4"><a href="unsupervised-learning.html#cb70-4"></a><span class="kw">library</span>(plotrix)</span>
<span id="cb70-5"><a href="unsupervised-learning.html#cb70-5"></a><span class="kw">library</span>(matrixStats)</span>
<span id="cb70-6"><a href="unsupervised-learning.html#cb70-6"></a><span class="kw">library</span>(tsne)</span>
<span id="cb70-7"><a href="unsupervised-learning.html#cb70-7"></a><span class="kw">library</span>(umap)</span>
<span id="cb70-8"><a href="unsupervised-learning.html#cb70-8"></a><span class="kw">library</span>(kohonen)</span>
<span id="cb70-9"><a href="unsupervised-learning.html#cb70-9"></a>seed &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb70-10"><a href="unsupervised-learning.html#cb70-10"></a><span class="kw">set.seed</span>(seed)</span>
<span id="cb70-11"><a href="unsupervised-learning.html#cb70-11"></a><span class="co"># it takes roughly 50 seconds</span></span>
<span id="cb70-12"><a href="unsupervised-learning.html#cb70-12"></a><span class="co"># KL divergence 0.427991566501587</span></span>
<span id="cb70-13"><a href="unsupervised-learning.html#cb70-13"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb70-14"><a href="unsupervised-learning.html#cb70-14"></a>   (K_res &lt;-<span class="st"> </span><span class="kw">tsne</span>(X, <span class="dt">k=</span><span class="dv">2</span>, <span class="dt">initial_dim=</span><span class="kw">ncol</span>(X), <span class="dt">perplexity=</span><span class="dv">30</span>))</span>
<span id="cb70-15"><a href="unsupervised-learning.html#cb70-15"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb70-16"><a href="unsupervised-learning.html#cb70-16"></a></span>
<span id="cb70-17"><a href="unsupervised-learning.html#cb70-17"></a>i1 &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb70-18"><a href="unsupervised-learning.html#cb70-18"></a>i2 &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb70-19"><a href="unsupervised-learning.html#cb70-19"></a>sign0 &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb70-20"><a href="unsupervised-learning.html#cb70-20"></a></span>
<span id="cb70-21"><a href="unsupervised-learning.html#cb70-21"></a>K_res1 &lt;-<span class="st"> </span>K_res[,<span class="kw">c</span>(i1,i2)]</span>
<span id="cb70-22"><a href="unsupervised-learning.html#cb70-22"></a>K_res1 &lt;-<span class="st"> </span>sign0 <span class="op">*</span><span class="st"> </span>K_res1</span>
<span id="cb70-23"><a href="unsupervised-learning.html#cb70-23"></a><span class="kw">plot</span>(K_res1, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;component &quot;</span>, i2, <span class="dt">paste=</span><span class="st">&quot;&quot;</span>), <span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;component &quot;</span>, i1, <span class="dt">paste=</span><span class="st">&quot;&quot;</span>), <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;t-SNE with seed &quot;</span>, seed, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), <span class="dt">cex=</span><span class="fl">1.5</span>))</span>
<span id="cb70-24"><a href="unsupervised-learning.html#cb70-24"></a>dat0 &lt;-<span class="st"> </span>K_res1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),]</span>
<span id="cb70-25"><a href="unsupervised-learning.html#cb70-25"></a><span class="kw">points</span>(dat0, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb70-26"><a href="unsupervised-learning.html#cb70-26"></a>dat0 &lt;-<span class="st"> </span>K_res1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),]</span>
<span id="cb70-27"><a href="unsupervised-learning.html#cb70-27"></a><span class="kw">points</span>(dat0, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb70-28"><a href="unsupervised-learning.html#cb70-28"></a><span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;tau&gt;=21&quot;</span>, <span class="st">&quot;17&lt;=tau&lt;21&quot;</span>, <span class="st">&quot;tau&lt;17 (sports car)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="umap-1" class="section level3">
<h3><span class="header-section-number">5.5.8</span> UMAP</h3>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="unsupervised-learning.html#cb71-1"></a>kNeighbors &lt;-<span class="st"> </span><span class="dv">15</span>     <span class="co"># default is 15</span></span>
<span id="cb71-2"><a href="unsupervised-learning.html#cb71-2"></a>seed &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb71-3"><a href="unsupervised-learning.html#cb71-3"></a><span class="kw">set.seed</span>(seed)</span>
<span id="cb71-4"><a href="unsupervised-learning.html#cb71-4"></a>min_dist &lt;-<span class="st"> </span><span class="fl">.1</span></span>
<span id="cb71-5"><a href="unsupervised-learning.html#cb71-5"></a></span>
<span id="cb71-6"><a href="unsupervised-learning.html#cb71-6"></a>umap.param &lt;-<span class="st"> </span>umap.defaults</span>
<span id="cb71-7"><a href="unsupervised-learning.html#cb71-7"></a>umap.param<span class="op">$</span>n_components &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb71-8"><a href="unsupervised-learning.html#cb71-8"></a>umap.param<span class="op">$</span>n_neighbors &lt;-<span class="st"> </span>kNeighbors  </span>
<span id="cb71-9"><a href="unsupervised-learning.html#cb71-9"></a>umap.param<span class="op">$</span>random_state &lt;-<span class="st"> </span>seed </span>
<span id="cb71-10"><a href="unsupervised-learning.html#cb71-10"></a>umap.param<span class="op">$</span>min_dist &lt;-<span class="st"> </span>min_dist </span>
<span id="cb71-11"><a href="unsupervised-learning.html#cb71-11"></a></span>
<span id="cb71-12"><a href="unsupervised-learning.html#cb71-12"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb71-13"><a href="unsupervised-learning.html#cb71-13"></a>   (K_res &lt;-<span class="st"> </span><span class="kw">umap</span>(X, <span class="dt">config=</span>umap.param, <span class="dt">method=</span><span class="st">&quot;naive&quot;</span>))</span>
<span id="cb71-14"><a href="unsupervised-learning.html#cb71-14"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb71-15"><a href="unsupervised-learning.html#cb71-15"></a></span>
<span id="cb71-16"><a href="unsupervised-learning.html#cb71-16"></a>i1 &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb71-17"><a href="unsupervised-learning.html#cb71-17"></a>i2 &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb71-18"><a href="unsupervised-learning.html#cb71-18"></a>sign0 &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb71-19"><a href="unsupervised-learning.html#cb71-19"></a></span>
<span id="cb71-20"><a href="unsupervised-learning.html#cb71-20"></a>K_res1 &lt;-<span class="st"> </span>K_res<span class="op">$</span>layout[,<span class="kw">c</span>(i1,i2)]</span>
<span id="cb71-21"><a href="unsupervised-learning.html#cb71-21"></a>K_res1 &lt;-<span class="st"> </span>sign0 <span class="op">*</span><span class="st"> </span>K_res1</span>
<span id="cb71-22"><a href="unsupervised-learning.html#cb71-22"></a><span class="kw">plot</span>(K_res1, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>, <span class="dt">ylab=</span><span class="kw">paste</span>(<span class="st">&quot;component &quot;</span>, i2, <span class="dt">paste=</span><span class="st">&quot;&quot;</span>), <span class="dt">xlab=</span><span class="kw">paste</span>(<span class="st">&quot;component &quot;</span>, i1, <span class="dt">paste=</span><span class="st">&quot;&quot;</span>), <span class="dt">main=</span><span class="kw">list</span>(<span class="kw">paste</span>(<span class="st">&quot;UMAP (k=&quot;</span>, kNeighbors, <span class="st">&quot; NN and min_dist=&quot;</span>, min_dist,<span class="st">&quot;)&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), <span class="dt">cex=</span><span class="fl">1.5</span>))</span>
<span id="cb71-23"><a href="unsupervised-learning.html#cb71-23"></a>dat0 &lt;-<span class="st"> </span>K_res1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>),]</span>
<span id="cb71-24"><a href="unsupervised-learning.html#cb71-24"></a><span class="kw">points</span>(dat0, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb71-25"><a href="unsupervised-learning.html#cb71-25"></a>dat0 &lt;-<span class="st"> </span>K_res1[<span class="kw">which</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">17</span>),]</span>
<span id="cb71-26"><a href="unsupervised-learning.html#cb71-26"></a><span class="kw">points</span>(dat0, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">pch=</span><span class="dv">20</span>)</span>
<span id="cb71-27"><a href="unsupervised-learning.html#cb71-27"></a><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;tau&gt;=21&quot;</span>, <span class="st">&quot;17&lt;=tau&lt;21&quot;</span>, <span class="st">&quot;tau&lt;17 (sports car)&quot;</span>), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>), <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>))</span></code></pre></div>
</div>
<div id="kohonen-map" class="section level3">
<h3><span class="header-section-number">5.5.9</span> Kohonen map</h3>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="unsupervised-learning.html#cb72-1"></a>n1 &lt;-<span class="st"> </span><span class="dv">2</span></span>
<span id="cb72-2"><a href="unsupervised-learning.html#cb72-2"></a>n2 &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb72-3"><a href="unsupervised-learning.html#cb72-3"></a></span>
<span id="cb72-4"><a href="unsupervised-learning.html#cb72-4"></a><span class="kw">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb72-5"><a href="unsupervised-learning.html#cb72-5"></a>{t1 &lt;-<span class="st"> </span><span class="kw">proc.time</span>()</span>
<span id="cb72-6"><a href="unsupervised-learning.html#cb72-6"></a> som.X &lt;-<span class="st"> </span><span class="kw">som</span>(<span class="kw">as.matrix</span>(X), <span class="dt">grid =</span> <span class="kw">somgrid</span>(<span class="dt">xdim=</span>n1, <span class="dt">ydim=</span>n2, <span class="dt">topo=</span><span class="st">&quot;rectangular&quot;</span>), </span>
<span id="cb72-7"><a href="unsupervised-learning.html#cb72-7"></a>                            <span class="dt">rlen=</span> <span class="dv">100</span>, <span class="dt">dist.fcts=</span><span class="st">&quot;sumofsquares&quot;</span>)</span>
<span id="cb72-8"><a href="unsupervised-learning.html#cb72-8"></a><span class="kw">proc.time</span>()<span class="op">-</span>t1}</span>
<span id="cb72-9"><a href="unsupervised-learning.html#cb72-9"></a></span>
<span id="cb72-10"><a href="unsupervised-learning.html#cb72-10"></a></span>
<span id="cb72-11"><a href="unsupervised-learning.html#cb72-11"></a><span class="kw">summary</span>(som.X)</span>
<span id="cb72-12"><a href="unsupervised-learning.html#cb72-12"></a></span>
<span id="cb72-13"><a href="unsupervised-learning.html#cb72-13"></a><span class="kw">plot</span>(som.X,<span class="kw">c</span>(<span class="st">&quot;changes&quot;</span>), <span class="dt">main=</span><span class="kw">list</span>(<span class="st">&quot;training progress&quot;</span>, <span class="dt">cex=</span><span class="fl">1.5</span>), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)            <span class="co"># training progress</span></span>
<span id="cb72-14"><a href="unsupervised-learning.html#cb72-14"></a></span>
<span id="cb72-15"><a href="unsupervised-learning.html#cb72-15"></a><span class="kw">plot</span>(som.X,<span class="kw">c</span>(<span class="st">&quot;counts&quot;</span>), <span class="dt">main=</span><span class="st">&quot;allocation counts to neurons&quot;</span>, <span class="dt">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb72-16"><a href="unsupervised-learning.html#cb72-16"></a></span>
<span id="cb72-17"><a href="unsupervised-learning.html#cb72-17"></a>d.data<span class="op">$</span>tau2 &lt;-<span class="st"> </span>d.data<span class="op">$</span>sports_car<span class="op">+</span><span class="kw">as.integer</span>(d.data<span class="op">$</span>tau<span class="op">&lt;</span><span class="dv">21</span>)<span class="op">+</span><span class="dv">1</span></span>
<span id="cb72-18"><a href="unsupervised-learning.html#cb72-18"></a><span class="kw">plot</span>(som.X,<span class="kw">c</span>(<span class="st">&quot;mapping&quot;</span>), <span class="dt">classif=</span><span class="kw">predict</span>(som.X), <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;blue&quot;</span>,<span class="st">&quot;green&quot;</span>,<span class="st">&quot;red&quot;</span>)[d.data<span class="op">$</span>tau2], <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">main=</span><span class="st">&quot;allocation of cases to neurons&quot;</span>)</span></code></pre></div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="boosting.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": "github"
},
"fontsettings": {
"theme": "sepia",
"family": "serif",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
