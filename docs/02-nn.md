# 神经网络 {#ch3}

## 建立神经网络的一般步骤

1. 明确研究目标和数据类型

2. 数据预处理

   - 总结性统计描述

   - 缺失值、异常值处理

   - 标准化处理
   
   - 训练-验证-测试

3. 选取合适的神经网络类型

   - 全连接神经网络
   
   - 卷积神经网络
   
   - 递归神经网络

4. 建立神经网络

   - 全连接神经网络：隐藏层个数，每一层的神经元个数，激活函数，正则化手段，损失函数
   
   - 卷积神经网络：卷积核大小，特征个数
   
   - 递归神经网络：time step
   
5. 训练神经网络

   - patience
   
   - epoch
   
   - batch size
   
   - optimizer
   
6. 返回第4步调整模型的结构参数（hyper-parameter tuning），观察验证损失的变化，选取最终模型。

## 数据预处理

- 连续型变量：标准化处理

- 分类变量：one-hot encoding 或者 embedding layer

- 训练集-验证集-测试集：分层抽样


```r
PreProcess.Continuous <- function(var1, dat1){
   names(dat1)[names(dat1) == var1]  <- "V1"
   dat1$X <- as.numeric(dat1$V1)
   dat1$X <- 2*(dat1$X-min(dat1$X))/(max(dat1$X)-min(dat1$X))-1
   names(dat1)[names(dat1) == "V1"]  <- var1
   names(dat1)[names(dat1) == "X"]  <- paste(var1,"X", sep="")
   dat1
   }

Features.PreProcess <- function(dat1){
   dat1$VehPower <- pmin(dat1$VehPower,9)
   dat1 <- PreProcess.Continuous("VehPower", dat1)   
   dat1$VehAge <- pmin(dat1$VehAge,20)
   dat1 <- PreProcess.Continuous("VehAge", dat1)   
   dat1$DrivAge <- pmin(dat1$DrivAge,90)
   dat1 <- PreProcess.Continuous("DrivAge", dat1)   
   dat1$BonusMalus <- pmin(dat1$BonusMalus,150)
   dat1 <- PreProcess.Continuous("BonusMalus", dat1)   
   dat1$VehBrandX <- as.integer(dat1$VehBrand)-1  # categorical variable
   dat1$VehGas <- as.factor(dat1$VehGas)
   dat1$VehGasX <- as.integer(dat1$VehGas) - 1.5 # binary: continuous or categorical
   dat1 <- PreProcess.Continuous("Area", dat1)   
   dat1 <- PreProcess.Continuous("Density", dat1)   
   dat1$RegionX <- as.integer(dat1$Region) - 1 # categorical
   dat1
}

dat2 <- Features.PreProcess(freMTPL2freq)   
names(dat2)
dat2_train<-dat2[index_train,]
dat2_valid<-dat2[index_valid,]
dat2_test<-dat2[index_test,]
dat2_learn<-dat2[index_learn,]
```

## 神经网络提升模型

基本结构

$$\ln \lambda(\mathbf{x})= e\hat{\lambda}^{\text{GAM}}(\mathbf{x})\hat{\lambda}^{\text{NN}}(\mathbf{x})$$

其中，$\hat{\lambda}^{\text{GAM}}$为广义线性模型的索赔频率估计值， $\hat{\lambda}^{\text{NN}}$为神经网络索赔频率的估计值。使用上述模型的优点：

- 部分可解释性

- 神经网络容易训练


```r
lambda.hom <- sum(dat2_train$ClaimNb)/sum(dat2_train$Exposure);lambda.hom
names(dat2)

# index of continous variables (non-categorical)

features <- c(13:16, 18:20)
names(dat2_learn)[features]
(q0 <- length(features))

# training data

Xtrain<- as.matrix(dat2_train[, features])  # design matrix learning sample
Brtrain <- as.matrix(dat2_train$VehBrandX)
Retrain <- as.matrix(dat2_train$RegionX)
Ytrain<- as.matrix(dat2_train$ClaimNb)
Vtrain<-as.matrix(log(dat2_train$Exposure*lambda.hom))

# validation data 

Xvalid<- as.matrix(dat2_valid[, features])  # design matrix learning sample
Brvalid <- as.matrix(dat2_valid$VehBrandX)
Revalid <- as.matrix(dat2_valid$RegionX)
Yvalid<- as.matrix(dat2_valid$ClaimNb)
Vvalid<-as.matrix(log(dat2_valid$Exposure*lambda.hom))
xxvalid<-list(Xvalid,Brvalid,Revalid,Vvalid)

# testing data

Xtest <- as.matrix(dat2_test[, features])    # design matrix test sample
Brtest <- as.matrix(dat2_test$VehBrandX)
Retest <- as.matrix(dat2_test$RegionX)
Ytest <- as.matrix(dat2_test$ClaimNb)
Vtest <- as.matrix(log(dat2_test$Exposure*lambda.hom))

CANN <- 1  # 0 = normal NN, 1=CANN
if (CANN==1){
     Vtrain <- as.matrix(log(dat1_train$fitGAM1))
     Vvalid<- as.matrix(log(dat1_valid$fitGAM1))
     Vtest <- as.matrix(log(dat1_test$fitGAM1))
}

# hyperparameters of the neural network architecture

(BrLabel <- length(unique(dat2_train$VehBrandX)))
(ReLabel <- length(unique(dat2_train$RegionX)))
q1 <- 20   
q2 <- 15
q3 <- 10
q4 <- 5
d <- 1        # dimensions embedding layers for categorical features

# input layer

(Design   <- layer_input(shape = c(q0),  dtype = 'float32', name = 'Design'))
(VehBrand <- layer_input(shape = c(1),   dtype = 'int32', name = 'VehBrand'))
(Region   <- layer_input(shape = c(1),   dtype = 'int32', name = 'Region'))
(LogVol   <- layer_input(shape = c(1),   dtype = 'float32', name = 'LogVol'))

# embedding layer

(BrandEmb = VehBrand %>% 
      layer_embedding(input_dim = BrLabel, output_dim = d, input_length = 1, name = 'BrandEmb') %>%
      layer_flatten(name='Brand_flat'))
# input_dim is the size of vocabulary; input_length is the length of input sequences
    
(RegionEmb = Region %>% 
      layer_embedding(input_dim = ReLabel, output_dim = d, input_length = 1, name = 'RegionEmb') %>%
      layer_flatten(name='Region_flat'))

Network = list(Design, BrandEmb, RegionEmb) %>% layer_concatenate(name='concate') %>%
          layer_dense(units=q1, activation='relu', name='hidden1') %>%
          layer_batch_normalization()%>%
          layer_dropout(rate =0.05) %>%
          layer_dense(units=q2, activation='relu', name='hidden2') %>%
          layer_batch_normalization()%>%
          layer_dropout(rate =0.05) %>%
          layer_dense(units=q3, activation='relu', name='hidden3') %>%
          layer_batch_normalization()%>%
          layer_dropout(rate =0.05) %>%
          layer_dense(units=q4, activation='relu', name='hidden4') %>%
          layer_batch_normalization()%>%
          layer_dropout(rate =0.05) %>%
          layer_dense(units=1, activation='linear', name='Network',
                      weights = list(array(0, dim=c(q4,1)), array(0, dim=c(1))))

Response = list(Network, LogVol) %>% layer_add(name='Add') %>% 
           layer_dense(units=1, activation=k_exp, name = 'Response', trainable=FALSE,
                        weights=list(array(1, dim=c(1,1)), array(0, dim=c(1))))

model <- keras_model(inputs = c(Design, VehBrand, Region, LogVol), outputs = c(Response))
model %>% compile(optimizer = optimizer_nadam(), loss = 'poisson')

compile(model,optimizer = optimizer_nadam(), loss = 'poisson')
summary(model)
```

## 训练模型


```r
# fitting the neural network
early_stop <- callback_early_stopping(monitor = "val_loss", patience =10)
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 50 == 0) cat("\n")
    cat(".")
  }
)  
{t1 <- proc.time();
fit <- model %>% fit(list(Xtrain, Brtrain, Retrain, Vtrain), Ytrain,
                     epochs=500, batch_size=5000, verbose=1,
                     validation_data=list(xxvalid,Yvalid),
                     callbacks=list(print_dot_callback,early_stop));
(proc.time()-t1)}

matplot(cbind(fit$metrics$loss,fit$metrics$val_loss), type="l")
fit$metrics

# calculating the predictions
dat2_test$fitNN <- as.vector(model %>% predict(list(Xtest, Brtest, Retest, Vtest)))
keras_poisson_dev(dat2_test$fitNN, dat2_test$ClaimNb)
Poisson.Deviance(dat2_test$fitNN, dat2_test$ClaimNb)
```


##  其它模型


```r
train.x <- list(as.matrix(dat2_train[,c("VehPowerX", "VehAgeX", "VehGasX")]),
                as.matrix(dat2_train[,"VehBrandX"]),
                as.matrix(dat2_train[,c("DrivAgeX", "BonusMalus")]),
                as.matrix(log(dat1_train$fitGAM1)) )
valid.x <- list(as.matrix(dat2_valid[,c("VehPowerX", "VehAgeX", "VehGasX")]),
                as.matrix(dat2_valid[,"VehBrandX"]),
                as.matrix(dat2_valid[,c("DrivAgeX", "BonusMalus")]),
                as.matrix(log(dat1_valid$fitGAM1)) )
test.x <- list(as.matrix(dat2_test[,c("VehPowerX", "VehAgeX", "VehGasX")]),
                as.matrix(dat2_test[,"VehBrandX"]),
                as.matrix(dat2_test[,c("DrivAgeX", "BonusMalus")]),
                as.matrix(log(dat1_test$fitGAM1)) )

neurons <- c(15,10,5)

model.2IA <- function(Brlabel){
   Cont1        <- layer_input(shape = c(3), dtype = 'float32', name='Cont1')
   Cat1         <- layer_input(shape = c(1), dtype = 'int32',   name='Cat1')
   Cont2        <- layer_input(shape = c(2), dtype = 'float32', name='Cont2')
   LogExposure  <- layer_input(shape = c(1), dtype = 'float32', name = 'LogExposure') 
   
   x.input <- c(Cont1, Cat1, Cont2, LogExposure)
   #
   Cat1_embed = Cat1 %>%  
            layer_embedding(input_dim = Brlabel, output_dim = 1, trainable=TRUE, 
                    input_length = 1, name = 'Cat1_embed') %>%
                    layer_flatten(name='Cat1_flat')
   #
   NNetwork1 = list(Cont1, Cat1_embed) %>% layer_concatenate(name='cont') %>%
            layer_dense(units=neurons[1], activation='relu', name='hidden1') %>%
            layer_dense(units=neurons[2], activation='relu', name='hidden2') %>%
            layer_dense(units=neurons[3], activation='relu', name='hidden3') %>%
            layer_dense(units=1, activation='linear', name='NNetwork1', 
                    weights=list(array(0, dim=c(neurons[3],1)), array(0, dim=c(1))))
   #
   NNetwork2 = Cont2 %>%
            layer_dense(units=neurons[1], activation='relu', name='hidden4') %>%
            layer_dense(units=neurons[2], activation='relu', name='hidden5') %>%
            layer_dense(units=neurons[3], activation='relu', name='hidden6') %>%
            layer_dense(units=1, activation='linear', name='NNetwork2', 
                    weights=list(array(0, dim=c(neurons[3],1)), array(0, dim=c(1))))

   #
   NNoutput = list(NNetwork1, NNetwork2, LogExposure) %>% layer_add(name='Add') %>%
                 layer_dense(units=1, activation=k_exp, name = 'NNoutput', trainable=FALSE,
                       weights=list(array(c(1), dim=c(1,1)), array(0, dim=c(1))))

   model <- keras_model(inputs = x.input, outputs = c(NNoutput))
   model %>% compile(optimizer = optimizer_nadam(), loss = 'poisson')        
   model
   }

model <- model.2IA(BrLabel)
summary(model)
```

## 模型训练


```r
early_stop <- callback_early_stopping(monitor = "val_loss", patience =10)
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 50 == 0) cat("\n")
    cat(".")
  }
)  
# may take a couple of minutes if epochs is more than 100
{t1 <- proc.time()
     fit <- model %>% fit(train.x, as.matrix(dat2_train$ClaimNb), 
                          epochs=500, batch_size=10000, verbose=1,
                                       validation_data=list(valid.x,dat2_valid$ClaimNb),
                          callback=list(early_stop,print_dot_callback))
(proc.time()-t1)}

matplot(cbind(fit$metrics$loss,fit$metrics$val_loss), type="l")
fit$metrics

dat2_test$fitGAMPlus <- as.vector(model %>% predict(test.x))
Poisson.Deviance(dat2_test$fitGAMPlus, dat2_test$ClaimNb)
keras_poisson_dev(dat2_test$fitGAMPlus, dat2_test$ClaimNb)
```

## 总结


```r
dev_sum<-fread("./plots/1/dev_sum.csv")[,-1]
AD<-data.frame(model="Neural network",test_error=0,test_error_keras=0)
dev_sum<-rbind(dev_sum,AD)
dev_sum$test_error[8]<-round(Poisson.Deviance(dat2_test$fitNN, dat2_test$ClaimNb),4)
dev_sum$test_error_keras[8]<-round(keras_poisson_dev(dat2_test$fitNN, dat2_test$ClaimNb),4)
# write.csv(dev_sum,"./plots/1/dev_sum.csv")
```


```
##                       model test_error test_error_keras
## 1                 Intercept    33.5695          21.7647
## 2                       GLM    31.7731          20.8665
## 3                 GLM Lasso    31.8132          20.8866
## 4                       GAM    31.6651          20.8125
## 5             Decision tree    30.9780          20.4690
## 6             Random forest    30.9652          20.4626
## 7 Generalized boosted model    30.8972          20.4286
## 8            Neural network    31.0607          20.5080
```

- Boosting > RF > Tree > NN > GAM > GLM > Homo

