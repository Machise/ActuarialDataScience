[["index.html", "现代精算统计模型 👨🏫 欢迎 🤔 答疑 🗓️ 课程安排", " 现代精算统计模型 Modern Actuarial Models 2020-11-23 00:11:45 👨🏫 欢迎 《现代精算统计模型》主要讲述如何使用统计学习和机器学习算法，提升传统的精算统计模型或者解决新的精算问题。这门课主要参考瑞士精算师协会发布的“精算数据科学”，该教程的主要目的是“为精算师提供一个对数据科学全面且易懂的介绍”，该教程提供了多篇方法性文章并开源代码，这样“读者可以相对容易地把这些数据科学方法用在自己的数据上”。 我们建议大家仔细阅读以下文献，尝试并理解所有代码。此网站将作为该课程的辅助，为大家答疑，总结文献，并对文献中的方法做扩展。该网站由授课老师高光远和助教张玮钰管理，欢迎大家反馈意见到助教、微信群、或邮箱 guangyuan.gao@ruc.edu.cn。 🤔 答疑 我定期把同学们的普遍疑问在这里解答，欢迎提问！ 👉 随机种子数(2020/11/20) 输入RNGversion(\"3.5.0\"); set.seed(100)，使得你的随机种子数和paper的相同，模型结果相近。 👉 MAC OS, Linux, WIN (2020/11/16) 据观察，在MAC OS和Linux系统下安装keras成功的比例较高。WIN系统下，Python各个包的依赖以及和R包的匹配有一定的问题，今天是通过更换镜像源解决了R中无法加载tensorflow.keras模块的问题，推测是TUNA源中WIN包依赖关系没有及时更新。 为了解决镜像源更新延迟、或者tensorflow版本过低的问题，这里共享WIN下经测试的conda环境配置。下载该文档，从该文档所在文件夹启动命令行，使用命令conda env create --name &lt;env&gt; --file filename.yaml，安装该conda环境。在R中使用reticulate::use_condaenv(\"&lt;env&gt;\",required=T)关联该环境。 另外，可下载MAC OS系统下经测试的conda环境配置。可通过conda env create --name &lt;env&gt; --file filename.yaml安装。 👉 CASdatasets (2020/11/13) 源文件在http://cas.uqam.ca/，但下载速度很慢，我把它放在坚果云共享。下载后选择install from local archive file。 👉 微信群 (2020/11/08) 🗓️ 课程安排 以下安排为初步计划，根据大家的需求和背景，我们可能要花更多的时间在某些重要的方法及其在精算上的应用。 第10周： 准备工作。 第11周: 1 - French Motor Third-Party Liability Claims https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764 机动 2 - Inisghts from Inside Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3226852 3 - Nesting Classical Actuarial Models into Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3320525 第12周： 4 - On Boosting: Theory and Applications https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3402687 第13周： 5 - Unsupervised Learning: What is a Sports Car https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3439358 第14周： 6 - Lee and Carter go Machine Learning: Recurrent Neural Networks https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3441030 第15周： 7 - The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547887 第16周： 8 - Peeking into the Black Box: An Actuarial Case Study for Interpretable Machine Learning https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3595944 第17周： 9 - Convolutional neural network case studies: (1) Anomalies in Mortality Rates (2) Image Recognition https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3656210 "],["ch0.html", "准备工作 0.1 常用链接 0.2 克隆代码 0.3 R interface to Keras 0.4 R interface to Python 0.5 Python", " 准备工作 “工欲善其事，必先利其器。” 在以下步骤中，当你发现安装非常慢时，可以尝试4G网络，尝试VPN，尝试改变CRAN的镜像源，或尝试改变conda的镜像源。conda镜像源通过修改用户目录下的.condarc文件使用TUNA镜像源，但该镜像源可能有更新延迟。 0.1 常用链接 准备工作中常用的链接有 GitHub Git SSH key GitHub and RStudio Jupyter Notebook Anaconda Miniconda 常用Conda命令 TUNA镜像源 R interface to Tensorflow and Keras reticulate Tensorflow Pytorch 校级计算云 CUDA cuDNN 0.2 克隆代码 GitHub提供了大量开源代码，这门课的代码主要来自此链接。通常，使用GitHub开源代码最方便的是fork到自己GitHub账户下，然后clone到本地。具体而言，需要进行以下操作： 注册GitHub账户。 Fork此链接到自己账户下的新仓库,可重新命名为如Modern-Actuarial-Models或其他名称。 安装git。在命令窗口使用$ git config --global user.name \"Your Name\" 和 $ git config --global user.email \"youremail@yourdomain.com\" 配置git的用户名和邮箱分别为GitHub账户的用户名和邮箱。最后可使用$ git config --list查看配置信息。 (选做)在本地电脑创建ssh public key，并拷贝到GitHub中Setting下SSH and GPG keys。ssh public key一般保存在本人目录下的隐藏文件夹.ssh中，扩展名为.pub。详见链接。设立SSH可以避免后续push代码到云端时，每次都需要输入密码的麻烦 电脑连接手机4G热点。一般地，在手机4G网络下克隆的速度比较快。 在RStudio中创建新的项目，选择Version Control，然后Git，在Repository URL中输入你的GitHub中刚才fork的新仓库地址（在Code下能找到克隆地址，如果第4步完成可以选择SSH地址，如果第4步没完成必须选择HTTPS地址），输入文件夹名称，选择存放位置，点击create project，RStudio开始克隆GitHub上该仓库的所有内容。 此时，你在GitHub上仓库的内容全部克隆到了本地，且放在了一个R Project中。在该Project中，会多两个文件，.Rproj和.gitignore，第一个文件保存了Project的设置，第二文件告诉git在push本地文件到GitHub时哪些文件被忽略。 如果你修改了本地文件，可以通过R中内嵌的Git上传到GitHub（先commit再push），这样方便在不同电脑上同步文件。git是代码版本控制工具，在push之前，你可以比较和上个代码版本的差异。GitHub记录了你每次push的详细信息，且存放在本地文件夹.git中。同时，如果GitHub上代码有变化，你可以pull到本地。如果经常在不同电脑上使用本仓库，一般需要先pull成最新版本，然后再编辑修改，最后commit-push到GitHub。 (选做) 你可以建立新的branch，使自己的修改和源代码分开。具体操作可参考链接，或者参考账户建立时自动产生的getting-started仓库。 (选做) 你可以尝试Github Desktop或者Jupyter Lab（加载git extension）管理，但对于这门课，这两种方式不是最优。 理论上，GitHub上所有仓库都可以采用以上方法在RStudio中管理，当然，RStudio对于R代码仓库管理最有效，因为我们可以直接在RStudio中运行仓库中的代码。 0.3 R interface to Keras 这里主要说明keras包的安装和使用。Keras是tensorflow的API，在keras中建立的神经网络模型都由tensorflow训练。安装keras包主要是安装Python库tensorflow，并让R与之相关联。 0.3.1 R自动安装 最简单的安装方式如下： 使用install.packages(\"tensorflow\")安装所有相关的包，然后library(\"tensorflow\")。 install_tensorflow() 这时大概率会出现 No non-system installation of Python could be found. Would you like to download and install Miniconda? Miniconda is an open source environment management system for Python. See https://docs.conda.io/en/latest/miniconda.html for more details. Would you like to install Miniconda? [Y/n]: 虽然你可能已经有Anaconda和Python，但R没有“智能”地识别出来，这时仍建议你选Y，让R自己装一下自己能更好识别的Miniconda, 这个命令还会自动建立一个独立conda环境r-reticulate，并在其中装好tensorflow, keras等。 上步如果正常运行，结束后会自动重启R。这时你运行library(tensorflow)然后tf$constant(\"Hellow Tensorflow\")，如果没报错，那继续install_packages(\"keras\"),library(\"keras\")。 用以下代码验证安装成功 model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 如果出现以下错误 错误: Installation of TensorFlow not found. Python environments searched for &#39;tensorflow&#39; package: C:\\Users\\...\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\python.exe You can install TensorFlow using the install_tensorflow() function. 这个错误通常是由于r-reticulate中tensorflow和其他包的依赖关系发生错误，或者tensorflow版本太低，你可以更换镜像源、使用conda/pip install调整该环境中的tensorflow版本和依赖关系。 更好的方式是在conda下安装好指定版本的tensorflow然后关联到R，或者用其他方式让R找到其他方式安装的tensorflow。这时，你先把之前失败的安装C:\\Users\\...\\AppData\\Local\\r-miniconda，这个文件夹完全删掉。然后参考以下安装步骤。 0.3.2 使用reticulate关联conda环境 下载并安装Anaconda或者Miniconda。 运行Anaconda Prompt或者Anaconda Powershell Prompt，在命令行输入conda create -n r-tensorflow tensorflow=2.1.0，conda会创建一个独立的r-tensorflow环境，并在其中安装tensorflow包。 继续在命令行运行conda activate r-tensorflow加载刚刚安装的环境，并pip install h5py pyyaml requests Pillow scipy在该环境下安装keras依赖的包。至此，R需要的tensorflow环境已经准备好，接下来让R关联此环境。 重启R，library(\"reticulate\")然后use_condaenv(\"r-tensorflow\",required=T),这时R就和上面建立的环境关联好。 library(\"keras“)。这里假设你已经装好tensorflow和keras包。 用以下代码验证安装成功 model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 0.3.3 指定conda安装 下载并安装Anaconda或者Miniconda。 命令行输入which -a python，找到Anaconda中Python的路径记为anapy。 R中install_packages(\"tensorflow\")，然后 install_tensorflow(method = &quot;conda&quot;, conda = &quot;anapy&quot;, envname = &quot;r-tensorflow&quot;, version = &quot;2.1.0&quot;) 此命令会在conda下创建r-tensorflow的环境并装好tensorflow包。 install_packages(\"keras\"); library(\"keras\") 用以下代码验证安装成功 model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 0.3.4 使用reticulate安装 重启R，library(\"reticulate\")。 options(timeout=300)，防止下载时间过长中断。 install_miniconda()，将会安装miniconda并创建一个r-reticulateconda环境。此环境为R默认调用的Python环境。 （重启R）library(\"tensorflow\"); install_tensorflow(version=\"2.1.0\")，将会在r-reticulate安装tensorflow。 install_packages(\"keras\"); library(\"keras\") 用以下代码验证安装成功 model &lt;- keras_model_sequential() %&gt;% layer_flatten(input_shape = c(28, 28)) %&gt;% layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% layer_dropout(0.2) %&gt;% layer_dense(10, activation = &quot;softmax&quot;) summary(model) 0.4 R interface to Python R包reticulate为tensorflow的依赖包，当你装tensorflow它也被自动安装。它可以建立R与Python的交互。 0.4.1 reticulate 常见命令 conda_list()列出已安装的conda环境 virtualenv_list()列出已存在的虚拟环境 use_python, use_condaenv, use_virtualenv可以指定与R关联的python。 py_config()可以查看当前Python关联信息。 很多时候，R会创建一个独立conda环境r-miniconda/envs/r-reticulate。 0.4.2 切换R关联的conda环境 根据需要，你可以切换R关联的conda环境。具体步骤为 重启R library(\"reticulate\") conda_list()列出可以关联的环境和路径。 use_condaenv(\"env-name\")。env-name为关联的conda环境。 py_config查看是否关联成功。 0.5 Python 一般在每个Python（Conda）环境都需要安装一个Jupyter Notebook (conda install notebook)。 0.5.1 Conda环境 Python（conda）环境建立比较简单，在使用reticulate关联conda环境我们已经建立过一个环境r-tensorflow。具体操作如下: 建立独立环境conda create -n env-name python=3.8 tensorflow=2.1.0 notebook。该命令会建立env-name的环境，并在其中安装python=3.8,tensorflow，notebook包及其依赖包。 激活环境conda activate env-name. cd 到你的工作目录。 启动jupyter notebook jupyter notebook。 如遇到缺少的包，在该环境env-name下使用conda install ***安装缺少的包。 0.5.2 常用的Conda命令 conda create -n env-name2 --clone env-name1:复制环境 conda env list：列出所有环境 conda deactivate：退出当前环境 conda remove -n env-name --all：删除环境env-name中的所有包 conda list -n env-name: 列出环境env-name所安装的包 conda clean -p：删除不使用的包 conda clean -t：删除下载的包 conda clean -a：删除所有不必要的包 pip freeze &gt; pip_pkg.txt, pip install -r pip_pkg.txt 保存当前环境PyPI包版本，从文件安装PyPI包（需同系统） conda env export &gt; conda_pkg.yaml, conda env export --name env_name &gt; conda_pkg.yaml, conda env create --name env-name2 --file conda_pkg.yaml 保存当前/env-name环境所有包，从文件安装所有包（需同系统） conda list --explicit &gt; spec-list.txt, conda create --name env-name2 --file spec-list.txt 保存当前环境Conda包下载地址，从文件安装Conda包（需同系统） conda list --export &gt; spec-list.txt, conda create --name env-name2 --file spec-list.txt 保存当前环境所有包（类似conda env export），从文件安装所有包（需同系统） 0.5.3 Tensorflow/Pytorch GPU version Tensorflow可以综合使用CPU和GPU进行计算，GPU的硬件结构适进行卷积运算，所以适于CNN，RNN等模型的求解。 你可以申请使用校级计算云或者使用学院计算云，它们的服务器都配置了GPU，并装好了可以使用GPU的Tensorflow或者Pytorch。使用校级计算云时，你通常只需要运行Jupyter Notebook就可以使用云端GPU进行计算。使用学院计算云时，你通常需要知道一些常用的Linux命令，你也可以安装Ubuntu来熟悉Linux系统。 校级计算云和学院计算云有专门的IT人员帮你解决如本页所示的大部分IT问题。 你的机器如果有GPU，可以按如下步骤让GPU发挥它的并行计算能力，关键点是让GPU型号、GPU驱动、CUDA版本、Tensorflow或Pytorch版本彼此匹配，且彼此“相连”。百度或者必应上有很多相关资料可以作为参考。 查看电脑GPU和驱动，以及支持的CUDA版本。 或者在终端执行以下命令：nvidia-smi，查看你的NVIDIA显卡驱动支持的CUDA版本。 查看各个Tensorflow版本，Pytorch版本对应的CUDA和cuDNN. 下载并安装正确版本的CUDA。注册、下载并安装正确版本的cuDNN 配置CUDA和cuDNN. 安装Tensorflow或者Pytorch. "],["ch2.html", "1 车险索赔频率预测 1.1 背景介绍 1.2 预测模型概述 1.3 特征工程 1.4 训练集-验证集-测试集 1.5 泊松偏差损失函数 1.6 GLM &amp; GAM 1.7 Possion tree 1.8 Random forest 1.9 Boosting Poisson tree 1.10 Summary", " 1 车险索赔频率预测 1.1 背景介绍 车险数据量大，风险特征多，对车险数据分析时可以体现出机器学习算法的优势，即使用算法从大数据中挖掘有用信息、提取特征。 在精算中，常常使用车险保单数据和历史索赔数据进行风险分析、车险定价等。保单数据库是在承保的时候建立的，索赔数据库是在索赔发生时建立的，大部分保单没有发生索赔，所以它们不会在索赔数据库中体现。 保单数据库记录了车险的风险信息，包括： 驾驶员特征：年龄、性别、工作、婚姻、地址等 车辆特征：品牌、车座数、车龄、价格、马力等 保单信息：保单编号、承保日期、到期日期 奖惩系数 索赔数据库记录了保单的索赔信息，可以得到索赔次数\\(N\\)和每次的索赔金额\\(Y_l,l=1,\\ldots,N\\)。理论上，车险的纯保费为以下随机和的期望 \\[S=\\sum_{l=1}^N Y_l\\] 假设索赔次数\\(N\\)和索赔金额\\(Y_l\\)独立且\\(Y_l\\)服从独立同分布，则 \\[\\mathbf{E}(S)=\\mathbf{E}(N)\\times\\mathbf{E}(Y)\\] 所以，车险定价问题很多时候都转化为两个独立模型：索赔次数（频率）模型和索赔金额（强度）模型。对于索赔次数模型，通常假设因变量服从泊松分布，建立泊松回归模型，使用的数据量等于保单数；对于索赔金额模型，通常假设因变量服从伽马分布，建立伽马回归模型，使用的数据量等于发生索赔的保单数。通常，在数据量不大时，索赔金额模型的建立难于索赔次数模型，因为只有发生索赔的保单才能用于索赔金额模型的建立。 记第\\(i\\)个保单的风险信息为\\(x_i\\in\\mathcal{X}\\)，保险公司定价的目标就是找到两个（最优）回归方程（映射），使之尽可能准确地预测索赔频率和索赔强度: \\[\\lambda: \\mathcal{X}\\rightarrow \\mathbf{R}_+, ~~~ x \\mapsto \\lambda(x_i)\\] \\[\\mu: \\mathcal{X}\\rightarrow \\mathbf{R}_+, ~~~ x \\mapsto \\mu(x_i)\\] 这里，\\(\\lambda(x_i)\\)是对\\(N\\)的期望的估计，\\(\\mu(x_i)\\)是对\\(Y\\)的期望的估计。基于这两个模型，纯保费估计为\\(\\lambda(x_i)\\mu(x_i)\\)。 1.2 预测模型概述 如何得到一个好的预测模型呢？可以从两个方面考虑： 让风险信息空间\\(\\mathcal{X}\\)丰富，也称为特征工程，比如包含\\(x,x^2,\\ln x\\)、或者加入车联网信息。 让映射空间\\(\\lambda\\in{\\Lambda},\\mu\\in M\\)丰富，如GLM只包含线性效应、相加效应，映射空间较小，神经网络包含非线性效应、交互作用，映射空间较大。 当你选取了映射空间较小的GLM，通常需要进行仔细的特征工程，使得风险信息空间适于GLM；当你选取了映射空间较大的神经网络，通常不需要进行特别仔细的特征工程，神经网络可以自动进行特征工程，发掘风险信息中的有用特征。 对于传统的统计回归模型，GLM，GAM，MARS，我们使用极大似然方法在映射空间中找到最优的回归方程，在极大似然中使用的数据集称为学习集（learning data set）。为了防止过拟合，我们需要进行协变量选择，可以删掉不显著的协变量，也可以使用逐步回归、最优子集、LASSO等，判断标准为AIC等。 对于树模型，我们使用 recursive partitioning by binary splits 算法对风险空间进行划分，使得各子空间内的应变量差异最小，差异通常使用偏差损失（deviance loss）度量。为了防止过拟合，通常使用交叉验证对树的深度进行控制。树模型训练使用的数据为学习集。 树模型的扩展为bootstrap aggregation（bagging）和random forest。第一种算法是对每个bootstrap样本建立树模型，然后平均每个树模型的预测；第二种算法类似第一种，但在建立树模型时，要求只在某些随机选定的协变量上分支。这两种扩展都属于集成学习（ensemble learning）。 提升算法有多种不同形式，它的核心思想类似逐步回归，区别是每步回归中需要依据上步的预测结果调整各个样本的权重，让上步预测结果差的样本在下步回归中占的权重较大。通常，每步回归使用的模型比较简单，如深度为3的树模型。提升算法也属于集成学习，和前面不同是它的弱学习器不是独立的，而bagging和random forest的弱学习器是彼此独立的。 对于集成算法，通常需要调整弱学习器的结构参数，如树的深度，也要判断弱学习器的个数，这些称为tuning parameters，通常通过比较在验证集（validation）的损失进行调参，防止过拟合。弱学习器中的参数通过在训练集（training）上训练模型得到。训练集和验证集的并集为学习集。 前馈神经网络的输入神经元为风险信息，下一层神经元为上一层神经元的线性组合并通过激活函数的非线性变换，最后输出神经元为神经网络对因变量期望的预测，通过减小输出神经元与因变量观察值的差异，训练神经网络中的参数。神经网络含有非常多的参数，很难找到全局最优解，而且最优解必然造成过拟合，所以一般采用梯度下降法对参数进行迭代，使得训练集损失在每次迭代中都有下降趋势。通过比较验证集损失确定迭代次数和神经网络的结构参数，防止过拟合。 如何评价一个预测模型的好坏呢？通常用样本外损失（test error）评价。对于索赔频率，使用泊松偏差损失，对于索赔强度，使用伽马偏差损失，可以证明这两个损失函数和似然函数成负相关。其中，平均泊松偏差损失为： \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{2}{|\\mathbf{N}|}\\sum_{i}N_i\\left[\\frac{\\hat{N}_i}{N_i}-1-\\ln\\left(\\frac{\\hat{N}_i}{N_i}\\right)\\right]\\] Keras中定义的损失函数为 \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] 1.3 特征工程 加载包。 rm(list=ls()) library(CASdatasets) library(keras) library(data.table) library(glmnet) library(plyr) library(mgcv) library(rpart) # library(rpart.plot) library(Hmisc) # library(randomForest) # library(distRforest) # devtools::install_github(&#39;henckr/distRforest&#39;) library(gbm) data(freMTPL2freq) #data(freMTPL2sev) textwidth&lt;-7.3 #inch #fwrite(freMTPL2freq,&quot;data/freMTPL2freq.txt&quot;) #freMTPL2freq&lt;-fread(&quot;data/freMTPL2freq_mac.txt&quot;) 1.3.1 截断 减少outliers/influential points 的影响 需根据每个变量的分布确定在哪里截断 索赔次数在4截断 风险暴露在1截断 马力在9截断 车龄在20截断 年龄在90截断 奖惩系数在150截断 1.3.2 离散化 目的是为了刻画非线性效应 需画出协变量的边缘经验索赔频率判断 离散化马力、车龄、年龄 VehPowerFac, VehAgeFac，DrivAgeFac 1.3.3 设定基础水平 方便假设检验 设定含有最多风险暴露的水平为基准水平 1.3.4 协变量变形 目的是为了刻画非线性效应 考虑协变量分布，使之变形后近似服从对称分布 DriveAgeLn/2/3/4, DensityLn dat &lt;- freMTPL2freq dat1 &lt;- dat # claim number dat1$ClaimNb &lt;- pmin(dat1$ClaimNb, 4) # exposure dat1$Exposure &lt;- pmin(dat1$Exposure, 1) # vehicle power dat1$VehPowerFac &lt;- as.factor(pmin(dat1$VehPower,9)) aggregate(dat1$Exposure,by=list(dat1$VehPowerFac),sum) dat1[,&quot;VehPowerFac&quot;] &lt;-relevel(dat1[,&quot;VehPowerFac&quot;], ref=&quot;6&quot;) # vehicle age dat1$VehAge &lt;- pmin(dat1$VehAge,20) VehAgeFac &lt;- cbind(c(0:110), c(1, rep(2,5), rep(3,5),rep(4,5), rep(5,5), rep(6,111-21))) dat1$VehAgeFac &lt;- as.factor(VehAgeFac[dat1$VehAge+1,2]) aggregate(dat1$Exposure,by=list(dat1$VehAgeFac),sum) dat1[,&quot;VehAgeFac&quot;] &lt;-relevel(dat1[,&quot;VehAgeFac&quot;], ref=&quot;2&quot;) # driver age dat1$DrivAge &lt;- pmin(dat1$DrivAge,90) DrivAgeFac &lt;- cbind(c(18:100), c(rep(1,21-18), rep(2,26-21), rep(3,31-26), rep(4,41-31), rep(5,51-41), rep(6,71-51), rep(7,101-71))) dat1$DrivAgeFac &lt;- as.factor(DrivAgeFac[dat1$DrivAge-17,2]) aggregate(dat1$Exposure,by=list(dat1$DrivAgeFac),sum) dat1[,&quot;DrivAgeFac&quot;] &lt;-relevel(dat1[,&quot;DrivAgeFac&quot;], ref=&quot;6&quot;) dat1$DrivAgeLn&lt;-log(dat1$DrivAge) dat1$DrivAge2&lt;-dat1$DrivAge^2 dat1$DrivAge3&lt;-dat1$DrivAge^3 dat1$DrivAge4&lt;-dat1$DrivAge^4 # bms dat1$BonusMalus &lt;- as.integer(pmin(dat1$BonusMalus, 150)) # vehicle brand dat1$VehBrand &lt;- factor(dat1$VehBrand) # consider VehGas as categorical aggregate(dat1$Exposure,by=list(dat1$VehBrand),sum) dat1[,&quot;VehBrand&quot;] &lt;-relevel(dat1[,&quot;VehBrand&quot;], ref=&quot;B1&quot;) # vehicle gas dat1$VehGas &lt;- factor(dat1$VehGas) # consider VehGas as categorical aggregate(dat1$Exposure,by=list(dat1$VehGas),sum) dat1[,&quot;VehGas&quot;] &lt;-relevel(dat1[,&quot;VehGas&quot;], ref=&quot;Regular&quot;) # area (related to density) dat1$Area &lt;- as.integer(dat1$Area) # density dat1$DensityLn &lt;- as.numeric(log(dat1$Density)) # region aggregate(dat1$Exposure,by=list(dat1$Region),sum)[order(aggregate(dat1$Exposure,by=list(dat1$Region),sum)$x),] dat1[,&quot;Region&quot;] &lt;-relevel(dat1[,&quot;Region&quot;], ref=&quot;Centre&quot;) str(dat1) # model matrix for GLM design_matrix&lt;-model.matrix( ~ ClaimNb + Exposure + VehPowerFac + VehAgeFac + DrivAge + DrivAgeLn + DrivAge2 + DrivAge3 + DrivAge4 + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge as factor variables # design_matrix2&lt;-model.matrix( ~ ClaimNb + Exposure + VehPower + VehAge + DrivAge + # BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge, and DrivAge as continuous variables # dim(design_matrix2) 1.4 训练集-验证集-测试集 比例为\\(0.6:0.2:0.2\\) 根据索赔次数分层抽样 经验索赔频率约为\\(10%\\) seed_split&lt;-11 # claim 0/1 proportions index_zero&lt;-which(dat1$ClaimNb==0) index_one&lt;-which(dat1$ClaimNb&gt;0) prop_zero&lt;-round(length(index_zero)/(length(index_one)+length(index_zero)),2);prop_zero prop_one&lt;-round(length(index_one)/(length(index_one)+length(index_zero)),2);prop_one # 0.6:0.2:0.2 size_valid&lt;-round(nrow(dat1)*0.2,0) size_test&lt;-size_valid size_train&lt;-nrow(dat1)-2*size_valid # stratified sampling set.seed(seed_split) index_train_0&lt;-sample(index_zero,size_train*prop_zero) index_train_1&lt;-sample(index_one, size_train-length(index_train_0)) index_train&lt;-union(index_train_0,index_train_1) length(index_train);size_train index_valid&lt;-c(sample(setdiff(index_zero,index_train_0),round(size_valid*prop_zero,0)), sample(setdiff(index_one,index_train_1),size_valid-round(size_valid*prop_zero,0))) length(index_valid);size_valid index_test&lt;-setdiff(union(index_zero,index_one),union(index_train,index_valid)) index_learn&lt;-union(index_train,index_valid) length(index_train);length(index_valid);length(index_test) # train-validation-test; learn-test的 dat1_train&lt;-dat1[index_train,] dat1_valid&lt;-dat1[index_valid,] dat1_test&lt;-dat1[index_test,] dat1_learn&lt;-dat1[index_learn,] sum(dat1_train$ClaimNb)/sum(dat1_train$Exposure) sum(dat1_valid$ClaimNb)/sum(dat1_valid$Exposure) sum(dat1_test$ClaimNb)/sum(dat1_test$Exposure) sum(dat1_learn$ClaimNb)/sum(dat1_learn$Exposure) # glm matrix matrix_train&lt;-design_matrix[index_train,] matrix_valid&lt;-design_matrix[index_valid,] matrix_test&lt;-design_matrix[index_test,] matrix_learn&lt;-design_matrix[index_learn,] # gbm matrix (learn) dat1_learn_gbm&lt;-dat1_learn[,c(&quot;ClaimNb&quot;, &quot;Exposure&quot;, &quot;VehPower&quot;, &quot;VehAge&quot;, &quot;DrivAge&quot;, &quot;BonusMalus&quot;, &quot;VehBrand&quot;, &quot;VehGas&quot;, &quot;Area&quot;, &quot;DensityLn&quot;, &quot;Region&quot;)] class(dat1_learn_gbm) train_pro&lt;-size_train/(size_train+size_valid) 1.5 泊松偏差损失函数 平均泊松偏差损失 \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{2}{|\\mathbf{N}|}\\sum_{i}N_i\\left[\\frac{\\hat{N}_i}{N_i}-1-\\ln\\left(\\frac{\\hat{N}_i}{N_i}\\right)\\right]\\] Keras定义平均泊松偏差损失为 \\[\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})=\\frac{1}{|\\mathbf{N}|}\\sum_{i}\\left[\\hat{N}_i-N_i\\ln\\left(\\hat{N}_i\\right)\\right]\\] 因为对于大部分保单，\\(N_i-N_i\\ln N_i\\approx0\\)，所以泊松偏差损失函数约为Keras定义的2倍（至少在一个量级）。 \\[\\mathcal{L}(\\mathbf{N},\\mathbf{\\hat{N}})\\approx2\\tilde{\\mathcal{L}}(\\mathbf{N},\\mathbf{\\hat{N}})\\] Poisson.Deviance &lt;- function(pred,obs) {200*(sum(pred)-sum(obs)+sum(log((obs/pred)^(obs))))/length(pred)} keras_poisson_dev&lt;-function(y_hat,y_true) {100*sum(y_hat-y_true*log(y_hat))/length(y_true)} f_keras&lt;-function(x) 100*(x-x*log(x)) f_keras(0.1);f_keras(0.2) # png(&quot;./plots/poi_dev.png&quot;) plot(seq(0.05,0.15,0.01),f_keras(seq(0.05,0.15,0.01)),type=&quot;l&quot;, xlab=&quot;frequency&quot;,ylab=&quot;approximated Poisson deviance&quot;) abline(v=0.1,lty=2);abline(h=f_keras((0.1)),lty=2) # dev.off() knitr::opts_chunk$set(fig.pos = &quot;!H&quot;, out.extra = &quot;&quot;) knitr::include_graphics(&quot;./plots/poi_dev.png&quot;) 1.6 GLM &amp; GAM 使用极大似然方法在映射空间中找到最优的回归方程 在极大似然中使用的数据集称为学习集（learning data set）。 为了防止过拟合，我们需要进行协变量选择，可以删掉不显著的协变量，也可以使用逐步回归、最优子集、LASSO等，判断标准为AIC等。 1.6.1 GLM 同质模型 \\[\\mathbf{E}(N)=\\beta_0\\] 全模型 \\[\\ln \\mathbf{E}(N)=\\ln e + \\beta_0 + \\beta_{\\text{VehPowerFac}} + \\beta_{\\text{VehAgeFac}} \\\\ + \\beta_1\\text{DrivAge} + \\beta_2\\ln\\text{DrivAge} + \\beta_3\\text{DrivAge}^2 + \\beta_4\\text{DrivAge}^3 + \\beta_5\\text{DrivAge}^4 \\\\ \\beta_6\\text{BM} + \\beta_{\\text{VehBrand}} + \\beta_{\\text{VehGas}} + \\beta_7\\text{Area} + \\beta_8\\text{DensityLn} + \\beta_{\\text{Region}}\\] # homogeneous model d.glm0 &lt;- glm(ClaimNb ~ 1 + offset(log (Exposure)), data=data.frame(matrix_learn), family=poisson()) #summary(d.glm0) dat1_test$fitGLM0 &lt;- predict(d.glm0, newdata=data.frame(matrix_test), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM0,matrix_test[,1]) Poisson.Deviance(dat1_test$fitGLM0,matrix_test[,1]) # full GLM names(data.frame(matrix_learn)) {t1 &lt;- proc.time() d.glm1 &lt;- glm(ClaimNb ~ .-Exposure + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson()) (proc.time()-t1)} # summary(d.glm1) dat1_train$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_train), type=&quot;response&quot;) dat1_valid$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_valid), type=&quot;response&quot;) dat1_test$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_test), type=&quot;response&quot;) dat1_learn$fitGLM1 &lt;- predict(d.glm1, newdata=data.frame(matrix_learn), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM1,matrix_test[,1]) Poisson.Deviance(dat1_test$fitGLM1,matrix_test[,1]) 1.6.2 GAM GAM边缘提升模型 \\[\\ln \\mathbf{E}(N)=\\ln\\hat{\\lambda}_{\\text{GLM}}+s_1(\\text{VehAge})+s_2(\\text{BM})\\] \\(s_1,s_2\\)为样条平滑函数。 使用ddply聚合数据，找到充分统计量，加快模型拟合速度。 # GAM marginals improvement (VehAge and BonusMalus) {t1 &lt;- proc.time() dat.GAM &lt;- ddply(dat1_learn, .(VehAge, BonusMalus), summarise, fitGLM1=sum(fitGLM1), ClaimNb=sum(ClaimNb)) set.seed(1) d.gam &lt;- gam(ClaimNb ~ s(VehAge, bs=&quot;cr&quot;)+s(BonusMalus, bs=&quot;cr&quot;) + offset(log(fitGLM1)), data=dat.GAM, method=&quot;GCV.Cp&quot;, family=poisson) (proc.time()-t1)} summary(d.gam) dat1_train$fitGAM1 &lt;- predict(d.gam, newdata=dat1_train,type=&quot;response&quot;) dat1_valid$fitGAM1 &lt;- predict(d.gam, newdata=dat1_valid,type=&quot;response&quot;) dat1_test$fitGAM1 &lt;- predict(d.gam, newdata=dat1_test,type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGAM1, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitGAM1,matrix_test[,1]) 1.6.3 Step wise、LASSO协变量选择 逐步回归非常慢，在Linux 8核i7 3.4GHz 16G内存都需要50多分钟。且样本外损失和全模型没有明显减小。 5折CV Lasso在Linux 8核i7 3.4GHz 16G内存需要5分钟。 根据5折CV-error选取正则参数beta=4*10^-5，但样本外损失和全模型没有明显减小。 # step wise selection； this takes a long time (more than 50 minutes!) # d.glm00 &lt;- glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + # DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + # BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + # offset(log (Exposure)), # data=data.frame(matrix_learn), family=poisson()) # {t1 &lt;- proc.time() # d.glm2&lt;-step(d.glm00,direction=&quot;forward&quot;,trace = 1, # scope =list(lower=formula(d.glm00), upper=formula(d.glm1))) # (proc.time()-t1)} d.glm2&lt;-glm(ClaimNb ~ VehAgeFac1 + VehAgeFac3 + VehAgeFac4 + VehAgeFac5 + DrivAge + DrivAge2 + DrivAge3 + DrivAge4 + DrivAgeLn + BonusMalus + VehBrandB12 + VehGasDiesel + DensityLn + VehPowerFac4 + VehPowerFac8 + RegionNord.Pas.de.Calais + VehPowerFac7 + RegionRhone.Alpes + RegionBretagne + RegionAuvergne + RegionLimousin + RegionLanguedoc.Roussillon + RegionIle.de.France + RegionAquitaine + RegionMidi.Pyrenees + RegionPays.de.la.Loire + RegionProvence.Alpes.Cotes.D.Azur + RegionPoitou.Charentes + RegionHaute.Normandie + VehBrandB5 + VehBrandB11 + RegionBasse.Normandie + VehBrandB14 + RegionCorse + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson()) summary(d.glm2) dat1_test$fitGLM2 &lt;- predict(d.glm2, newdata=data.frame(matrix_test), type=&quot;response&quot;) keras_poisson_dev(dat1_test$fitGLM2,data.frame(matrix_test)$ClaimNb) Poisson.Deviance(dat1_test$fitGLM2,matrix_test[,1]) # lasso regression； this takes a few minutes alpha0=1 # 1 for lasso, 0 for ridge. set.seed(7) # {t1 &lt;- proc.time() # cvfit = cv.glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], # family = &quot;poisson&quot;,offset=log(matrix_learn[,2]), # alpha = alpha0,nfolds = 5,trace.it = 1) # (proc.time()-t1)} # cvfit$lambda.min #4*10^-5 # cvfit$lambda.1se # 0.0016 # plot(cvfit) d.glm3 = glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], family = &quot;poisson&quot;, offset=log(matrix_learn[,2]), alpha=alpha0, lambda=4.024746e-05, trace.it = 1) dat1_test$fitLasso&lt;-predict(d.glm3, newx = matrix_test[,-c(1,2)], newoffset=log(matrix_test[,2]),type = &quot;response&quot;) keras_poisson_dev(dat1_test$fitLasso, matrix_test[,1]) Poisson.Deviance(dat1_test$fitLasso, matrix_test[,1]) 1.7 Possion tree 使用 recursive partitioning by binary splits 算法对风险空间进行划分，使得各子空间内的应变量差异最小 为了防止过拟合，使用交叉验证确定cost-complexity parameter cp=10^-3.32(1-SD rule)或者cp=10^-4(min CV rule)，进而对树的深度进行控制。 # cross validation using xval in rpart.control names(dat1_learn) set.seed(1) {t1 &lt;- proc.time() tree0&lt;-rpart(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_learn, method = &quot;poisson&quot;, seed =1, control = rpart.control (xval=5, minbucket=1000, cp=10^-5, maxcompete = 0, maxsurrogate = 0)) (proc.time()-t1)} # printcp(tree0) x0 &lt;- log10(tree0$cptable[,1]) err0&lt;-tree0$cptable[,4] std0&lt;-tree0$cptable[,5] xmain &lt;- &quot;cross-validation error plot&quot; xlabel &lt;- &quot;cost-complexity parameter (log-scale)&quot; ylabel &lt;- &quot;relative CV error&quot; # png(&quot;./plots/tree_cv.png&quot;) errbar(x=x0, y=err0*100, yplus=(err0+std0)*100, yminus=(err0-std0)*100, xlim=rev(range(x0)), col=&quot;blue&quot;, main=xmain, ylab=ylabel, xlab=xlabel) lines(x=x0, y=err0*100, col=&quot;blue&quot;) abline(h=c(min(err0+std0)*100), lty=1, col=&quot;orange&quot;) abline(h=c(min(err0)*100), lty=1, col=&quot;magenta&quot;) abline(v=-3.32,lty=2) legend(x=&quot;topright&quot;, col=c(&quot;blue&quot;, &quot;orange&quot;, &quot;magenta&quot;,&quot;black&quot;), lty=c(1,1,1,2), lwd=c(1,1,1,1), pch=c(19,-1,-1,-1), legend=c(&quot;tree0&quot;, &quot;1-SD rule&quot;, &quot;min.CV rule&quot;,&quot;log cp = -3.32&quot;)) # dev.off() tree1 &lt;- prune(tree0, cp=10^-3.32) # cp=10^-3.32 tree11&lt;- prune(tree0, cp=tree0$cptable[which.min(err0),1]) # cp=10^-4 # printcp(tree1) # printcp(tree11) # tree1 dat1_test$fitRT_1se &lt;- predict(tree1, newdata=dat1_test)*dat1_test$Exposure dat1_test$fitRT_min &lt;- predict(tree11, newdata=dat1_test)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitRT_1se, dat1_test$ClaimNb) keras_poisson_dev(dat1_test$fitRT_min, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT_1se, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT_min, dat1_test$ClaimNb) tree1$variable.importance tree11$variable.importance knitr::opts_chunk$set(fig.pos = &quot;!H&quot;, out.extra = &quot;&quot;) knitr::include_graphics(&quot;./plots/tree_cv.png&quot;) 交叉验证可使用rpart(..., control=rpart.control(xval= ,...))或者xpred.rpart(tree, group)。 以上两种方式得到相同的剪枝树，cp=10^-4（min CV rule） Variable importance (min CV rule) BonusMalus VehAge VehBrand DrivAge VehGas VehPower Region DensityLn 4675.0231 4396.8667 1389.2909 877.9473 795.6308 715.3584 480.3459 140.5463 # K-fold cross-validation using xpred.rpart set.seed(1) {t1 &lt;- proc.time() tree00&lt;-rpart(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_learn, method = &quot;poisson&quot;, seed = 1, control = rpart.control (xval=1, minbucket=1000 ,cp=5*10^-5, maxcompete = 0, maxsurrogate = 0)) (proc.time()-t1)} (n_subtrees &lt;- dim(tree00$cptable)[1]) std1&lt;- numeric(n_subtrees) err1 &lt;- numeric(n_subtrees) K &lt;- 10 xgroup &lt;- rep(1:K, length = nrow(dat1_learn)) xfit &lt;- xpred.rpart(tree00, xgroup) dim(xfit) for (i in 1:n_subtrees){ err_group&lt;-rep(NA,K) for (k in 1:K){ ind_group &lt;- which(xgroup ==k) err_group[k] &lt;- keras_poisson_dev(dat1_learn[ind_group,&quot;Exposure&quot;]*xfit[ind_group,i], dat1_learn[ind_group,&quot;ClaimNb&quot;]) } err1[i] &lt;- mean(err_group) std1[i] &lt;- sd(err_group) } x1 &lt;- log10(tree00$cptable[,1]) xmain &lt;- &quot;cross-validation error plot&quot; xlabel &lt;- &quot;cost-complexity parameter (log-scale)&quot; ylabel &lt;- &quot;CV error (in 10^(-2))&quot; errbar(x=x1, y=err1*100, yplus=(err1+std1)*100, yminus=(err1-std1)*100, xlim=rev(range(x1)), col=&quot;blue&quot;, main=xmain, ylab=ylabel, xlab=xlabel) lines(x=x1, y=err1*100, col=&quot;blue&quot;) abline(h=c(min(err1+std1)*100), lty=1, col=&quot;orange&quot;) abline(h=c(min(err1)*100), lty=1, col=&quot;magenta&quot;) abline(v=-3.12,lty=2) legend(x=&quot;topright&quot;, col=c(&quot;blue&quot;, &quot;orange&quot;, &quot;magenta&quot;,&quot;black&quot;), lty=c(1,1,1,2), lwd=c(1,1,1,1), pch=c(19,-1,-1,-1), legend=c(&quot;tree1&quot;, &quot;1-SD rule&quot;, &quot;min.CV rule&quot;,&quot;log cp = -3.12&quot;)) tree2 &lt;- prune(tree00, cp=10^-3.12) tree22 &lt;- prune(tree00, cp=tree00$cptable[which.min(err1),1]) printcp(tree2) printcp(tree22) dat1_test$fitRT2 &lt;- predict(tree2, newdata=dat1_test)*dat1_test$Exposure dat1_test$fitRT22 &lt;- predict(tree22, newdata=dat1_test)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitRT2, dat1_test$ClaimNb) keras_poisson_dev(dat1_test$fitRT22, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT2, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRT22, dat1_test$ClaimNb) sum((dat1_test$fitRT22-dat1_test$fitRT11)^2) tree2$variable.importance tree22$variable.importance tree11$variable.importance 1.8 Random forest 使用https://github.com/henckr/distRforest建立泊松随机森林。 ncand每次分裂考虑的协变量个数；subsample训练每棵树的样本。 使用验证损失确定树的数量。 # fit the random forest library(distRforest) ntrees0&lt;-100 set.seed(1) {t1 &lt;- proc.time() forest1&lt;-rforest(cbind(Exposure, ClaimNb) ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region, data = dat1_train, method = &quot;poisson&quot;, control = rpart.control (xval=0, minbucket=1000 ,cp=5*10^-5, maxcompete = 0,maxsurrogate = 0), parms=list(shrink=0), ncand=5,ntrees = ntrees0, subsample = 0.5, red_mem = T) (proc.time()-t1)} # determine number of trees using validation error fit_valid&lt;-rep(0,nrow(dat1_valid)) error_valid&lt;-rep(0,ntrees0) for (i in 1:ntrees0){ fit_valid&lt;-fit_valid + predict(forest1$trees[[i]], newdata=dat1_valid) * dat1_valid$Exposure fit_valid_norm &lt;- fit_valid/i error_valid[i]&lt;-Poisson.Deviance(fit_valid_norm, dat1_valid$ClaimNb) } # png(&quot;./plots/random_forest_error.png&quot;) plot(error_valid,type=&quot;l&quot;,xlab=&quot;number of trees&quot;,ylab=&quot;validation error in 10^-2&quot;) abline(v=which.min(error_valid),lty=2) # dev.off() (best.trees=which.min(error_valid)) # test error fitRF&lt;-rep(0,nrow(dat1_test)) for (i in 1:best.trees){ fitRF&lt;-fitRF+predict(forest1$trees[[i]], newdata=dat1_test)*dat1_test$Exposure } dat1_test$fitRF &lt;- fitRF/best.trees keras_poisson_dev(dat1_test$fitRF, dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitRF, dat1_test$ClaimNb) names(forest1$trees[[2]]$variable.importance) sum(forest1$trees[[3]]$variable.importance) knitr::opts_chunk$set(fig.pos = &quot;!H&quot;, out.extra = &quot;&quot;) knitr::include_graphics(&quot;./plots/random_forest_error.png&quot;) 1.9 Boosting Poisson tree n.trees 树的数量；shrinkage 学习步长，和树的数量成反比；interaction.depth 交互项深度；bag.fraction 每棵树使用的数据比例；train.fraction 训练集比例；n.minobsinnode叶子上最少样本量。 Variable importance rel.inf BonusMalus 27.687137 VehAge 19.976441 VehBrand 13.515198 Region 13.495375 DrivAge 9.284520 VehGas 7.082648 VehPower 4.583522 DensityLn 4.375159 Area 0.000000 set.seed(1) {t1 &lt;- proc.time() gbm1 &lt;- gbm( ClaimNb ~ VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + Area + DensityLn + Region + offset(log(Exposure)), data = dat1_learn_gbm, distribution = &quot;poisson&quot;, n.trees = 200, shrinkage = 0.3, interaction.depth = 5, bag.fraction = 0.5, train.fraction = train_pro, cv.folds = 0, n.minobsinnode = 1000, verbose = T ) (proc.time()-t1)} # plot the performance # png(&quot;./plots/gbm_error.png&quot;) gbm.perf(gbm1,method=&quot;test&quot;) legend(&quot;topright&quot;,lty=c(1,1,2),col=c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;),c(&quot;training error&quot;, &quot;validation error&quot;, &quot;best iterations&quot;)) # dev.off() best.iter&lt;-gbm.perf(gbm1,method=&quot;test&quot;) dat1_test$fitGBM1&lt;-predict(gbm1, dat1_test,n.trees=best.iter,type=&quot;response&quot;)*dat1_test$Exposure keras_poisson_dev(dat1_test$fitGBM1,dat1_test$ClaimNb) Poisson.Deviance(dat1_test$fitGBM1,dat1_test$ClaimNb) # plot variable importance # png(&quot;./plots/gbm_imp.png&quot;) summary(gbm1) # dev.off() gbm1 # plot the important variable after &quot;best&quot; iterations # png(&quot;./plots/gbm_mar.png&quot;) par(mfrow=c(2,2)) plot(gbm1,4,best.iter) plot(gbm1,2,best.iter) plot(gbm1,5,best.iter) plot(gbm1,3,best.iter) # dev.off() par(mfrow=c(2,2)) plot(gbm1,c(4,2),best.iter,col=terrain.colors(20)) plot(gbm1,c(4,5),best.iter) plot(gbm1,c(4,3),best.iter) plot(gbm1,c(2,3),best.iter) 1.10 Summary dev_sum &lt;- data.frame(model=c(&quot;Intercept&quot;,&quot;GLM&quot;,&quot;GLM Lasso&quot;,&quot;GAM&quot;,&quot;Decision tree&quot;, &quot;Random forest&quot;,&quot;Generalized boosted model&quot;), test_error=rep(NA,7),test_error_keras=rep(NA,7)) dev_sum$test_error[1]&lt;-Poisson.Deviance(dat1_test$fitGLM0,matrix_test[,1]) dev_sum$test_error_keras[1]&lt;-keras_poisson_dev(dat1_test$fitGLM0,matrix_test[,1]) dev_sum$test_error[2]&lt;-Poisson.Deviance(dat1_test$fitGLM1,matrix_test[,1]) dev_sum$test_error_keras[2]&lt;-keras_poisson_dev(dat1_test$fitGLM1,matrix_test[,1]) dev_sum$test_error[3]&lt;-Poisson.Deviance(dat1_test$fitLasso,matrix_test[,1]) dev_sum$test_error_keras[3]&lt;-keras_poisson_dev(dat1_test$fitLasso,matrix_test[,1]) dev_sum$test_error[4]&lt;-Poisson.Deviance(dat1_test$fitGAM1, dat1_test$ClaimNb) dev_sum$test_error_keras[4]&lt;-keras_poisson_dev(dat1_test$fitGAM1, dat1_test$ClaimNb) dev_sum$test_error[5]&lt;-Poisson.Deviance(dat1_test$fitRT_min, dat1_test$ClaimNb) dev_sum$test_error_keras[5]&lt;-keras_poisson_dev(dat1_test$fitRT_min, dat1_test$ClaimNb) dev_sum$test_error[6]&lt;-Poisson.Deviance(dat1_test$fitRF, dat1_test$ClaimNb) dev_sum$test_error_keras[6]&lt;-keras_poisson_dev(dat1_test$fitRF, dat1_test$ClaimNb) dev_sum$test_error[7]&lt;-Poisson.Deviance(dat1_test$fitGBM1,dat1_test$ClaimNb) dev_sum$test_error_keras[7]&lt;-keras_poisson_dev(dat1_test$fitGBM1,dat1_test$ClaimNb) dev_sum[,2:3]&lt;-round(dev_sum[,2:3],4) write.csv(dev_sum,&quot;./plots/dev_sum.csv&quot;) dev_sum "]]
