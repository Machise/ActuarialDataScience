---
output: html_document
editor_options: 
  chunk_output_type: console
---
load packages
```{r} 
library(CASdatasets)
library(keras)
library(data.table)
library(glmnet)
library(plyr) # ddply
library(mgcv)
data(freMTPL2freq)
data(freMTPL2sev)
#fwrite(freMTPL2freq,"data/freMTPL2freq.txt")
#freMTPL2freq<-fread("data/freMTPL2freq_mac.txt")
```

# Models GLM2, neural net with embeddings, and CANN
## clean data
```{r}
dat <- freMTPL2freq
dat$VehGas <- factor(dat$VehGas)      # consider VehGas as categorical
dat$ClaimNb <- pmin(dat$ClaimNb, 4)   # correct for unreasonable observations (that might be data error)
dat$Exposure <- pmin(dat$Exposure, 1) # correct for unreasonable observations (that might be data error)
str(dat)
```

## Poisson deviance statistics
```{r}
Poisson.Deviance <- function(pred, obs){200*(sum(pred)-sum(obs)+sum(log((obs/pred)^(obs))))/length(pred)}
keras_poisson_dev<-function(y_hat,y_true){
  sum(y_hat-y_true*log(y_hat))/length(y_true)
}
```

## feature pre-processing for GLM
```{r}
dat1 <- dat
dat1$Area <- as.integer(dat1$Area)
dat1$VehPowerFac <- as.factor(pmin(dat1$VehPower,9))
dat1[,"VehPowerFac"] <-relevel(dat1[,"VehPowerFac"], ref="6")
dat1$VehAge <- pmin(dat1$VehAge,20)
VehAgeFac <- cbind(c(0:110), c(1, rep(2,5), rep(3,5),rep(4,5), rep(5,5), rep(6,111-21)))
dat1$VehAgeFac <- as.factor(VehAgeFac[dat1$VehAge+1,2])
dat1[,"VehAgeFac"] <-relevel(dat1[,"VehAgeFac"], ref="2")
dat1$DrivAge <- pmin(dat1$DrivAge,90)
DrivAgeFac <- cbind(c(18:100), c(rep(1,21-18), rep(2,26-21), rep(3,31-26), rep(4,41-31), rep(5,51-41), rep(6,71-51), rep(7,101-71)))
dat1$DrivAgeFac <- as.factor(DrivAgeFac[dat1$DrivAge-17,2])
dat1[,"DrivAgeFac"] <-relevel(dat1[,"DrivAgeFac"], ref="5")
dat1$DrivAgeLn<-log(dat1$DrivAge)
dat1$DrivAge2<-dat1$DrivAge^2
dat1$DrivAge3<-dat1$DrivAge^3
dat1$DrivAge4<-dat1$DrivAge^4
dat1$BonusMalus <- as.integer(pmin(dat1$BonusMalus, 150))
dat1$DensityLn <- as.numeric(log(dat1$Density))
dat1[,"Region"] <-relevel(dat1[,"Region"], ref="R24")
str(dat1)
design_matrix<-model.matrix( ~ ClaimNb + Exposure + Area + VehPowerFac + VehAgeFac + DrivAge + DrivAgeLn + DrivAge2 + DrivAge3 + DrivAge4 + BonusMalus + VehBrand + VehGas + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge, and DrivAge as factor variables
#design_matrix2<-model.matrix( ~ ClaimNb + Exposure + Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + DensityLn + Region, data=dat1)[,-1] # VehPower, VehAge, and DrivAge as continuous variables
#dim(design_matrix2)
```

## choosing training\validation\test split
```{r}
seed_split<-11
summary(dat1$Exposure)
sum(dat1$Exposure==1)
hist(dat1$Exposure[dat1$Exposure<1])
exp_claim<-aggregate(dat1$Exposure,by=list(as.factor(dat1$ClaimNb)),sum)
sum(dat1$ClaimNb>1)
sum(dat1$ClaimNb==1)
round(exp_claim$x/sum(exp_claim$x),2)
index_zero<-which(dat1$ClaimNb==0)
index_one<-which(dat1$ClaimNb>0)
prop_zero<-round(length(index_zero)/(length(index_one)+length(index_zero)),2);prop_zero
prop_one<-round(length(index_one)/(length(index_one)+length(index_zero)),2);prop_one
size_valid<-round(nrow(dat1)*0.2,0)
size_test<-size_valid
size_train<-nrow(dat1)-2*size_valid
set.seed(seed_split)
index_train_0<-sample(index_zero,size_train*prop_zero)
index_train_1<-sample(index_one, size_train-length(index_train_0))
index_train<-union(index_train_0,index_train_1)
length(index_train);size_train
index_valid<-c(sample(setdiff(index_zero,index_train_0),round(size_valid*prop_zero,0)), sample(setdiff(index_one,index_train_1),size_valid-round(size_valid*prop_zero,0)))
length(index_valid);size_valid
index_test<-setdiff(union(index_zero,index_one),union(index_train,index_valid))
index_learn<-union(index_train,index_valid)
length(index_train);length(index_valid);length(index_test)
dat1_train<-dat1[index_train,]
dat1_valid<-dat1[index_valid,]
dat1_test<-dat1[index_test,]
dat1_learn<-dat1[index_learn,]
sum(dat1_train$ClaimNb)/sum(dat1_train$Exposure)
sum(dat1_valid$ClaimNb)/sum(dat1_valid$Exposure)
sum(dat1_test$ClaimNb)/sum(dat1_test$Exposure)
sum(dat1_learn$ClaimNb)/sum(dat1_learn$Exposure)
matrix_train<-design_matrix[index_train,]
matrix_valid<-design_matrix[index_valid,]
matrix_test<-design_matrix[index_test,]
matrix_learn<-design_matrix[index_learn,]
```

##  GLM analysis
```{r}
d.glm0 <- glm(ClaimNb ~ 1 + offset(log (Exposure)), data=data.frame(matrix_learn), family=poisson())
#summary(d.glm0)
dat1_test$fitGLM0 <- predict(d.glm0, newdata=data.frame(matrix_test), type="response")
keras_poisson_dev(dat1_test$fitGLM0,matrix_test[,1])

{t1 <- proc.time()
d.glm1 <- glm(ClaimNb ~ .-Exposure + offset(log(Exposure)), data=data.frame(matrix_learn), family=poisson())
(proc.time()-t1)}
#summary(d.glm1)
dat1_train$fitGLM1 <- predict(d.glm1, newdata=data.frame(matrix_train), type="response")
dat1_valid$fitGLM1 <- predict(d.glm1, newdata=data.frame(matrix_valid), type="response")
dat1_test$fitGLM1 <- predict(d.glm1, newdata=data.frame(matrix_test), type="response")
dat1_learn$fitGLM1 <- predict(d.glm1, newdata=data.frame(matrix_learn), type="response")
keras_poisson_dev(dat1_test$fitGLM1,matrix_test[,1])

# stepwise selection； this takes a few minutes
#{t1 <- proc.time()
#d.glm2<-step(d.glm0,direction="both",trace = 0,scope = list(lower=formula(d.glm0),upper=formula(d.glm1)))
#(proc.time()-t1)}
#summary(d.glm2)
#dat1_test$fitGLM2 <- predict(d.glm2, newdata=data.frame(matrix_test), type="response")
#keras_poisson_dev(dat1_test$fitGLM2,data.frame(matrix_test)$ClaimNb)

# lasso regression； this takes a few minutes
#alpha0=1 # 1 for lasso, 0 for ridge.
#d.glm3 = glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], family = "poisson", offset=log(matrix_learn[,2]), alpha=alpha0)
#plot(d.glm3,label="T")
#set.seed(7)
#{t1 <- proc.time()
#cvfit = cv.glmnet(matrix_learn[,-c(1,2)], matrix_learn[,1], family = "poisson", offset=log(matrix_learn[,2]),alpha=alpha0)
#(proc.time()-t1)}
#plot(cvfit);min(cvfit$cvm)
#opt.lam = c(cvfit$lambda.min, cvfit$lambda.1se) # min= 5.320478e-05, 1se= 2.198428e-03
#coef(cvfit, s = opt.lam)
#dat1_test$fitGLM3<-predict(d.glm3, newx = matrix_test[,-c(1,2)], newoffset=log(matrix_test[,2]),type = "response", s = opt.lam[1])
#keras_poisson_dev(dat1_test$fitGLM3,matrix_test[,1])
```

## feature pre-processing for (CA)NN Embedding
```{r}
PreProcess.Continuous <- function(var1, dat1){
   names(dat1)[names(dat1) == var1]  <- "V1"
   dat1$X <- as.numeric(dat1$V1)
   dat1$X <- 2*(dat1$X-min(dat1$X))/(max(dat1$X)-min(dat1$X))-1
   names(dat1)[names(dat1) == "V1"]  <- var1
   names(dat1)[names(dat1) == "X"]  <- paste(var1,"X", sep="")
   dat1
   }

Features.PreProcess <- function(dat1){
   dat1 <- PreProcess.Continuous("Area", dat1)   
   dat1 <- PreProcess.Continuous("VehPower", dat1)   
   dat1$VehAge <- pmin(dat1$VehAge,20)
   dat1 <- PreProcess.Continuous("VehAge", dat1)   
   dat1$DrivAge <- pmin(dat1$DrivAge,90)
   dat1 <- PreProcess.Continuous("DrivAge", dat1)   
   dat1$BonusMalus <- pmin(dat1$BonusMalus,150)
   dat1 <- PreProcess.Continuous("BonusMalus", dat1)   
   dat1$VehBrandX <- as.integer(dat1$VehBrand)-1
   dat1$VehGasX <- as.integer(dat1$VehGas)-1.5
   dat1$Density <- round(log(dat1$Density),2)
   dat1 <- PreProcess.Continuous("Density", dat1)   
   dat1$RegionX <- as.integer(dat1$Region)-1
   dat1
    }
dat2 <- Features.PreProcess(dat)   
names(dat2)
dat2_train<-dat2[index_train,]
dat2_valid<-dat2[index_valid,]
dat2_test<-dat2[index_test,]
dat2_learn<-dat2[index_learn,]
```

## neural network (with embeddings)
### structure
```{r}
names(dat2_learn)
lambda.hom <- sum(dat2_train$ClaimNb)/sum(dat2_train$Exposure);lambda.hom

# definition of feature variables (non-categorical)
features <- c(13:17, 19:20)
(q0 <- length(features))
# training data
Xtrain<- as.matrix(dat2_train[, features])  # design matrix learning sample
Brtrain <- as.matrix(dat2_train$VehBrandX)
Retrain <- as.matrix(dat2_train$RegionX)
Ytrain<- as.matrix(dat2_train$ClaimNb)
Vtrain<-as.matrix(log(dat2_train$Exposure*lambda.hom))
# validation data 
Xvalid<- as.matrix(dat2_valid[, features])  # design matrix learning sample
Brvalid <- as.matrix(dat2_valid$VehBrandX)
Revalid <- as.matrix(dat2_valid$RegionX)
Yvalid<- as.matrix(dat2_valid$ClaimNb)
Vvalid<-as.matrix(log(dat2_valid$Exposure*lambda.hom))
xxvalid<-list(Xvalid,Brvalid,Revalid,Vvalid)
# testing data
Xtest <- as.matrix(dat2_test[, features])    # design matrix test sample
Brtest <- as.matrix(dat2_test$VehBrandX)
Retest <- as.matrix(dat2_test$RegionX)
Ytest <- as.matrix(dat2_test$ClaimNb)
Vtest <- as.matrix(log(dat2_test$Exposure*lambda.hom))

CANN <- 1  # 0=Embedding NN, 1=CANN
if (CANN==1){
     Vtrain <- as.matrix(log(dat1_train$fitGLM1))
     Vvalid<- as.matrix(log(dat1_valid$fitGLM1))
     Vtest <- as.matrix(log(dat1_test$fitGLM1))
            }

# hyperparameters of the neural network architecture
(BrLabel <- length(unique(dat2_train$VehBrandX)))
(ReLabel <- length(unique(dat2_train$RegionX)))
q1 <- 20   
q2 <- 15
q3 <- 10
q4 <- 5
d <- 1        # dimensions embedding layers for categorical features
# define the network architecture
Design   <- layer_input(shape = c(q0),  dtype = 'float32', name = 'Design')
VehBrand <- layer_input(shape = c(1),   dtype = 'int32', name = 'VehBrand')
Region   <- layer_input(shape = c(1),   dtype = 'int32', name = 'Region')
LogVol   <- layer_input(shape = c(1),   dtype = 'float32', name = 'LogVol')

BrandEmb = VehBrand %>% 
      layer_embedding(input_dim = BrLabel, output_dim = d, input_length = 1, name = 'BrandEmb') %>%
      layer_flatten(name='Brand_flat')
# input_dim is the size of vocabulary; input_length is the length of input sequences
       
RegionEmb = Region %>% 
      layer_embedding(input_dim = ReLabel, output_dim = d, input_length = 1, name = 'RegionEmb') %>%
      layer_flatten(name='Region_flat')

Network = list(Design, BrandEmb, RegionEmb) %>% layer_concatenate(name='concate') %>% 
          layer_dense(units=q1, activation='tanh', name='hidden1') %>%
          layer_batch_normalization()%>%
          layer_dropout(rate =0.05) %>%
          layer_dense(units=q2, activation='tanh', name='hidden2') %>%
          layer_batch_normalization()%>%
          layer_dropout(rate =0.05) %>%
          layer_dense(units=q3, activation='tanh', name='hidden3') %>%
          layer_batch_normalization()%>%
          layer_dropout(rate =0.05) %>%
          layer_dense(units=q4, activation='tanh', name='hidden4') %>%
          layer_batch_normalization()%>%
          layer_dropout(rate =0.05) %>%   
          layer_dense(units=1, activation='linear', name='Network', weights = list(array(0, dim=c(q4,1)), array(0, dim=c(1))))

Response = list(Network, LogVol) %>% layer_add(name='Add') %>% 
           layer_dense(units=1, activation=k_exp, name = 'Response', trainable=FALSE,
                        weights=list(array(1, dim=c(1,1)), array(0, dim=c(1))))

model <- keras_model(inputs = c(Design, VehBrand, Region, LogVol), outputs = c(Response))
model %>% compile(optimizer = optimizer_nadam(), loss = 'poisson')

compile(model,optimizer = optimizer_nadam(), loss = 'poisson')
summary(model)
```

### calibration
```{r}
# fitting the neural network
early_stop <- callback_early_stopping(monitor = "val_loss", patience =20)
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 50 == 0) cat("\n")
    cat(".")
  }
)  
{t1 <- proc.time();
fit <- model %>% fit(list(Xtrain, Brtrain, Retrain, Vtrain), Ytrain,
                     epochs=500, batch_size=10000, verbose=0,
                     validation_data=list(xxvalid,Yvalid),
                     callbacks=list(print_dot_callback,early_stop));
(proc.time()-t1)}

plot(fit,smooth = F)
fit

# calculating the predictions
dat2_test$fitNN <- as.vector(model %>% predict(list(Xtest, Brtest, Retest, Vtest)))
keras_poisson_dev(dat2_test$fitNN, dat2_test$ClaimNb)
Poisson.Deviance(dat2_test$fitNN, dat2_test$ClaimNb)
```

# CANN improved Model GAM1 corresponding to formula (3.11)
##  GAM marginals improvement (VehAge and BonusMalus)
```{r}
{t1 <- proc.time()
dat.GAM <- ddply(dat1_learn, .(VehAge, BonusMalus), summarise, fitGLM1=sum(fitGLM1), ClaimNb=sum(ClaimNb))
set.seed(1)
d.gam <- gam(ClaimNb ~ s(VehAge, bs="cr")+s(BonusMalus, bs="cr")+ offset(log(fitGLM1)), data=dat.GAM, method="GCV.Cp", family=poisson)
(proc.time()-t1)}
summary(d.gam)
dat2_train$fitGAM1 <- predict(d.gam, newdata=dat1_train,type="response")
dat2_valid$fitGAM1 <- predict(d.gam, newdata=dat1_valid,type="response")
dat2_test$fitGAM1 <- predict(d.gam, newdata=dat1_test,type="response")
keras_poisson_dev(dat2_test$fitGAM1, dat2_test$ClaimNb)
```

##  neural network definitions for model (3.11)
### structure
```{r}
train.x <- list(as.matrix(dat2_train[,c("VehPowerX", "VehAgeX", "VehGasX")]),
                as.matrix(dat2_train[,"VehBrandX"]),
                as.matrix(dat2_train[,c("DrivAgeX", "BonusMalus")]),
                as.matrix(log(dat2_train$fitGAM1)) )
valid.x <- list(as.matrix(dat2_valid[,c("VehPowerX", "VehAgeX", "VehGasX")]),
                as.matrix(dat2_valid[,"VehBrandX"]),
                as.matrix(dat2_valid[,c("DrivAgeX", "BonusMalus")]),
                as.matrix(log(dat2_valid$fitGAM1)) )
test.x <- list(as.matrix(dat2_test[,c("VehPowerX", "VehAgeX", "VehGasX")]),
                as.matrix(dat2_test[,"VehBrandX"]),
                as.matrix(dat2_test[,c("DrivAgeX", "BonusMalus")]),
                as.matrix(log(dat2_test$fitGAM1)) )

neurons <- c(15,10,5)
No.Labels <- length(unique(dat2_train$VehBrandX))

model.2IA <- function(No.Labels){
   Cont1        <- layer_input(shape = c(3), dtype = 'float32', name='Cont1')
   Cat1         <- layer_input(shape = c(1), dtype = 'int32',   name='Cat1')
   Cont2        <- layer_input(shape = c(2), dtype = 'float32', name='Cont2')
   LogExposure  <- layer_input(shape = c(1), dtype = 'float32', name = 'LogExposure')     
   x.input <- c(Cont1, Cat1, Cont2, LogExposure)
   #
   Cat1_embed = Cat1 %>%  
            layer_embedding(input_dim = No.Labels, output_dim = 1, trainable=TRUE, 
                    input_length = 1, name = 'Cat1_embed') %>%
                    layer_flatten(name='Cat1_flat')
   #
   NNetwork1 = list(Cont1, Cat1_embed) %>% layer_concatenate(name='cont') %>%
            layer_dense(units=neurons[1], activation='tanh', name='hidden1') %>%
            layer_dense(units=neurons[2], activation='tanh', name='hidden2') %>%
            layer_dense(units=neurons[3], activation='tanh', name='hidden3') %>%
            layer_dense(units=1, activation='linear', name='NNetwork1', 
                    weights=list(array(0, dim=c(neurons[3],1)), array(0, dim=c(1))))
   #
   NNetwork2 = Cont2 %>%
            layer_dense(units=neurons[1], activation='tanh', name='hidden4') %>%
            layer_dense(units=neurons[2], activation='tanh', name='hidden5') %>%
            layer_dense(units=neurons[3], activation='tanh', name='hidden6') %>%
            layer_dense(units=1, activation='linear', name='NNetwork2', 
                    weights=list(array(0, dim=c(neurons[3],1)), array(0, dim=c(1))))

   #
   NNoutput = list(NNetwork1, NNetwork2, LogExposure) %>% layer_add(name='Add') %>%
                 layer_dense(units=1, activation=k_exp, name = 'NNoutput', trainable=FALSE,
                       weights=list(array(c(1), dim=c(1,1)), array(0, dim=c(1))))

   model <- keras_model(inputs = x.input, outputs = c(NNoutput))
   model %>% compile(optimizer = optimizer_nadam(), loss = 'poisson')        
   model
   }

model <- model.2IA(No.Labels)
summary(model)
```

### calibration
```{r}
early_stop <- callback_early_stopping(monitor = "val_loss", patience =20)
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 50 == 0) cat("\n")
    cat(".")
  }
)  
# may take a couple of minutes if epochs is more than 100
{t1 <- proc.time()
     fit <- model %>% fit(train.x, as.matrix(dat2_train$ClaimNb), epochs=500, batch_size=10000, verbose=0,validation_data=list(valid.x,dat2_valid$ClaimNb),,callback=list(early_stop,print_dot_callback))
(proc.time()-t1)}
plot(fit,smooth=F)

dat2_test$fitGAMPlus <- as.vector(model %>% predict(test.x))
Poisson.Deviance(dat2_test$fitGAMPlus, dat2_test$ClaimNb)
keras_poisson_dev(dat2_test$fitGAMPlus, dat2_test$ClaimNb)
```

# summarize
```{r}
dev_sum<-fread("dev_sum.csv")[,-1]
AD<-data.frame(model="Neural network",test_error=0,test_error_keras=0)
dev_sum<-rbind(dev_sum,AD)
dev_sum$test_error[8]<-round(Poisson.Deviance(dat2_test$fitNN, dat2_test$ClaimNb),4)
dev_sum$test_error_keras[8]<-round(keras_poisson_dev(dat2_test$fitNN, dat2_test$ClaimNb),4)
write.csv(dev_sum,"dev_sum.csv")
```
