{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "/Users/huihuajiang/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "1. [环境设置](#started)\n",
    "2. [导入数据](#import)\n",
    "3. [数据预处理](#preprocess)\n",
    "4. [描述性分析](#describe)\n",
    "5. [Machine learning](#ML)  \n",
    "    5.1. [Adaptive boosting (ADA)](#ADA)  \n",
    "    .......5.1.1. [Bag-of-words](#ADA_BOW)  \n",
    "    .......5.1.2. [Bag-of-POS](#ADA_BOP)  \n",
    "    .......5.1.3. [Embeddings](#ADA_E)  \n",
    "    5.2. [Random forests (RF)](#RF)  \n",
    "    .......5.2.1. [Bag-of-words](#RF_BOW)  \n",
    "    .......5.2.2. [Bag-of-POS](#RF_BOP)  \n",
    "    .......5.2.3. [Embeddings](#RF_E)  \n",
    "    5.3. [Extreme gradient boosting (XGB)](#XGB)  \n",
    "    .......5.3.1. [Bag-of-words](#XGB_BOW)  \n",
    "    .......5.3.2. [Bag-of-POS](#XGB_BOP)  \n",
    "    .......5.3.3. [Embeddings](#XGB_E)\n",
    "6. [Deep learning](#DL)  \n",
    "    6.1. [Train test split](#trainsplit)  \n",
    "    6.2. [Define the model](#modeldef)  \n",
    "    .......6.2.1. [Shallow LSTM](#LSTM1)  \n",
    "    .......6.2.2. [Shallow GRU](#GRU1)  \n",
    "    .......6.2.3. [Deep LSTM](#LSTM2)    \n",
    "    6.3. [Train the model](#trainmodel)  \n",
    "    6.4. [Evaluate the model on test data](#evalmodel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 环境设置<a name=\"started\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量初始化和显示设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 导入数据<a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们从原始数据（50000条评论数据，平均分为train/pos、train/neg、test/pos和test/neg四部分）中随机抽取了1%，即500条数据进行测试。\n",
    "该部分将分布在四个文件夹中（保持原始文件结构不变）500条数据依次全部读取到DataFrame类型的df变量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def listdirInMac(path):\n",
    "    os_list = os.listdir(path)\n",
    "    for item in os_list:\n",
    "        if item.startswith('.') and os.path.isfile(os.path.join(path, item)):\n",
    "            os_list.remove(item)\n",
    "    return os_list\n",
    "#因为os.listdir会默认把隐藏文件读进去 所以对该函数进行重新定义 不然会报错\n",
    "# we use the import function, as in Chapter 8 of Raschka's book (see the tutorial)\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "basepath = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/' # insert basepath, where original data are stored\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(500)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in listdirInMac(path):\n",
    "            #把原来的os.listdir改为listdirInMac\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  I rented this obscure aussie relic a few years...          1\n"
     ]
    }
   ],
   "source": [
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 数据预处理<a name=\"preprocess\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该部分包括：<br><br/>\n",
    "3.1 去重<br><br/>\n",
    "3.2 随机排列<br><br/>\n",
    "3.3 符号和大小写处理<br><br/>\n",
    "3.4 生成针对模型的数据<br><br/>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;3.4.1 词性标注<br><br/>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;3.4.2 词嵌入<br><br/>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;3.4.3 用Keras进行针对RNN模型的数据预处理<br><br/>\n",
    "分别生成data_POStagged.csv、data_dedandprep.csv和movie_data_processed.csv三个数据文件，其中前两个文件用于ml模型，带三个文件用于RNN模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 去重<a name=\"duplicated\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "duplicates = df[df.duplicated()]  #equivalent to keep = first. Duplicated rows, except the first entry, are marked as 'True'\n",
    "print(len(duplicates))\n",
    "#抽取的部分不存在重复数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates: \n",
    "df = df.drop_duplicates()\n",
    "df.shape\n",
    "#总共抽取了train和test中pos和neg各125个数据 总计500个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [review, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check\n",
    "df[df.duplicated(subset='review')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 随机化<a name=\"Shuffle\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为读取的时候是按照train/pos、train/neg、test/pos和test/neg顺序读取的，有模型训练时，我们采用前80%作为训练数据，后20%作为预测数据，如果不做随机处理，预测数据的实际类型都是一致的。（实际上在ml模型拟合之前都重新对数据进行了随机化，所以这一步是针对DL模型的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shuffle the data to ensure randomness in the training input\n",
    "# 随机排列\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df = df.reset_index(drop=True) # reset the index after the permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 符号和大小写处理<a name=\"mark\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This movie is the only movie to feature a scene in which Michael Jackson wields a Tommy Gun. Plain and simple.<br /><br />This movie rocks because it is freaking' hilarious! It may be creepy to see Jacko w/ little kids, but this movie also stars.......................................... wait for it,.....................<br /><br />JOE PESCI!!!!!!!!!!!!!!!!!!!!!<br /><br />Think about it, Joe Pesci and Jacko with Tommy guns, throwing coins into jukeboxes from 20 feet away? Whats not to like? As stated before, THIS MOVIE ROCKS!!!!!!!!!!!! !!!!!!! !!!!!!!!!!! !!!!!!!! !!!! !!!!!!! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! !!!!!!!!!!!!!! !!!!!!!!!! !!!!!!! ! !!!! !!!!!!!!!!!!!!!!!!!!!!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of 'raw' review: we have all sort of HTML markup\n",
    "df.loc[2, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing by Raschka, Chpater 8 (see tutorial)\n",
    "# we remove all markups, substitute non-alphanumeric characters (including \n",
    "# underscore) with whitespaces, and remove the nose from emoticons\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    #sub() 替换函数\n",
    "    #用’‘替换 <[^>]*> 这些字符\n",
    "    #即将 <[^>]*> 这些字符都删去\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # ?: 匹配但不获取结果 要和其他的模式一起使用\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie is the only movie to feature a scene in which michael jackson wields a tommy gun plain and simple this movie rocks because it is freaking hilarious it may be creepy to see jacko w little kids but this movie also stars wait for it joe pesci think about it joe pesci and jacko with tommy guns throwing coins into jukeboxes from 20 feet away whats not to like as stated before this movie rocks '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data as csv \n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_preprocessed.csv'  # insert path\n",
    "df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 生成针对模型的数据<a name=\"preprocess2model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，这一步生成的数据并不是最终进行模型拟合的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 词性标准<a name=\"POS-tagging\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该步骤的目的是为每一条文本生成'text_pos'属性，该属性是一个由“-”连接的字符串，它按顺序连接了经过预处理的文本中每个单词的词性数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we apply POS-tagging on (deduplicated and) pre-processed data - let us import them\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_preprocessed.csv' # insert path\n",
    "# 注意路径最好不要带中文，有可能会报错\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the NLTK resources\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "#NLTK包的pos_tag方法（part-of-speech tagging ）来对单词的词性进行标记\n",
    "#标记后的结果是二元数组格式\n",
    "\n",
    "# introduction of POS tagger per NLTK token\n",
    "def pos_tags(text):\n",
    "    text_processed = word_tokenize(text)\n",
    "    return \"-\".join( tag for (word, tag) in nltk.pos_tag(text_processed))\n",
    "\n",
    "# applying POS tagger to data \n",
    "############################################\n",
    "df['text_pos']=df.apply(lambda x: pos_tags(x['review']), axis=1)\n",
    "#text_pos是一个由“-”连接的字符串，它按顺序连接了经过预处理的文本中每个单词的词性数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果出现LookupError:   \n",
    "    Resource punkt not found.\n",
    "    Please use the NLTK Downloader to obtain the resource:\n",
    "    >>> import nltk\n",
    "    >>> nltk.download('punkt')\n",
    "要用以上命令先下载'punkt'\n",
    "\n",
    "如果nltk.download('punkt')下载失败：\n",
    "解决方法1：修改host文件（详情请参考https://blog.csdn.net/xiangduixuexi/article/details/108601873）\n",
    "解决方法2：把'punkt.zip'拷贝至'/Users/huihuajiang/nltk_data/tokenizers/'(如果没有tokenizers文件则新建一个)\n",
    "         用以下命令可以查看你的 nltk_data 文件夹路径：\n",
    "             import nltk\n",
    "             print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/huihuajiang/nltk_data', '/Users/huihuajiang/anaconda3/nltk_data', '/Users/huihuajiang/anaconda3/share/nltk_data', '/Users/huihuajiang/anaconda3/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT-NN-VBZ-DT-JJ-NN-TO-VB-DT-NN-IN-WDT-NN-NN-VBZ-DT-JJ-NN-NN-CC-NN-DT-NN-VBZ-IN-PRP-VBZ-VBG-JJ-PRP-MD-VB-VBN-TO-VB-JJ-JJ-JJ-NNS-CC-DT-NN-RB-VBZ-NN-IN-PRP-VBZ-JJ-VBP-IN-PRP-VBZ-NN-CC-NN-IN-JJ-NNS-VBG-NNS-IN-NNS-IN-CD-NNS-RB-VBZ-RB-TO-VB-IN-VBN-IN-DT-NN-NNS\n"
     ]
    }
   ],
   "source": [
    "print(df['text_pos'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save POS-tagged data as csv \n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/new/data_POStagged.csv' # insert path \n",
    "df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 词嵌入<a name=\"Word-embeddings\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we apply embeddings on de-duplicated and pre-processed data - let us import them\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_preprocessed.csv' # insert path\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre-trained word embedding model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "# load the model first if necessary: python -m spacy download en_core_web_md\n",
    "# 这个模型要下载很久\n",
    "# https://spacy.io/models/en#en_core_web_md\n",
    "# 根据下一个cell的提示，我们换用en_core_web_sm模型\n",
    "#OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory. \n",
    "#解决方法：https://www.freesion.com/article/73801416523/ 可以用：python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we stack (like a numpy vertical stack) the 300 variables obtained from averaging the embedding of each df.review entry\n",
    "# WARNING: this is computationally expensive. Alternatively try with the smaller model en_core_web_sm\n",
    "import numpy as np\n",
    "emb = np.vstack(df.review.apply(lambda x: nlp(x).vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 96)\n",
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15'\n",
      " '16' '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29'\n",
      " '30' '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43'\n",
      " '44' '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57'\n",
      " '58' '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71'\n",
      " '72' '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85'\n",
      " '86' '87' '88' '89' '90' '91' '92' '93' '94' '95']\n"
     ]
    }
   ],
   "source": [
    "# embeddings into a dataframe\n",
    "emb = pd.DataFrame(emb, columns = np.array([str(x) for x in range(0, 96)]) )\n",
    "print(emb.shape)\n",
    "print(emb.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join embeddings with dataframe\n",
    "df_embed = pd.concat([df, emb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 98)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the resulting dataframe\n",
    "df_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word embedding data as csv \n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/new/data_dedandprep.csv' # insert path\n",
    "df_embed.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 用Keras进行针对RNN模型的数据预处理<a name=\"preprocess2rnn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN方法在文本所需要的预处理上具备优势"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_preprocessed.csv' # insert path\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Minimal preprocessing and generating word counts:\n",
    "#  - we surround all punctuation by whitespaces\n",
    "#  - all text is converted to lowercase\n",
    "#  - word counts are generated by splitting the text on whitespaces\n",
    "import pyprind\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' '  \n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split()) # splitting on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 12173\n"
     ]
    }
   ],
   "source": [
    "# get the size of the vocabulary\n",
    "print('Number of unique words:', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that appear more than once: 5930\n",
      "Number of words that appear more than 30 times: 409\n"
     ]
    }
   ],
   "source": [
    "# investigate how many words appear only rarely in the reviews\n",
    "print('Number of words that appear more than once:', \n",
    "      len([k for k, v in counts.items() if v > 1]))\n",
    "print('Number of words that appear more than 30 times:', \n",
    "      len([k for k, v in counts.items() if v > 30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hence we use only the 400 most common in our vocabulary \n",
    "# this will make training more efficient without loosing too much information\n",
    "vocab_size = 400\n",
    "\n",
    "# create a dictionary with word:integer pairs for all unique words\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "word_counts = word_counts[0:vocab_size]\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map reviews to ints\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Mapping words to integers\n",
    "# create a list with all reviews in integer coded form\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] \n",
    "                           for word in review.split() \n",
    "                           if word in word_to_int.keys()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length of mapped reviews: 121.0\n"
     ]
    }
   ],
   "source": [
    "# get the median length of the mapped review sequences to inform the choice of sequence_length\n",
    "print('Median length of mapped reviews:',\n",
    "      np.median([len(review) for review in mapped_reviews]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding: set sequence length and ensure all mapped reviews are coerced to required length\n",
    "# if sequence length < T: left-pad with zeros\n",
    "# if sequence length > T: use the last T elements\n",
    "sequence_length = 200  # (Known as T in our RNN formulae)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>221</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>375</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>249</td>\n",
       "      <td>47</td>\n",
       "      <td>143</td>\n",
       "      <td>133</td>\n",
       "      <td>5</td>\n",
       "      <td>343</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>35</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>171</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>367</td>\n",
       "      <td>34</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>89</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>197</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>166</td>\n",
       "      <td>28</td>\n",
       "      <td>329</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment   0    1   2    3    4   5   6    7    8 ...   190  191  192  \\\n",
       "0          1   0    0   0    0    0   0   0    0    0 ...     9  221    9   \n",
       "1          1  11  255   1   22   10   1   6  375    8 ...     2  249   47   \n",
       "2          1   0    0   0    0    0   0   0    0    0 ...    85   35  200   \n",
       "3          0   1  367  34  151    2  89  32   11  112 ...    19    2   42   \n",
       "4          0   8    5  26    8  197  15  28   16    3 ...     1  144    4   \n",
       "\n",
       "   193  194  195  196  197  198  199  \n",
       "0   14   32    1  258    7   14    2  \n",
       "1  143  133    5  343   46   10   19  \n",
       "2   23    5   38   16  171   10   15  \n",
       "3   52    4  112    5   22    1    6  \n",
       "4    1    4  166   28  329   10   29  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with processed data\n",
    "df_processed = pd.concat([df['sentiment'],pd.DataFrame(sequences)], axis=1)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>221</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>258</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>375</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>249</td>\n",
       "      <td>47</td>\n",
       "      <td>143</td>\n",
       "      <td>133</td>\n",
       "      <td>5</td>\n",
       "      <td>343</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>85</td>\n",
       "      <td>35</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>171</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>367</td>\n",
       "      <td>34</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>89</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>197</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>166</td>\n",
       "      <td>28</td>\n",
       "      <td>329</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment   0    1   2    3    4   5   6    7    8 ...   190  191  192  \\\n",
       "0          1   0    0   0    0    0   0   0    0    0 ...     9  221    9   \n",
       "1          1  11  255   1   22   10   1   6  375    8 ...     2  249   47   \n",
       "2          1   0    0   0    0    0   0   0    0    0 ...    85   35  200   \n",
       "3          0   1  367  34  151    2  89  32   11  112 ...    19    2   42   \n",
       "4          0   8    5  26    8  197  15  28   16    3 ...     1  144    4   \n",
       "\n",
       "   193  194  195  196  197  198  199  \n",
       "0   14   32    1  258    7   14    2  \n",
       "1  143  133    5  343   46   10   19  \n",
       "2   23    5   38   16  171   10   15  \n",
       "3   52    4  112    5   22    1    6  \n",
       "4    1    4  166   28  329   10   29  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#这个类用于对文本语料进行向量化\n",
    "#在机器学习中，一篇文本向量的给维度代表词汇表中的一个词，一篇文本在各维度上的值可以是布尔类型，TF值，TF-IDF值。\n",
    "#在神经网络中，文本向量化多了一个方法，就是将文本中的词语替换为该词在词汇表中的索引，加入这种方法的原因是可以方便地使用词嵌入技术。\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 400\n",
    "\n",
    "# map words to integers including minimal preprocessing\n",
    "tokenizer = Tokenizer(num_words=vocab_size, #词汇表中可使用的最大词语数量。词汇表按照词语频次降序存储各词语，可使用的词语为前num_words。\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', # filters out all punctuation other than '\n",
    "                            #在读入文本时，文本中出现的属于该字符串的任意字符都将被替换为空格。默认情况下该字符串为所有英文标点符号（除了'）\n",
    "                      lower=True, # convert to lowercase\n",
    "                      split=' ') # split on whitespaces\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "list_tokenized = tokenizer.texts_to_sequences(df['review'])\n",
    "\n",
    "# Padding to sequence_length\n",
    "sequence_length = 200\n",
    "sequences = pad_sequences(list_tokenized, maxlen=sequence_length)\n",
    "\n",
    "# create df with processed data\n",
    "df_processed = pd.concat([df['sentiment'], pd.DataFrame(sequences)], axis=1)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv \n",
    "path = \"/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/new/movie_data_processed.csv\" # TODO: update to your path\n",
    "df_processed.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以上步骤有问题可以直接读取处理好的数据，不影响后续步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 描述性分析<a name=\"describe\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "import pandas as pd\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_dedandprep.csv' \n",
    "# insert path for deduplicated and preprocessed data\n",
    "# 如果前面的步骤没有问题 你也可以把路径改成刚在生成的相应csv文件的路径\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 98)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imported data structure\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21',\n",
       "       '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33',\n",
       "       '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',\n",
       "       '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57',\n",
       "       '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69',\n",
       "       '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81',\n",
       "       '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93',\n",
       "       '94', '95'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns in data\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i ve only seen this film once when it was show...</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.214782</td>\n",
       "      <td>0.299167</td>\n",
       "      <td>0.530876</td>\n",
       "      <td>-0.753905</td>\n",
       "      <td>0.718617</td>\n",
       "      <td>-0.613121</td>\n",
       "      <td>0.27328</td>\n",
       "      <td>1.213849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653203</td>\n",
       "      <td>0.608409</td>\n",
       "      <td>-0.174202</td>\n",
       "      <td>0.319868</td>\n",
       "      <td>-0.037081</td>\n",
       "      <td>-0.167153</td>\n",
       "      <td>-0.278357</td>\n",
       "      <td>0.061519</td>\n",
       "      <td>-0.08818</td>\n",
       "      <td>-0.56683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment         0  \\\n",
       "0  i ve only seen this film once when it was show...          1 -0.214782   \n",
       "\n",
       "          1         2         3         4         5        6         7  \\\n",
       "0  0.299167  0.530876 -0.753905  0.718617 -0.613121  0.27328  1.213849   \n",
       "\n",
       "    ...           86        87        88        89        90        91  \\\n",
       "0   ...     0.653203  0.608409 -0.174202  0.319868 -0.037081 -0.167153   \n",
       "\n",
       "         92        93       94       95  \n",
       "0 -0.278357  0.061519 -0.08818 -0.56683  \n",
       "\n",
       "[1 rows x 98 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imported data: first entry\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    250\n",
       "0    250\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counts of rviews per sentiment value\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i ve only seen this film once when it was show...</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disney the film name that once stood for all t...</td>\n",
       "      <td>1</td>\n",
       "      <td>922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this movie is the only movie to feature a scen...</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the eternal jew der ewige jude does not have w...</td>\n",
       "      <td>0</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this movie is just plain terrible poor john sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  word_count\n",
       "0  i ve only seen this film once when it was show...          1         144\n",
       "1  disney the film name that once stood for all t...          1         922\n",
       "2  this movie is the only movie to feature a scen...          1          77\n",
       "3  the eternal jew der ewige jude does not have w...          0         562\n",
       "4  this movie is just plain terrible poor john sa...          0         301"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show distribution of review lenghts \n",
    "# we strip leading and trailing whitespaces and tokenize by whitespace\n",
    "df['word_count'] = df['review'].apply(lambda x: len(x.strip().split(\" \")))\n",
    "df[['review','sentiment', 'word_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     500.000000\n",
      "mean      228.182000\n",
      "std       172.093432\n",
      "min         6.000000\n",
      "25%       126.000000\n",
      "50%       170.500000\n",
      "75%       274.000000\n",
      "max      1032.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summary statistics of word counts\n",
    "print(df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show histograms of word counts divided per sentiment\n",
    "from matplotlib import pyplot\n",
    "\n",
    "x = df[df['sentiment']==0].word_count\n",
    "y = df[df['sentiment']==1].word_count\n",
    "\n",
    "pyplot.hist(x, bins=50, alpha=0.5, label='negative sentiment reviews')\n",
    "pyplot.hist(y, bins=50, alpha=0.5, label='positive sentiment reviews')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     250.000000\n",
      "mean      225.304000\n",
      "std       167.889365\n",
      "min         6.000000\n",
      "25%       128.000000\n",
      "50%       173.000000\n",
      "75%       273.250000\n",
      "max      1032.000000\n",
      "Name: word_count, dtype: float64\n",
      "count    250.000000\n",
      "mean     231.060000\n",
      "std      176.487301\n",
      "min       26.000000\n",
      "25%      125.000000\n",
      "50%      169.000000\n",
      "75%      274.000000\n",
      "max      995.000000\n",
      "Name: word_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# summary of distributions of word counts\n",
    "print(x.describe())\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>primary plot primary direction poor interpreta...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.57987</td>\n",
       "      <td>1.775504</td>\n",
       "      <td>0.308832</td>\n",
       "      <td>-1.488625</td>\n",
       "      <td>1.640675</td>\n",
       "      <td>-0.014321</td>\n",
       "      <td>-0.066279</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.838312</td>\n",
       "      <td>0.649058</td>\n",
       "      <td>-0.420057</td>\n",
       "      <td>0.550122</td>\n",
       "      <td>0.333427</td>\n",
       "      <td>-1.017469</td>\n",
       "      <td>0.123175</td>\n",
       "      <td>-1.249303</td>\n",
       "      <td>0.806109</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment        0  \\\n",
       "238  primary plot primary direction poor interpreta...          0  1.57987   \n",
       "\n",
       "            1         2         3         4         5         6         7  \\\n",
       "238  1.775504  0.308832 -1.488625  1.640675 -0.014321 -0.066279  1.008091   \n",
       "\n",
       "        ...            87        88        89        90        91        92  \\\n",
       "238     ...     -0.838312  0.649058 -0.420057  0.550122  0.333427 -1.017469   \n",
       "\n",
       "           93        94        95  word_count  \n",
       "238  0.123175 -1.249303  0.806109           6  \n",
       "\n",
       "[1 rows x 99 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. word_counts=6 or 1550 or 2498 )\n",
    "df[df['word_count']==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i ve only seen this film once when it was show...</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>4.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disney the film name that once stood for all t...</td>\n",
       "      <td>922</td>\n",
       "      <td>1</td>\n",
       "      <td>4.364425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this movie is the only movie to feature a scen...</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>4.220779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the eternal jew der ewige jude does not have w...</td>\n",
       "      <td>562</td>\n",
       "      <td>0</td>\n",
       "      <td>4.765125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this movie is just plain terrible poor john sa...</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>3.943522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  word_count  sentiment  \\\n",
       "0  i ve only seen this film once when it was show...         144          1   \n",
       "1  disney the film name that once stood for all t...         922          1   \n",
       "2  this movie is the only movie to feature a scen...          77          1   \n",
       "3  the eternal jew der ewige jude does not have w...         562          0   \n",
       "4  this movie is just plain terrible poor john sa...         301          0   \n",
       "\n",
       "   avg_word  \n",
       "0  4.125000  \n",
       "1  4.364425  \n",
       "2  4.220779  \n",
       "3  4.765125  \n",
       "4  3.943522  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average word length (again, we tokenize by whitespaces)\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df['avg_word'] = df['review'].apply(lambda x: avg_word(x.strip()))\n",
    "df[['review','word_count', 'sentiment', 'avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    250.000000\n",
      "mean       4.292849\n",
      "std        0.365534\n",
      "min        3.500000\n",
      "25%        4.079142\n",
      "50%        4.295587\n",
      "75%        4.509106\n",
      "max        7.500000\n",
      "Name: avg_word, dtype: float64\n",
      "\n",
      "count    250.000000\n",
      "mean       4.316774\n",
      "std        0.340776\n",
      "min        3.385093\n",
      "25%        4.099108\n",
      "50%        4.295241\n",
      "75%        4.542389\n",
      "max        5.512195\n",
      "Name: avg_word, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distributions of word lengths conditional per sentiment\n",
    "x = df[df['sentiment']==0].avg_word\n",
    "y = df[df['sentiment']==1].avg_word\n",
    "print(x.describe())\n",
    "print()\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>primary plot primary direction poor interpreta...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.57987</td>\n",
       "      <td>1.775504</td>\n",
       "      <td>0.308832</td>\n",
       "      <td>-1.488625</td>\n",
       "      <td>1.640675</td>\n",
       "      <td>-0.014321</td>\n",
       "      <td>-0.066279</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649058</td>\n",
       "      <td>-0.420057</td>\n",
       "      <td>0.550122</td>\n",
       "      <td>0.333427</td>\n",
       "      <td>-1.017469</td>\n",
       "      <td>0.123175</td>\n",
       "      <td>-1.249303</td>\n",
       "      <td>0.806109</td>\n",
       "      <td>6</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment        0  \\\n",
       "238  primary plot primary direction poor interpreta...          0  1.57987   \n",
       "\n",
       "            1         2         3         4         5         6         7  \\\n",
       "238  1.775504  0.308832 -1.488625  1.640675 -0.014321 -0.066279  1.008091   \n",
       "\n",
       "       ...           88        89        90        91        92        93  \\\n",
       "238    ...     0.649058 -0.420057  0.550122  0.333427 -1.017469  0.123175   \n",
       "\n",
       "           94        95  word_count  avg_word  \n",
       "238 -1.249303  0.806109           6       7.5  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. avg_word>=7)\n",
    "df[df['avg_word']>=7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i ve only seen this film once when it was show...</td>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disney the film name that once stood for all t...</td>\n",
       "      <td>922</td>\n",
       "      <td>1</td>\n",
       "      <td>4.364425</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this movie is the only movie to feature a scen...</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>4.220779</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the eternal jew der ewige jude does not have w...</td>\n",
       "      <td>562</td>\n",
       "      <td>0</td>\n",
       "      <td>4.765125</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this movie is just plain terrible poor john sa...</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>3.943522</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  word_count  sentiment  \\\n",
       "0  i ve only seen this film once when it was show...         144          1   \n",
       "1  disney the film name that once stood for all t...         922          1   \n",
       "2  this movie is the only movie to feature a scen...          77          1   \n",
       "3  the eternal jew der ewige jude does not have w...         562          0   \n",
       "4  this movie is just plain terrible poor john sa...         301          0   \n",
       "\n",
       "   avg_word  stopwords  \n",
       "0  4.125000         79  \n",
       "1  4.364425        461  \n",
       "2  4.220779         32  \n",
       "3  4.765125        253  \n",
       "4  3.943522        161  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop words statistics - stopword from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['stopwords'] = df['review'].apply(lambda x: len([x for x in x.strip().split() if x in stop]))\n",
    "df[['review','word_count', 'sentiment', 'avg_word', 'stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    250.00000\n",
      "mean     110.61600\n",
      "std       82.60588\n",
      "min        0.00000\n",
      "25%       62.00000\n",
      "50%       85.00000\n",
      "75%      136.00000\n",
      "max      523.00000\n",
      "Name: stopwords, dtype: float64\n",
      "\n",
      "count    250.000000\n",
      "mean     113.608000\n",
      "std       86.026978\n",
      "min        8.000000\n",
      "25%       62.000000\n",
      "50%       87.000000\n",
      "75%      130.750000\n",
      "max      491.000000\n",
      "Name: stopwords, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# distributions of stop words conditional per sentiment\n",
    "x = df[df['sentiment']==0].stopwords\n",
    "y = df[df['sentiment']==1].stopwords\n",
    "print(x.describe())\n",
    "print()\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>primary plot primary direction poor interpreta...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.57987</td>\n",
       "      <td>1.775504</td>\n",
       "      <td>0.308832</td>\n",
       "      <td>-1.488625</td>\n",
       "      <td>1.640675</td>\n",
       "      <td>-0.014321</td>\n",
       "      <td>-0.066279</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420057</td>\n",
       "      <td>0.550122</td>\n",
       "      <td>0.333427</td>\n",
       "      <td>-1.017469</td>\n",
       "      <td>0.123175</td>\n",
       "      <td>-1.249303</td>\n",
       "      <td>0.806109</td>\n",
       "      <td>6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  sentiment        0  \\\n",
       "238  primary plot primary direction poor interpreta...          0  1.57987   \n",
       "\n",
       "            1         2         3         4         5         6         7  \\\n",
       "238  1.775504  0.308832 -1.488625  1.640675 -0.014321 -0.066279  1.008091   \n",
       "\n",
       "       ...            89        90        91        92        93        94  \\\n",
       "238    ...     -0.420057  0.550122  0.333427 -1.017469  0.123175 -1.249303   \n",
       "\n",
       "           95  word_count  avg_word  stopwords  \n",
       "238  0.806109           6       7.5          0  \n",
       "\n",
       "[1 rows x 101 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some checks (e.g. stopwords==0)\n",
    "df[df['stopwords']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine Learning<a name=\"ML\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Adaptive boosting (ADA)<a name=\"ADA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1. Bag-of-words<a name=\"ADA_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400,)\n",
      "X_test shape check:  (100,)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#调参用的\n",
    "#https://blog.csdn.net/weixin_41988628/article/details/83098130\n",
    "#GridSearchCV 网格搜索\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import \n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_dedandprep.csv'  # insert path to deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)     \n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(400).review\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.tail(100).review\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "#定义tfidf函数，目的是将文本转化为tfidf向量，在该模型中是pipeline的第一步\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],           # choose (1, 2) to compute 2-grams\n",
    "              'vect__stop_words': [stopwords, None],    #表示使用停用词和不使用停用词两种情况\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],     #max_df用于删除过于频繁出现的术语,也称为“语料库特定的停用词”.例如：\n",
    "                                                        #max_df = 0.50表示“忽略出现在50％以上文档中的术语”\n",
    "              'vect__max_features': [None, 400],    #1000改为400                                        \n",
    "              'clf__n_estimators': [10, 20, 30, 40], #[100, 200, 300, 400]改为[10, 20, 30, 40]\n",
    "              #该参数表示基分类器提升（循环）次数，默认是50次，这个值过大，模型容易过拟合；值过小，模型容易欠拟合\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1] #去掉1.0的值 减少选择数\n",
    "              }\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', AdaBoostClassifier(base_estimator=tree))]\n",
    "                    )\n",
    "#pipeline管道机制使用方法：\n",
    "#流水线的输入为一连串的数据挖掘步骤，其中最后一步必须是估计器（Estimator），可理解成分类器\n",
    "#前几步是转换器（Transformer）。输入的数据集经过转换器的处理后，输出的结果作为下一步的输入。\n",
    "#最后，用位于流水线最后一步的估计器对数据进行分类。\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "#交叉验证参数\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1) \n",
    "#第一个参数是estimator，即所使用的分类器\n",
    "#第二个参数是param_grid，值为字典或者列表，表示需要最优化的参数的取值\n",
    "#第三个参数是scoring，模型评价标准，这里选择了roc_auc\n",
    "#第四个参数是cv，交叉验证参数。\n",
    "#第五个参数是n_jobs，并行线程数，取-1表示跟CPU核数一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 125.21319794654846 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__learning_rate': 0.01, 'clf__n_estimators': 10, 'vect__max_df': 0.1, 'vect__max_features': None, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['had', 'under', 't', 'did', 'which', 'from', 'wouldn', 'are', 'some', \"don't\", \"you'll\", \"that'll\", 's', 'why', 'again', 'into', 'all', 'can', 'mustn', \"aren't\", 'mightn', 'an', 'while', 'during', 'than', 'hadn', \"mustn't\", 'shouldn', 'their', \"you've\", 'your', 'isn', 'so', 'but', \"couldn't\", 'or', 'yourselves', \"wouldn't\", 'am', 'him', 'my', 'over', 'further', 'each', 'not', 'down', 'won', 'if', \"weren't\", 'in', 'been', 'where', 'will', 'who', 'd', 'here', 'nor', 'before', 'off', 'shan', 'don', 'those', 'and', 'was', 'hers', 'ourselves', 'his', 'after', 'because', 'when', 'theirs', 'm', 'other', \"she's\", 'itself', 'on', 'have', 'them', 'out', 'by', \"should've\", 'he', 'doing', 'hasn', 'for', 'then', 'only', \"needn't\", 'i', 'we', 'being', 'as', \"doesn't\", 'didn', 'she', 'until', \"you're\", 'through', 'to', 'of', 'you', 'wasn', 'it', 'should', 'own', 'herself', 'weren', 'any', 'how', 'themselves', 'at', 'o', \"you'd\", 'both', \"hasn't\", 'll', \"didn't\", 'no', 'ours', \"haven't\", \"wasn't\", 'me', 'himself', 'be', 'has', 'were', 'does', 'a', 'too', 'this', 'do', 'haven', 'they', 'against', 've', 'just', 'doesn', \"won't\", 'her', 'the', 'now', 'same', 'most', 'that', 'up', \"isn't\", 'between', 'what', 'these', 'its', \"shouldn't\", 'there', \"it's\", 're', \"mightn't\", 'ain', 'below', 'our', 'is', 'ma', 'above', 'myself', 'with', 'aren', 'yours', 'very', 'more', 'having', 'once', 'such', 'about', 'y', 'few', 'whom', 'couldn', 'needn', \"shan't\", \"hadn't\", 'yourself']} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.714\n",
      "Test Accuracy: 0.680\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2. Bag-of-POS<a name=\"ADA_BOP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟5.1.1不同的地方在于，5.1.2输入的是data_POStagged.csv数据\n",
    "即5.1.1用文本数据生成tfidf向量，5.1.2采用词性序生成tfidf向量作为AdaBoost的分类依据\n",
    "从原理上看，POS数据损失了部分信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400,)\n",
      "X_test shape check:  (100,)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Running time: 5.367911100387573 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__learning_rate': 0.001, 'clf__n_estimators': 30, 'vect__ngram_range': (1, 1)} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.487\n",
      "Test Accuracy: 0.500\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_POStagged.csv'  \n",
    "# insert path to data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(400).text_pos\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.tail(100).text_pos\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [10, 20, 30, 40],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', AdaBoostClassifier(base_estimator=tree))]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1)                        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3. Embeddings<a name=\"ADA_E\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和前面两种方法不一样，Embeddings方法用的是3.4.2 中根据en_core_web_sm模型得到的词向量数据，所以在pipline中不需要再对输入数据做处理，直接用AdaBoostClassifier进行拟合即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400, 96)\n",
      "X_test shape check:  (100, 96)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Running time: 7.930799961090088 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__learning_rate': 0.001, 'clf__n_estimators': 40} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.544\n",
      "Test Accuracy: 0.560\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_dedandprep.csv'  \n",
    "# insert path to data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(400)    # we use only the 300 embeddings\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(100)      # we use only the 300 embeddings\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'clf__n_estimators': [10, 20, 30, 40],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1]}\n",
    "\n",
    "# extended parameter grid (Table 6, Section 6.4.5 in the tutorial)\n",
    "# param_grid = {'clf__n_estimators': [100, 200, 300, 400, 500, 700, 900, 1000],\n",
    "#              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "pipe = Pipeline([('clf', AdaBoostClassifier(base_estimator=tree))])\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1)                        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Random Forests (RF)<a name=\"RF\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. Bag-of-words<a name=\"RF_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400,)\n",
      "X_test shape check:  (100,)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Running time: 41.61217403411865 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__max_depth': 10, 'clf__n_estimators': 40, 'vect__max_df': 1.0, 'vect__max_features': 400, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['had', 'under', 't', 'did', 'which', 'from', 'wouldn', 'are', 'some', \"don't\", \"you'll\", \"that'll\", 's', 'why', 'again', 'into', 'all', 'can', 'mustn', \"aren't\", 'mightn', 'an', 'while', 'during', 'than', 'hadn', \"mustn't\", 'shouldn', 'their', \"you've\", 'your', 'isn', 'so', 'but', \"couldn't\", 'or', 'yourselves', \"wouldn't\", 'am', 'him', 'my', 'over', 'further', 'each', 'not', 'down', 'won', 'if', \"weren't\", 'in', 'been', 'where', 'will', 'who', 'd', 'here', 'nor', 'before', 'off', 'shan', 'don', 'those', 'and', 'was', 'hers', 'ourselves', 'his', 'after', 'because', 'when', 'theirs', 'm', 'other', \"she's\", 'itself', 'on', 'have', 'them', 'out', 'by', \"should've\", 'he', 'doing', 'hasn', 'for', 'then', 'only', \"needn't\", 'i', 'we', 'being', 'as', \"doesn't\", 'didn', 'she', 'until', \"you're\", 'through', 'to', 'of', 'you', 'wasn', 'it', 'should', 'own', 'herself', 'weren', 'any', 'how', 'themselves', 'at', 'o', \"you'd\", 'both', \"hasn't\", 'll', \"didn't\", 'no', 'ours', \"haven't\", \"wasn't\", 'me', 'himself', 'be', 'has', 'were', 'does', 'a', 'too', 'this', 'do', 'haven', 'they', 'against', 've', 'just', 'doesn', \"won't\", 'her', 'the', 'now', 'same', 'most', 'that', 'up', \"isn't\", 'between', 'what', 'these', 'its', \"shouldn't\", 'there', \"it's\", 're', \"mightn't\", 'ain', 'below', 'our', 'is', 'ma', 'above', 'myself', 'with', 'aren', 'yours', 'very', 'more', 'having', 'once', 'such', 'about', 'y', 'few', 'whom', 'couldn', 'needn', \"shan't\", \"hadn't\", 'yourself']} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.838\n",
      "Test Accuracy: 0.760\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_dedandprep.csv'  # insert path to deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(400).review\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.tail(100).review\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],    # for 2-grams:(1, 2)\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 400],                                           \n",
    "              'clf__n_estimators': [10, 20, 30, 40],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', RandomForestClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1)       # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. Bag-of-POS<a name=\"RF_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400,)\n",
      "X_test shape check:  (100,)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Running time: 1.977264165878296 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__max_depth': 5, 'clf__n_estimators': 40, 'vect__ngram_range': (1, 1)} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.572\n",
      "Test Accuracy: 0.550\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_POStagged.csv'  # insert path to data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(400).text_pos\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.tail(100).text_pos\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [10, 20, 30, 40],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', RandomForestClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1)              # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3. Embeddings<a name=\"RF_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400, 96)\n",
      "X_test shape check:  (100, 96)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Running time: 1.193748950958252 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__max_depth': 10, 'clf__n_estimators': 40} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.612\n",
      "Test Accuracy: 0.580\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import \n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_dedandprep.csv'  # insert path to data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(400)    # we use only the 300 embeddings\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(100)     # we use only the 300 embeddings\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "              'clf__n_estimators': [10, 20, 30, 40],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "             }\n",
    "\n",
    "# extended parameter grid (Table 6, Section 6.4.5 in the tutorial)\n",
    "# param_grid = {'clf__n_estimators': [100, 200, 300, 400, 500, 600, 800, 1000],\n",
    "#              'clf__max_depth': [1, 5, 10, 20]}\n",
    "\n",
    "pipe = Pipeline([('clf', RandomForestClassifier())\n",
    "               ])\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1)               # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Extreme gradient boosting (XGB)<a name=\"XGB\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Bag-of-words<a name=\"XGB_BOW\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个最久，要四分多钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400,)\n",
      "X_test shape check:  (100,)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Running time: 253.30183291435242 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__learning_rate': 0.1, 'clf__max_depth': 10, 'clf__n_estimators': 40, 'vect__max_df': 0.1, 'vect__max_features': 400, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['had', 'under', 't', 'did', 'which', 'from', 'wouldn', 'are', 'some', \"don't\", \"you'll\", \"that'll\", 's', 'why', 'again', 'into', 'all', 'can', 'mustn', \"aren't\", 'mightn', 'an', 'while', 'during', 'than', 'hadn', \"mustn't\", 'shouldn', 'their', \"you've\", 'your', 'isn', 'so', 'but', \"couldn't\", 'or', 'yourselves', \"wouldn't\", 'am', 'him', 'my', 'over', 'further', 'each', 'not', 'down', 'won', 'if', \"weren't\", 'in', 'been', 'where', 'will', 'who', 'd', 'here', 'nor', 'before', 'off', 'shan', 'don', 'those', 'and', 'was', 'hers', 'ourselves', 'his', 'after', 'because', 'when', 'theirs', 'm', 'other', \"she's\", 'itself', 'on', 'have', 'them', 'out', 'by', \"should've\", 'he', 'doing', 'hasn', 'for', 'then', 'only', \"needn't\", 'i', 'we', 'being', 'as', \"doesn't\", 'didn', 'she', 'until', \"you're\", 'through', 'to', 'of', 'you', 'wasn', 'it', 'should', 'own', 'herself', 'weren', 'any', 'how', 'themselves', 'at', 'o', \"you'd\", 'both', \"hasn't\", 'll', \"didn't\", 'no', 'ours', \"haven't\", \"wasn't\", 'me', 'himself', 'be', 'has', 'were', 'does', 'a', 'too', 'this', 'do', 'haven', 'they', 'against', 've', 'just', 'doesn', \"won't\", 'her', 'the', 'now', 'same', 'most', 'that', 'up', \"isn't\", 'between', 'what', 'these', 'its', \"shouldn't\", 'there', \"it's\", 're', \"mightn't\", 'ain', 'below', 'our', 'is', 'ma', 'above', 'myself', 'with', 'aren', 'yours', 'very', 'more', 'having', 'once', 'such', 'about', 'y', 'few', 'whom', 'couldn', 'needn', \"shan't\", \"hadn't\", 'yourself']} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.744\n",
      "Test Accuracy: 0.670\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_dedandprep.csv'  # insert path to preprocessed and deduplicated data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(400).review\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.tail(100).review\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "#如果import失败要先安装，安装代码如下\n",
    "#import pip\n",
    "#pip.main(['install', 'xgboost'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],    # for 2-grams: (1,2)\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 400],                                           \n",
    "              'clf__n_estimators': [10, 20, 30, 40],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', XGBClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1)                 # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Bag-of-POS<a name=\"XGB_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400,)\n",
      "X_test shape check:  (100,)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Running time: 25.77832818031311 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__learning_rate': 0.1, 'clf__max_depth': 1, 'clf__n_estimators': 40, 'vect__ngram_range': (1, 1)} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.770\n",
      "Test Accuracy: 0.660\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_POStagged.csv'  # insert data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(400).review\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.tail(100).review\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [10, 20, 30, 40],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', XGBClassifier())]\n",
    "                    )\n",
    "\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1)              # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Embeddings<a name=\"XGB_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "X_train shape check:  (400, 96)\n",
      "X_test shape check:  (100, 96)\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Running time: 4.340090990066528 Seconds\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Best parameter set: {'clf__learning_rate': 0.1, 'clf__max_depth': 1, 'clf__n_estimators': 40} \n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "Test AUC: 0.575\n",
      "Test Accuracy: 0.560\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/data_dedandprep.csv'  # insert data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(400)    # we use only the 300 embeddings\n",
    "y_train = df.head(400).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(100)     # we use only the 300 embeddings\n",
    "y_test = df.tail(100).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "              'clf__n_estimators': [10, 20, 30, 40],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "pipe = Pipeline([('clf', XGBClassifier())\n",
    "                ])\n",
    "                    \n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=-1)        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "start=time.time()\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deep Learning<a name=\"DL\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure that all keras functionalities work as intended\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 201)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = \"/Users/huihuajiang/Desktop/t1课程/现代精算统计模型/nlp/toymdb/movie_data_processed.csv\" # TODO: update to your path\n",
    "df_processed = pd.read_csv(path, encoding='utf-8')\n",
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Train test split<a name=\"trainsplit\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/20 train test split: 400 / 100\n"
     ]
    }
   ],
   "source": [
    "# get the number of samples for the training and test datasets\n",
    "perc_train = 0.8\n",
    "n_train = round(df_processed.shape[0]*perc_train)\n",
    "n_test = round(df_processed.shape[0]*(1-perc_train))\n",
    "\n",
    "print(str(int(perc_train*100))+'/'+str(int(100-perc_train*100))+' train test split:', \n",
    "      n_train, '/', n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape check X, y: (400, 200) (400,)\n",
      "Testing data shape check X, y: (100, 200) (100,)\n"
     ]
    }
   ],
   "source": [
    "# create the training and testing datasets\n",
    "X_train = np.array(df_processed.head(n_train).drop('sentiment', axis=1)) # replace with n_train\n",
    "y_train = df_processed.head(n_train).sentiment.values\n",
    "\n",
    "X_test = np.array(df_processed.tail(n_test).drop('sentiment', axis=1)) # replace with n_test\n",
    "y_test = df_processed.tail(n_test).sentiment.values\n",
    "\n",
    "print('Training data shape check X, y:', X_train.shape, y_train.shape)\n",
    "print('Testing data shape check X, y:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Define the model<a name=\"modeldef\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 399\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# get the total number of words in vocabulary (+1 for the padding with 0)\n",
    "vocab_size = df_processed.drop('sentiment', axis=1).values.max() + 1\n",
    "print('Vocabulary size:', vocab_size-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Shallow LSTM architecture<a name=\"LSTM1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a LSTM layer with 128 internal units\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a GRU layer with 128 internal units\n",
    "model.add(layers.GRU(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Deep LSTM architecture<a name=\"LSTM2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a LSTM layer with 128 internal units\n",
    "# Return sequences so we can stack the the next LSTM layer on top\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "# Add a second LSTM layer\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Train the model<a name=\"trainmodel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 256)         102400    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 128)         197120    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 431,233\n",
      "Trainable params: 431,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# print the summary of the model we have defined\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "# we select here the optimizer, loss and metric to be used in training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for early stopping during training\n",
    "# stop training when the validation loss `val_accuracy` is no longer improving\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=1e-3,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 0.6934 - accuracy: 0.5281 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.6906 - accuracy: 0.6344 - val_loss: 0.6911 - val_accuracy: 0.6375\n",
      "Epoch 3/30\n",
      "2/2 [==============================] - 1s 299ms/step - loss: 0.6884 - accuracy: 0.6531 - val_loss: 0.6879 - val_accuracy: 0.5750\n",
      "Epoch 4/30\n",
      "2/2 [==============================] - 1s 293ms/step - loss: 0.6816 - accuracy: 0.6469 - val_loss: 0.6826 - val_accuracy: 0.5750\n",
      "Epoch 5/30\n",
      "2/2 [==============================] - 1s 334ms/step - loss: 0.6657 - accuracy: 0.7031 - val_loss: 0.6747 - val_accuracy: 0.5875\n",
      "Epoch 6/30\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.6286 - accuracy: 0.7344 - val_loss: 0.6598 - val_accuracy: 0.5875\n",
      "Epoch 7/30\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.7469Restoring model weights from the end of the best epoch.\n",
      "2/2 [==============================] - 1s 303ms/step - loss: 0.5485 - accuracy: 0.7469 - val_loss: 0.7067 - val_accuracy: 0.5750\n",
      "Epoch 00007: early stopping\n",
      "Running time: 17.755706071853638 Seconds\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "start=time.time()\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_split=0.2,\n",
    "                    epochs=30, \n",
    "                    batch_size=256,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "end=time.time()\n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13d7aed30>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the development of the accuracy over epochs to see the training progress\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x13b517898>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the development of the loss over epochs to see the training progress\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Evaluate the model on test data<a name=\"evalmodel\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-95-d652e8b291a3>:7: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "Test loss: 0.6916659474372864\n",
      "Test accuracy: 0.5199999809265137\n",
      "Test AUC: 0.6061698717948718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# evalute model to get accuracy and loss on test data\n",
    "results = model.evaluate(X_test, y_test, batch_size=256, verbose=0)\n",
    "\n",
    "# calculate AUC on test data\n",
    "y_pred = model.predict_proba(X_test, batch_size=256)\n",
    "auc_res = roc_auc_score(y_test, y_pred[:, 0])\n",
    "\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])\n",
    "print('Test AUC:', auc_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
